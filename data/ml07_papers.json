{
  "owasp_id": "ML07",
  "owasp_name": "Transfer Attacks",
  "total": 2,
  "updated": "2026-01-09",
  "papers": [
    {
      "paper_id": "2307.07873",
      "title": "Why Does Little Robustness Help? A Further Step Towards Understanding Adversarial Transferability",
      "abstract": "Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in adversarial training explains the degradation of gradient similarity. Building on these insights, we explore the impacts of data augmentation and gradient regularization on transferability and identify that the trade-off generally exists in the various training mechanisms, thus building a comprehensive blueprint for the regulation mechanism behind transferability. Finally, we provide a general route for constructing better surrogates to boost transferability which optimizes both model smoothness and gradient similarity simultaneously, e.g., the combination of input gradient regularization and sharpness-aware minimization (SAM), validated by extensive experiments. In summary, we call for attention to the united impacts of these two factors for launching effective transfer attacks, rather than optimizing one while ignoring the other, and emphasize the crucial role of manipulating surrogate models.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Yechao Zhang",
        "Shengshan Hu",
        "Leo Yu Zhang",
        "Junyu Shi",
        "Minghui Li",
        "Xiaogeng Liu",
        "Wei Wan",
        "Hai Jin"
      ],
      "url": "https://arxiv.org/abs/2307.07873",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_cd05f758",
      "title": "Transferring Adversarial Robustness Through Robust Representation Matching",
      "abstract": "With the widespread use of machine learning, concerns over its security and reliability have become prevalent. As such, many have developed defenses to harden neural networks against adversarial examples, imperceptibly perturbed inputs that are reliably misclassified. Adversarial training in which adversarial examples are generated and used during training is one of the few known defenses able to reliably withstand such attacks against neural networks. However, adversarial training imposes a significant training overhead and scales poorly with model complexity and input dimension. In this paper, we propose Robust Representation Matching (RRM), a low-cost method to transfer the robustness of an adversarially trained model to a new model being trained for the same task irrespective of architectural differences. Inspired by student-teacher learning, our method introduces a novel training loss that encourages the student to learn the teacher's robust representations. Compared to prior works, RRM is superior with respect to both model performance and adversarial training time. On CIFAR-10, RRM trains a robust model $\\sim 1.8\\times$ faster than the state-of-the-art. Furthermore, RRM remains effective on higher-dimensional datasets. On Restricted-ImageNet, RRM trains a ResNet50 model $\\sim 18\\times$ faster than standard adversarial training.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "P. Vaishnavi",
        "Kevin Eykholt",
        "Amir Rahmati"
      ],
      "url": "https://openalex.org/W4221166176",
      "classification_confidence": "HIGH"
    }
  ]
}