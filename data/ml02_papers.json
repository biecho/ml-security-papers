{
  "owasp_id": "ML02",
  "owasp_name": "Data Poisoning",
  "total": 74,
  "updated": "2026-01-09",
  "papers": [
    {
      "paper_id": "2208.12897",
      "title": "ATTRITION: Attacking Static Hardware Trojan Detection Techniques Using Reinforcement Learning",
      "abstract": "Stealthy hardware Trojans (HTs) inserted during the fabrication of integrated circuits can bypass the security of critical infrastructures. Although researchers have proposed many techniques to detect HTs, several limitations exist, including: (i) a low success rate, (ii) high algorithmic complexity, and (iii) a large number of test patterns. Furthermore, the most pertinent drawback of prior detection techniques stems from an incorrect evaluation methodology, i.e., they assume that an adversary inserts HTs randomly. Such inappropriate adversarial assumptions enable detection techniques to claim high HT detection accuracy, leading to a \"false sense of security.\" Unfortunately, to the best of our knowledge, despite more than a decade of research on detecting HTs inserted during fabrication, there have been no concerted efforts to perform a systematic evaluation of HT detection techniques.   In this paper, we play the role of a realistic adversary and question the efficacy of HT detection techniques by developing an automated, scalable, and practical attack framework, ATTRITION, using reinforcement learning (RL). ATTRITION evades eight detection techniques across two HT detection categories, showcasing its agnostic behavior. ATTRITION achieves average attack success rates of $47\\times$ and $211\\times$ compared to randomly inserted HTs against state-of-the-art HT detection techniques. We demonstrate ATTRITION's ability to evade detection techniques by evaluating designs ranging from the widely-used academic suites to larger designs such as the open-source MIPS and mor1kx processors to AES and a GPS module. Additionally, we showcase the impact of ATTRITION-generated HTs through two case studies (privilege escalation and kill switch) on the mor1kx processor. We envision that our work, along with our released HT benchmarks and models, fosters the development of better HT detection techniques.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Vasudev Gohil",
        "Hao Guo",
        "Satwik Patnaik",
        " Jeyavijayan",
        " Rajendran"
      ],
      "url": "https://arxiv.org/abs/2208.12897",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_9a529de7",
      "title": "Local Model Poisoning Attacks to Byzantine-Robust Federated Learning",
      "abstract": "In federated learning, multiple client devices jointly learn a machine learning model: each client device maintains a local model for its local training dataset, while a master device maintains a global model via aggregating the local models from the client devices. The machine learning community recently proposed several federated learning methods that were claimed to be robust against Byzantine failures (e.g., system failures, adversarial manipulations) of certain client devices. In this work, we perform the first systematic study on local model poisoning attacks to federated learning. We assume an attacker has compromised some client devices, and the attacker manipulates the local model parameters on the compromised client devices during the learning process such that the global model has a large testing error rate. We formulate our attacks as optimization problems and apply our attacks to four recent Byzantine-robust federated learning methods. Our empirical results on four real-world datasets show that our attacks can substantially increase the error rates of the models learnt by the federated learning methods that were claimed to be robust against Byzantine failures of some client devices. We generalize two defenses for data poisoning attacks to defend against our local model poisoning attacks. Our evaluation results show that one defense can effectively defend against our attacks in some cases, but the defenses are not effective enough in other cases, highlighting the need for new defenses against our local model poisoning attacks to federated learning.",
      "year": 2019,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Minghong Fang",
        "Xiaoyu Cao",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://openalex.org/W2990614164",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_7922130e",
      "title": "Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning",
      "abstract": "Federated learning (FL) enables many data owners (e.g., mobile devices) to train a joint ML model (e.g., a nextword prediction classifier) without the need of sharing their private training data.However, FL is known to be susceptible to poisoning attacks by malicious participants (e.g., adversaryowned mobile devices) who aim at hampering the accuracy of the jointly trained model through sending malicious inputs during the federated training process.In this paper, we present a generic framework for model poisoning attacks on FL.We show that our framework leads to poisoning attacks that substantially outperform state-of-the-art model poisoning attacks by large margins.For instance, our attacks result in 1.5\u00d7 to 60\u00d7 higher reductions in the accuracy of FL models compared to previously discovered poisoning attacks.Our work demonstrates that existing Byzantine-robust FL algorithms are significantly more susceptible to model poisoning than previously thought.Motivated by this, we design a defense against FL poisoning, called divide-and-conquer (DnC).We demonstrate that DnC outperforms all existing Byzantine-robust FL algorithms in defeating model poisoning attacks, specifically, it is 2.5\u00d7 to 12\u00d7 more resilient in our experiments with different datasets and models.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Virat Shejwalkar",
        "Amir Houmansadr"
      ],
      "url": "https://openalex.org/W3138153888",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2201.00763",
      "title": "DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection",
      "abstract": "Federated Learning (FL) allows multiple clients to collaboratively train a Neural Network (NN) model on their private data without revealing the data. Recently, several targeted poisoning attacks against FL have been introduced. These attacks inject a backdoor into the resulting model that allows adversary-controlled inputs to be misclassified. Existing countermeasures against backdoor attacks are inefficient and often merely aim to exclude deviating models from the aggregation. However, this approach also removes benign models of clients with deviating data distributions, causing the aggregated model to perform poorly for such clients.   To address this problem, we propose DeepSight, a novel model filtering approach for mitigating backdoor attacks. It is based on three novel techniques that allow to characterize the distribution of data used to train model updates and seek to measure fine-grained differences in the internal structure and outputs of NNs. Using these techniques, DeepSight can identify suspicious model updates. We also develop a scheme that can accurately cluster model updates. Combining the results of both components, DeepSight is able to identify and eliminate model clusters containing poisoned models with high attack impact. We also show that the backdoor contributions of possibly undetected poisoned models can be effectively mitigated with existing weight clipping-based defenses. We evaluate the performance and effectiveness of DeepSight and show that it can mitigate state-of-the-art backdoor attacks with a negligible impact on the model's performance on benign data.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Phillip Rieger",
        "Thien Duc Nguyen",
        "Markus Miettinen",
        "Ahmad-Reza Sadeghi"
      ],
      "url": "https://arxiv.org/abs/2201.00763",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_0f448dd2",
      "title": "FLAME: Taming Backdoors in Federated Learning",
      "abstract": "Federated Learning (FL) is a collaborative machine learning approach allowing participants to jointly train a model without having to share their private, potentially sensitive local datasets with others. Despite its benefits, FL is vulnerable to backdoor attacks, in which an adversary injects manipulated model updates into the model aggregation process so that the resulting model will provide targeted false predictions for specific adversary-chosen inputs. Proposed defenses against backdoor attacks based on detecting and filtering out malicious model updates consider only very specific and limited attacker models, whereas defenses based on differential privacy-inspired noise injection significantly deteriorate the benign performance of the aggregated model. To address these deficiencies, we introduce FLAME, a defense framework that estimates the sufficient amount of noise to be injected to ensure the elimination of backdoors while maintaining the model performance. To minimize the required amount of noise, FLAME uses a model clustering and weight clipping approach. Our evaluation of FLAME on several datasets stemming from application areas including image classification, word prediction, and IoT intrusion detection demonstrates that FLAME removes backdoors effectively with a negligible impact on the benign performance of the models. Furthermore, following the considerable attention that our research has received after its presentation at USENIX SEC 2022, FLAME has become the subject of numerous investigations proposing diverse attack methodologies in an attempt to circumvent it. As a response to these endeavors, we provide a comprehensive analysis of these attempts. Our findings show that these papers (e.g., 3DFed [36]) have not fully comprehended nor correctly employed the fundamental principles underlying FLAME, i.e., our defense mechanism effectively repels these attempted attacks.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Thien Duc Nguyen",
        "Phillip Rieger",
        "Huili Chen",
        "Hossein Yalame",
        "Helen M\u00f6llering",
        "Hossein Fereidooni",
        "Samuel Marchal",
        "Markus Miettinen",
        "Azalia Mirhoseini",
        "Shaza Zeitouni",
        "Farinaz Koushanfar",
        "Ahmad\u2010Reza Sadeghi",
        "Thomas Schneider"
      ],
      "url": "https://openalex.org/W4287393324",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2210.10936",
      "title": "FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information",
      "abstract": "Federated learning is vulnerable to poisoning attacks in which malicious clients poison the global model via sending malicious model updates to the server. Existing defenses focus on preventing a small number of malicious clients from poisoning the global model via robust federated learning methods and detecting malicious clients when there are a large number of them. However, it is still an open challenge how to recover the global model from poisoning attacks after the malicious clients are detected. A naive solution is to remove the detected malicious clients and train a new global model from scratch, which incurs large cost that may be intolerable for resource-constrained clients such as smartphones and IoT devices.   In this work, we propose FedRecover, which can recover an accurate global model from poisoning attacks with small cost for the clients. Our key idea is that the server estimates the clients' model updates instead of asking the clients to compute and communicate them during the recovery process. In particular, the server stores the global models and clients' model updates in each round, when training the poisoned global model. During the recovery process, the server estimates a client's model update in each round using its stored historical information. Moreover, we further optimize FedRecover to recover a more accurate global model using warm-up, periodic correction, abnormality fixing, and final tuning strategies, in which the server asks the clients to compute and communicate their exact model updates. Theoretically, we show that the global model recovered by FedRecover is close to or the same as that recovered by train-from-scratch under some assumptions. Empirically, our evaluation on four datasets, three federated learning methods, as well as untargeted and targeted poisoning attacks (e.g., backdoor attacks) shows that FedRecover is both accurate and efficient.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Xiaoyu Cao",
        "Jinyuan Jia",
        "Zaixi Zhang",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://arxiv.org/abs/2210.10936",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_ad933014",
      "title": "Every Vote Counts: Ranking-Based Training of Federated Learning to Resist Poisoning Attacks",
      "abstract": "Federated learning (FL) allows mutually untrusted clients to collaboratively train a common machine learning model without sharing their private/proprietary training data among each other. FL is unfortunately susceptible to poisoning by malicious clients who aim to hamper the accuracy of the commonly trained model through sending malicious model updates during FL's training process.   We argue that the key factor to the success of poisoning attacks against existing FL systems is the large space of model updates available to the clients, allowing malicious clients to search for the most poisonous model updates, e.g., by solving an optimization problem. To address this, we propose Federated Rank Learning (FRL). FRL reduces the space of client updates from model parameter updates (a continuous space of float numbers) in standard FL to the space of parameter rankings (a discrete space of integer values). To be able to train the global model using parameter ranks (instead of parameter weights), FRL leverage ideas from recent supermasks training mechanisms. Specifically, FRL clients rank the parameters of a randomly initialized neural network (provided by the server) based on their local training data. The FRL server uses a voting mechanism to aggregate the parameter rankings submitted by clients in each training epoch to generate the global ranking of the next training epoch.   Intuitively, our voting-based aggregation mechanism prevents poisoning clients from making significant adversarial modifications to the global model, as each client will have a single vote! We demonstrate the robustness of FRL to poisoning through analytical proofs and experimentation. We also show FRL's high communication efficiency. Our experiments demonstrate the superiority of FRL in real-world FL settings.",
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [
        "Hamid Mozaffari",
        "Virat Shejwalkar",
        "Amir Houmansadr"
      ],
      "url": "https://arxiv.org/abs/2110.04350",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_fdf2c3b3",
      "title": "Securing Federated Sensitive Topic Classification against Poisoning Attacks",
      "abstract": "We present a Federated Learning (FL) based solution for building a distributed classifier capable of detecting URLs containing sensitive content, i.e., content related to categories such as health, political beliefs, sexual orientation, etc. Although such a classifier addresses the limitations of previous offline/centralised classifiers, it is still vulnerable to poisoning attacks from malicious users that may attempt to reduce the accuracy for benign users by disseminating faulty model updates. To guard against this, we develop a robust aggregation scheme based on subjective logic and residual-based attack detection. Employing a combination of theoretical analysis, trace-driven simulation, as well as experimental validation with a prototype and real users, we show that our classifier can detect sensitive content with high accuracy, learn new labels fast, and remain robust in view of poisoning attacks from malicious users, as well as imperfect input from non-malicious ones.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Tianyue Chu",
        "\u00c1lvaro Garc\u00eda-Recuero",
        "Costas Iordanou",
        "Georgios Smaragdakis",
        "Nikolaos Laoutaris"
      ],
      "url": "https://openalex.org/W4307300892",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2301.09508",
      "title": "BayBFed: Bayesian Backdoor Defense for Federated Learning",
      "abstract": "Federated learning (FL) allows participants to jointly train a machine learning model without sharing their private data with others. However, FL is vulnerable to poisoning attacks such as backdoor attacks. Consequently, a variety of defenses have recently been proposed, which have primarily utilized intermediary states of the global model (i.e., logits) or distance of the local models (i.e., L2-norm) from the global model to detect malicious backdoors. However, as these approaches directly operate on client updates, their effectiveness depends on factors such as clients' data distribution or the adversary's attack strategies. In this paper, we introduce a novel and more generic backdoor defense framework, called BayBFed, which proposes to utilize probability distributions over client updates to detect malicious updates in FL: it computes a probabilistic measure over the clients' updates to keep track of any adjustments made in the updates, and uses a novel detection algorithm that can leverage this probabilistic measure to efficiently detect and filter out malicious updates. Thus, it overcomes the shortcomings of previous approaches that arise due to the direct usage of client updates; as our probabilistic measure will include all aspects of the local client training strategies. BayBFed utilizes two Bayesian Non-Parametric extensions: (i) a Hierarchical Beta-Bernoulli process to draw a probabilistic measure given the clients' updates, and (ii) an adaptation of the Chinese Restaurant Process (CRP), referred by us as CRP-Jensen, which leverages this probabilistic measure to detect and filter out malicious updates. We extensively evaluate our defense approach on five benchmark datasets: CIFAR10, Reddit, IoT intrusion detection, MNIST, and FMNIST, and show that it can effectively detect and eliminate malicious updates in FL without deteriorating the benign performance of the global model.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Kavita Kumari",
        "Phillip Rieger",
        "Hossein Fereidooni",
        "Murtuza Jadliwala",
        "Ahmad-Reza Sadeghi"
      ],
      "url": "https://arxiv.org/abs/2301.09508",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_19e67c1a",
      "title": "3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning",
      "abstract": "Federated Learning (FL), the de-facto distributed machine learning paradigm that locally trains datasets at individual devices, is vulnerable to backdoor model poisoning attacks. By compromising or impersonating those devices, an attacker can upload crafted malicious model updates to manipulate the global model with backdoor behavior upon attacker-specified triggers. However, existing backdoor attacks require more information on the victim FL system beyond a practical black-box setting. Furthermore, they are often specialized to optimize for a single objective, which becomes ineffective as modern FL systems tend to adopt in-depth defense that detects backdoor models from different perspectives. Motivated by these concerns, in this paper, we propose 3DFed, an adaptive, extensible, and multi-layered framework to launch covert FL backdoor attacks in a black-box setting. 3DFed sports three evasion modules that camouflage backdoor models: backdoor training with constrained loss, noise mask, and decoy model. By implanting indicators into a backdoor model, 3DFed can obtain the attack feedback in the previous epoch from the global model and dynamically adjust the hyper-parameters of these backdoor evasion modules. Through extensive experimental results, we show that when all its components work together, 3DFed can evade the detection of all state-of-the-art FL backdoor defenses, including Deepsight, Foolsgold, FLAME, FL-Detector, and RFLBAT. New evasion modules can also be incorporated in 3DFed in the future as it is an extensible framework.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Haoyang Li",
        "Qingqing Ye",
        "Haibo Hu",
        "Jin Li",
        "Leixia Wang",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "url": "https://openalex.org/W4385187226",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2308.05832",
      "title": "FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks",
      "abstract": "Federated learning (FL) is revolutionizing how we learn from data. With its growing popularity, it is now being used in many safety-critical domains such as autonomous vehicles and healthcare. Since thousands of participants can contribute in this collaborative setting, it is, however, challenging to ensure security and reliability of such systems. This highlights the need to design FL systems that are secure and robust against malicious participants' actions while also ensuring high utility, privacy of local data, and efficiency. In this paper, we propose a novel FL framework dubbed as FLShield that utilizes benign data from FL participants to validate the local models before taking them into account for generating the global model. This is in stark contrast with existing defenses relying on server's access to clean datasets -- an assumption often impractical in real-life scenarios and conflicting with the fundamentals of FL. We conduct extensive experiments to evaluate our FLShield framework in different settings and demonstrate its effectiveness in thwarting various types of poisoning and backdoor attacks including a defense-aware one. FLShield also preserves privacy of local data against gradient inversion attacks.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Ehsanul Kabir",
        "Zeyu Song",
        "Md Rafi Ur Rashid",
        "Shagufta Mehnaz"
      ],
      "url": "https://arxiv.org/abs/2308.05832",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2304.08847",
      "title": "BadVFL: Backdoor Attacks in Vertical Federated Learning",
      "abstract": "Federated learning (FL) enables multiple parties to collaboratively train a machine learning model without sharing their data; rather, they train their own model locally and send updates to a central server for aggregation. Depending on how the data is distributed among the participants, FL can be classified into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the same set of training instances but only host a different and non-overlapping subset of the whole feature space. Whereas in HFL, each participant shares the same set of features while the training set is split into locally owned training data subsets.   VFL is increasingly used in applications like financial fraud detection; nonetheless, very little work has analyzed its security. In this paper, we focus on robustness in VFL, in particular, on backdoor attacks, whereby an adversary attempts to manipulate the aggregate model during the training process to trigger misclassifications. Performing backdoor attacks in VFL is more challenging than in HFL, as the adversary i) does not have access to the labels during training and ii) cannot change the labels as she only has access to the feature embeddings. We present a first-of-its-kind clean-label backdoor attack in VFL, which consists of two phases: a label inference and a backdoor phase. We demonstrate the effectiveness of the attack on three different datasets, investigate the factors involved in its success, and discuss countermeasures to mitigate its impact.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Mohammad Naseri",
        "Yufei Han",
        "Emiliano De Cristofaro"
      ],
      "url": "https://arxiv.org/abs/2304.08847",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2210.07714",
      "title": "CrowdGuard: Federated Backdoor Detection in Federated Learning",
      "abstract": "Federated Learning (FL) is a promising approach enabling multiple clients to train Deep Neural Networks (DNNs) collaboratively without sharing their local training data. However, FL is susceptible to backdoor (or targeted poisoning) attacks. These attacks are initiated by malicious clients who seek to compromise the learning process by introducing specific behaviors into the learned model that can be triggered by carefully crafted inputs. Existing FL safeguards have various limitations: They are restricted to specific data distributions or reduce the global model accuracy due to excluding benign models or adding noise, are vulnerable to adaptive defense-aware adversaries, or require the server to access local models, allowing data inference attacks.   This paper presents a novel defense mechanism, CrowdGuard, that effectively mitigates backdoor attacks in FL and overcomes the deficiencies of existing techniques. It leverages clients' feedback on individual models, analyzes the behavior of neurons in hidden layers, and eliminates poisoned models through an iterative pruning scheme. CrowdGuard employs a server-located stacked clustering scheme to enhance its resilience to rogue client feedback. The evaluation results demonstrate that CrowdGuard achieves a 100% True-Positive-Rate and True-Negative-Rate across various scenarios, including IID and non-IID data distributions. Additionally, CrowdGuard withstands adaptive adversaries while preserving the original performance of protected models. To ensure confidentiality, CrowdGuard uses a secure and privacy-preserving architecture leveraging Trusted Execution Environments (TEEs) on both client and server sides.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Phillip Rieger",
        "Torsten Krau\u00df",
        "Markus Miettinen",
        "Alexandra Dmitrienko",
        "Ahmad-Reza Sadeghi"
      ],
      "url": "https://arxiv.org/abs/2210.07714",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_c61a2c5c",
      "title": "Automatic Adversarial Adaption for Stealthy Poisoning Attacks in Federated Learning",
      "abstract": "Federated Learning (FL) enables the training of machine learning models using distributed data.This approach offers benefits such as improved data privacy, reduced communication costs, and enhanced model performance through increased data diversity.However, FL systems are vulnerable to poisoning attacks, where adversaries introduce malicious updates to compromise the integrity of the aggregated model.Existing defense strategies against such attacks include filtering, influence reduction, and robust aggregation techniques.Filtering approaches have the advantage of not reducing classification accuracy, but face the challenge of adversaries adapting to the defense mechanisms.The lack of a universally accepted definition of \"adaptive adversaries\" in the literature complicates the assessment of detection capabilities and meaningful comparisons of FL defenses.In this paper, we address the limitations of the commonly used definition of \"adaptive attackers\" proposed by Bagdasaryan et al.We propose AutoAdapt, a novel adaptation method that leverages an Augmented Lagrangian optimization technique.AutoAdapt eliminates the manual search for optimal hyper-parameters by providing a more rational alternative.It generates more effective solutions by accommodating multiple inequality constraints, allowing adaptation to valid value ranges within the defensive metrics.Our proposed method significantly enhances adversaries' capabilities and accelerates research in developing attacks and defenses.By accommodating multiple valid range constraints and adapting to diverse defense metrics, AutoAdapt challenges defenses relying on multiple metrics and expands the range of potential adversarial behaviors.Through comprehensive studies, we demonstrate the effectiveness of AutoAdapt in simultaneously adapting to multiple constraints and showcasing its power by accelerating the performance of tests by a factor of 15.Furthermore, we establish the versatility of AutoAdapt across various application scenarios, encompassing datasets, model architectures, and hyper-parameters, emphasizing its practical utility in real-world contexts.Overall, our contributions advance the evaluation of FL defenses and drive progress in this field.Defenses against poisoning attacks employ three strategies: Influence Reduction (IR), Robust Aggregation (RA), and Detection and Filtering (DF).IR approaches [6], [8], [49], [71] perturb model parameters to cancel malicious behavior, RAbased defenses [81], [46] secure aggregation algorithms even in the presence of poisoned models, and DF-based solutions [13], [48], [67], [53], [31], [60], [84], [16] detect and filter out poisoned models before aggregation.Among the three categories, DF approaches appear to be more prominent solutions, as IR and RA-based systems unavoidably affect benign functionality.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Torsten Krau\u00df",
        "Jan K\u00f6nig",
        "Alexandra Dmitrienko",
        "Christian Kanzow"
      ],
      "url": "https://openalex.org/W4391725340",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2312.04432",
      "title": "FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning",
      "abstract": "Federated learning (FL) is a collaborative learning paradigm allowing multiple clients to jointly train a model without sharing their training data. However, FL is susceptible to poisoning attacks, in which the adversary injects manipulated model updates into the federated model aggregation process to corrupt or destroy predictions (untargeted poisoning) or implant hidden functionalities (targeted poisoning or backdoors). Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maintaining the utility of the aggregated model. To address the deficiencies of existing defenses, we take a generic and completely different approach to detect poisoning (targeted and untargeted) attacks. We present FreqFed, a novel aggregation mechanism that transforms the model updates (i.e., weights) into the frequency domain, where we can identify the core frequency components that inherit sufficient information about weights. This allows us to effectively filter out malicious updates during local training on the clients, regardless of attack types, strategies, and clients' data distributions. We extensively evaluate the efficiency and effectiveness of FreqFed in different application domains, including image classification, word prediction, IoT intrusion detection, and speech recognition. We demonstrate that FreqFed can mitigate poisoning attacks effectively with a negligible impact on the utility of the aggregated model.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Hossein Fereidooni",
        "Alessandro Pegoraro",
        "Phillip Rieger",
        "Alexandra Dmitrienko",
        "Ahmad-Reza Sadeghi"
      ],
      "url": "https://arxiv.org/abs/2312.04432",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_f53dc38c",
      "title": "Dealing Doubt: Unveiling Threat Models in Gradient Inversion Attacks under Federated Learning \u2013 A Survey and Taxonomy",
      "abstract": "Federated learning (FL) is a collaborative learning paradigm allowing multiple clients to jointly train a model without sharing their training data. However, FL is susceptible to poisoning attacks, in which the adversary injects manipulated model updates into the federated model aggregation process to corrupt or destroy predictions (untargeted poisoning) or implant hidden functionalities (targeted poisoning or backdoors). Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maintaining the utility of the aggregated model. To address the deficiencies of existing defenses, we take a generic and completely different approach to detect poisoning (targeted and untargeted) attacks. We present FreqFed, a novel aggregation mechanism that transforms the model updates (i.e., weights) into the frequency domain, where we can identify the core frequency components that inherit sufficient information about weights. This allows us to effectively filter out malicious updates during local training on the clients, regardless of attack types, strategies, and clients' data distributions. We extensively evaluate the efficiency and effectiveness of FreqFed in different application domains, including image classification, word prediction, IoT intrusion detection, and speech recognition. We demonstrate that FreqFed can mitigate poisoning attacks effectively with a negligible impact on the utility of the aggregated model.",
      "year": 2023,
      "venue": "CCS",
      "authors": [
        "Hossein Fereidooni",
        "Alessandro Pegoraro",
        "Phillip Rieger",
        "Alexandra Dmitrienko",
        "Ahmad-Reza Sadeghi"
      ],
      "url": "https://arxiv.org/abs/2312.04432",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_abf32fe4",
      "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
      "abstract": "Backdoor attacks are among the most effective, practical, and stealthy attacks in deep learning. In this paper, we consider a practical scenario where a developer obtains a deep model from a third party and uses it as part of a safety-critical system. The developer wants to inspect the model for potential backdoors prior to system deployment. We find that most existing detection techniques make assumptions that are not applicable to this scenario. In this paper, we present a novel framework for detecting backdoors under realistic restrictions. We generate candidate triggers by deductively searching over the space of possible triggers. We construct and optimize a smoothed version of Attack Success Rate as our search objective. Starting from a broad class of template attacks and just using the forward pass of a deep model, we reverse engineer the backdoor attack. We conduct extensive evaluation on a wide range of attacks, models, and datasets, with our technique performing almost perfectly across these settings.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "D Popovi\u0107",
        "Amin Sadeghi",
        "Ting Yu",
        "Sanjay Chawla",
        "Issa Khalil"
      ],
      "url": "https://openalex.org/W4415062401",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_a8f190bf",
      "title": "Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning",
      "abstract": "Word embeddings, i.e., low-dimensional vector representations such as GloVe and SGNS, encode word \"meaning\" in the sense that distances between words' vectors correspond to their semantic proximity. This enables transfer learning of semantics for a variety of natural language processing tasks. Word embeddings are typically trained on large public corpora such as Wikipedia or Twitter. We demonstrate that an attacker who can modify the corpus on which the embedding is trained can control the \"meaning\" of new and existing words by changing their locations in the embedding space. We develop an explicit expression over corpus features that serves as a proxy for distance between words and establish a causative relationship between its values and embedding distances. We then show how to use this relationship for two adversarial objectives: (1) make a word a top-ranked neighbor of another word, and (2) move a word from one semantic cluster to another. An attack on the embedding can affect diverse downstream tasks, demonstrating for the first time the power of data poisoning in transfer learning scenarios. We use this attack to manipulate query expansion in information retrieval systems such as resume search, make certain names more or less visible to named entity recognition models, and cause new words to be translated to a particular target word regardless of the language. Finally, we show how the attacker can generate linguistically likely corpus modifications, thus fooling defenses that attempt to filter implausible sentences from the corpus using a language model.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Roei Schuster",
        "Tal Schuster",
        "Yoav Meri",
        "Vitaly Shmatikov"
      ],
      "url": "https://openalex.org/W2999480335",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_7f4a9058",
      "title": "You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion",
      "abstract": "Code autocompletion is an integral feature of modern code editors and IDEs. The latest generation of autocompleters uses neural language models, trained on public open-source code repositories, to suggest likely (not just statically feasible) completions given the current context. \r\nWe demonstrate that neural code autocompleters are vulnerable to poisoning attacks. By adding a few specially-crafted files to the autocompleter's training corpus (data poisoning), or else by directly fine-tuning the autocompleter on these files (model poisoning), the attacker can influence its suggestions for attacker-chosen contexts. For example, the attacker can teach the autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3 for the SSL/TLS protocol version, or a low iteration count for password-based encryption. Moreover, we show that these attacks can be targeted: an autocompleter poisoned by a targeted attack is much more likely to suggest the insecure completion for files from a specific repo or specific developer. \r\nWe quantify the efficacy of targeted and untargeted data- and model-poisoning attacks against state-of-the-art autocompleters based on Pythia and GPT-2. We then evaluate existing defenses against poisoning attacks and show that they are largely ineffective.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Roei Schuster",
        "Congzheng Song",
        "Eran Tromer",
        "Vitaly Shmatikov"
      ],
      "url": "https://openalex.org/W3155981360",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2301.02344",
      "title": "TROJANPUZZLE: Covertly Poisoning Code-Suggestion Models",
      "abstract": "With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training by injecting malicious data. Poisoning attacks could be designed to influence the model's suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, COVERT and TROJANPUZZLE, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE, goes one step further in generating less suspicious poison data by never explicitly including certain (suspicious) parts of the payload in the poison data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TROJANPUZZLE robust against signature-based dataset-cleansing methods that can filter out suspicious sequences from the training data. Our evaluation against models of two sizes demonstrates that both COVERT and TROJANPUZZLE have significant implications for practitioners when selecting code used to train or tune code-suggestion models.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Hojjat Aghakhani",
        "Wei Dai",
        "Andre Manoel",
        "Xavier Fernandes",
        "Anant Kharkar",
        "Christopher Kruegel",
        "Giovanni Vigna",
        "David Evans",
        "Ben Zorn",
        "Robert Sim"
      ],
      "url": "https://arxiv.org/abs/2301.02344",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_94d1790d",
      "title": "Poisoning the Unlabeled Dataset of Semi-Supervised Learning",
      "abstract": "Semi-supervised machine learning models learn from a (small) set of labeled training examples, and a (large) set of unlabeled training examples. State-of-the-art models can reach within a few percentage points of fully-supervised training, while requiring 100x less labeled data. We study a new class of vulnerabilities: poisoning attacks that modify the unlabeled dataset. In order to be useful, unlabeled datasets are given strictly less review than labeled datasets, and adversaries can therefore poison them easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1% of the dataset size, we can manipulate a model trained on this poisoned dataset to misclassify arbitrary examples at test time (as any desired label). Our attacks are highly effective across datasets and semi-supervised learning methods. We find that more accurate methods (thus more likely to be used) are significantly more vulnerable to poisoning attacks, and as such better training methods are unlikely to prevent this attack. To counter this we explore the space of defenses, and propose two methods that mitigate our attack.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Nicholas Carlini"
      ],
      "url": "https://openalex.org/W3157597523",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2101.02644",
      "title": "Data Poisoning Attacks to Deep Learning Based Recommender Systems",
      "abstract": "Recommender systems play a crucial role in helping users to find their interested information in various web services such as Amazon, YouTube, and Google News. Various recommender systems, ranging from neighborhood-based, association-rule-based, matrix-factorization-based, to deep learning based, have been developed and deployed in industry. Among them, deep learning based recommender systems become increasingly popular due to their superior performance.   In this work, we conduct the first systematic study on data poisoning attacks to deep learning based recommender systems. An attacker's goal is to manipulate a recommender system such that the attacker-chosen target items are recommended to many users. To achieve this goal, our attack injects fake users with carefully crafted ratings to a recommender system. Specifically, we formulate our attack as an optimization problem, such that the injected ratings would maximize the number of normal users to whom the target items are recommended. However, it is challenging to solve the optimization problem because it is a non-convex integer programming problem. To address the challenge, we develop multiple techniques to approximately solve the optimization problem. Our experimental results on three real-world datasets, including small and large datasets, show that our attack is effective and outperforms existing attacks. Moreover, we attempt to detect fake users via statistical analysis of the rating patterns of normal and fake users. Our results show that our attack is still effective and outperforms existing attacks even if such a detector is deployed.",
      "year": 2021,
      "venue": "NDSS",
      "authors": [
        "Hai Huang",
        "Jiaming Mu",
        "Neil Zhenqiang Gong",
        "Qi Li",
        "Bin Liu",
        "Mingwei Xu"
      ],
      "url": "https://arxiv.org/abs/2101.02644",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_2e985c83",
      "title": "Reverse Attack: Black-box Attacks on Collaborative Recommendation",
      "abstract": "Collaborative filtering (CF) recommender systems have been extensively developed and widely deployed in various social websites, promoting products or services to the users of interest. Meanwhile, work has been attempted at poisoning attacks to CF recommender systems for distorting the recommend results to reap commercial or personal gains stealthily. While existing poisoning attacks have demonstrated their effectiveness with the offline social datasets, they are impractical when applied to the real setting on online social websites. This paper develops a novel and practical poisoning attack solution toward the CF recommender systems without knowing involved specific algorithms nor historical social data information a priori. Instead of directly attacking the unknown recommender systems, our solution performs certain operations on the social websites to collect a set of sampling data for use in constructing a surrogate model for deeply learning the inherent recommendation patterns. This surrogate model can estimate the item proximities, learned by the recommender systems. By attacking the surrogate model, the corresponding solutions (for availability and target attacks) can be directly migrated to attack the original recommender systems. Extensive experiments validate the generated surrogate model's reproductive capability and demonstrate the effectiveness of our attack upon various CF recommender algorithms.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Yihe Zhang",
        "Xu Yuan",
        "Jin Li",
        "Jiadong Lou",
        "Li Chen",
        "Nian-Feng Tzeng"
      ],
      "url": "https://openalex.org/W3214009246",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2006.14026",
      "title": "Subpopulation Data Poisoning Attacks",
      "abstract": "Machine learning systems are deployed in critical settings, but they might fail in unexpected ways, impacting the accuracy of their predictions. Poisoning attacks against machine learning induce adversarial modification of data used by a machine learning algorithm to selectively change its output when it is deployed. In this work, we introduce a novel data poisoning attack called a \\emph{subpopulation attack}, which is particularly relevant when datasets are large and diverse. We design a modular framework for subpopulation attacks, instantiate it with different building blocks, and show that the attacks are effective for a variety of datasets and machine learning models. We further optimize the attacks in continuous domains using influence functions and gradient optimization methods. Compared to existing backdoor poisoning attacks, subpopulation attacks have the advantage of inducing misclassification in naturally distributed data points at inference time, making the attacks extremely stealthy. We also show that our attack strategy can be used to improve upon existing targeted attacks. We prove that, under some assumptions, subpopulation attacks are impossible to defend against, and empirically demonstrate the limitations of existing defenses against our attacks, highlighting the difficulty of protecting machine learning against this threat.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Matthew Jagielski",
        "Giorgio Severi",
        "Niklas Pousette Harger",
        "Alina Oprea"
      ],
      "url": "https://arxiv.org/abs/2006.14026",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2111.04394",
      "title": "Get a Model! Model Hijacking Attack Against Machine Learning Models",
      "abstract": "Machine learning (ML) has established itself as a cornerstone for various critical applications ranging from autonomous driving to authentication systems. However, with this increasing adoption rate of machine learning models, multiple attacks have emerged. One class of such attacks is training time attack, whereby an adversary executes their attack before or during the machine learning model training. In this work, we propose a new training time attack against computer vision based machine learning models, namely model hijacking attack. The adversary aims to hijack a target model to execute a different task than its original one without the model owner noticing. Model hijacking can cause accountability and security risks since a hijacked model owner can be framed for having their model offering illegal or unethical services. Model hijacking attacks are launched in the same way as existing data poisoning attacks. However, one requirement of the model hijacking attack is to be stealthy, i.e., the data samples used to hijack the target model should look similar to the model's original training dataset. To this end, we propose two different model hijacking attacks, namely Chameleon and Adverse Chameleon, based on a novel encoder-decoder style ML model, namely the Camouflager. Our evaluation shows that both of our model hijacking attacks achieve a high attack success rate, with a negligible drop in model utility.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Ahmed Salem",
        "Michael Backes",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2111.04394",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_0c08f38b",
      "title": "PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning",
      "abstract": "Contrastive learning pre-trains an image encoder using a large amount of unlabeled data such that the image encoder can be used as a general-purpose feature extractor for various downstream tasks. In this work, we propose PoisonedEncoder, a data poisoning attack to contrastive learning. In particular, an attacker injects carefully crafted poisoning inputs into the unlabeled pre-training data, such that the downstream classifiers built based on the poisoned encoder for multiple target downstream tasks simultaneously classify attacker-chosen, arbitrary clean inputs as attacker-chosen, arbitrary classes. We formulate our data poisoning attack as a bilevel optimization problem, whose solution is the set of poisoning inputs; and we propose a contrastive-learning-tailored method to approximately solve it. Our evaluation on multiple datasets shows that PoisonedEncoder achieves high attack success rates while maintaining the testing accuracy of the downstream classifiers built upon the poisoned encoder for non-attacker-chosen inputs. We also evaluate five defenses against PoisonedEncoder, including one pre-processing, three in-processing, and one post-processing defenses. Our results show that these defenses can decrease the attack success rate of PoisonedEncoder, but they also sacrifice the utility of the encoder or require a large clean pre-training dataset.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Hongbin Liu",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://openalex.org/W4280514286",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2402.01920",
      "title": "Preference Poisoning Attacks on Reward Model Learning",
      "abstract": "Learning reward models from pairwise comparisons is a fundamental component in a number of domains, including autonomous control, conversational agents, and recommendation systems, as part of a broad goal of aligning automated decisions with user preferences. These approaches entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability by considering an attacker who can flip a small subset of preference comparisons to either promote or demote a target outcome. We propose two classes of algorithmic approaches for these attacks: a gradient-based framework, and several variants of rank-by-distance methods. Next, we evaluate the efficacy of best attacks in both these classes in successfully achieving malicious goals on datasets from three domains: autonomous control, recommendation system, and textual prompt-response preference learning. We find that the best attacks are often highly successful, achieving in the most extreme case 100\\% success rate with only 0.3\\% of the data poisoned. However, \\emph{which} attack is best can vary significantly across domains. In addition, we observe that the simpler and more scalable rank-by-distance approaches are often competitive with, and on occasion significantly outperform, gradient-based methods. Finally, we show that state-of-the-art defenses against other classes of poisoning attacks exhibit limited efficacy in our setting.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Junlin Wu",
        "Jiongxiao Wang",
        "Chaowei Xiao",
        "Chenguang Wang",
        "Ning Zhang",
        "Yevgeniy Vorobeychik"
      ],
      "url": "https://arxiv.org/abs/2402.01920",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2204.00032",
      "title": "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets",
      "abstract": "We introduce a new class of attacks on machine learning models. We show that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties. Our active inference attacks connect two independent lines of work targeting the integrity and privacy of machine learning training data.   Our attacks are effective across membership inference, attribute inference, and data extraction. For example, our targeted attacks can poison <0.1% of the training dataset to boost the performance of inference attacks by 1 to 2 orders of magnitude. Further, an adversary who controls a significant fraction of the training data (e.g., 50%) can launch untargeted attacks that enable 8x more precise inference on all other users' otherwise-private data points.   Our results cast doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Florian Tram\u00e8r",
        "Reza Shokri",
        "Ayrton San Joaquin",
        "Hoang Le",
        "Matthew Jagielski",
        "Sanghyun Hong",
        "Nicholas Carlini"
      ],
      "url": "https://arxiv.org/abs/2204.00032",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2308.08505",
      "title": "Test-Time Poisoning Attacks Against Test-Time Adaptation Models",
      "abstract": "Deploying machine learning (ML) models in the wild is challenging as it suffers from distribution shifts, where the model trained on an original domain cannot generalize well to unforeseen diverse transfer domains. To address this challenge, several test-time adaptation (TTA) methods have been proposed to improve the generalization ability of the target pre-trained models under test data to cope with the shifted distribution. The success of TTA can be credited to the continuous fine-tuning of the target model according to the distributional hint from the test samples during test time. Despite being powerful, it also opens a new attack surface, i.e., test-time poisoning attacks, which are substantially different from previous poisoning attacks that occur during the training time of ML models (i.e., adversaries cannot intervene in the training process). In this paper, we perform the first test-time poisoning attack against four mainstream TTA methods, including TTT, DUA, TENT, and RPL. Concretely, we generate poisoned samples based on the surrogate models and feed them to the target TTA models. Experimental results show that the TTA methods are generally vulnerable to test-time poisoning attacks. For instance, the adversary can feed as few as 10 poisoned samples to degrade the performance of the target model from 76.20% to 41.83%. Our results demonstrate that TTA algorithms lacking a rigorous security assessment are unsuitable for deployment in real-life scenarios. As such, we advocate for the integration of defenses against test-time poisoning attacks into the design of TTA methods.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Tianshuo Cong",
        "Xinlei He",
        "Yun Shen",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2308.08505",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_d2bc1e0d",
      "title": "Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks",
      "abstract": "In adversarial machine learning, new defenses against attacks on deep learning systems are routinely broken soon after their release by more powerful attacks. In this context, forensic tools can offer a valuable complement to existing defenses, by tracing back a successful attack to its root cause, and offering a path forward for mitigation to prevent similar attacks in the future. In this paper, we describe our efforts in developing a forensic traceback tool for poison attacks on deep neural networks. We propose a novel iterative clustering and pruning solution that trims \"innocent\" training samples, until all that remains is the set of poisoned data responsible for the attack. Our method clusters training samples based on their impact on model parameters, then uses an efficient data unlearning method to prune innocent clusters. We empirically demonstrate the efficacy of our system on three types of dirty-label (backdoor) poison attacks and three types of clean-label poison attacks, across domains of computer vision and malware classification. Our system achieves over 98.4% precision and 96.8% recall across all attacks. We also show that our system is robust against four anti-forensics measures specifically designed to attack it.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Shawn Shan",
        "Arjun Nitin Bhagoji",
        "Hai-Tao Zheng",
        "Ben Y. Zhao"
      ],
      "url": "https://openalex.org/W4286904975",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2409.12314",
      "title": "Understanding Implosion in Text-to-Image Generative Models",
      "abstract": "Recent works show that text-to-image generative models are surprisingly vulnerable to a variety of poisoning attacks. Empirical results find that these models can be corrupted by altering associations between individual text prompts and associated visual features. Furthermore, a number of concurrent poisoning attacks can induce \"model implosion,\" where the model becomes unable to produce meaningful images for unpoisoned prompts. These intriguing findings highlight the absence of an intuitive framework to understand poisoning attacks on these models. In this work, we establish the first analytical framework on robustness of image generative models to poisoning attacks, by modeling and analyzing the behavior of the cross-attention mechanism in latent diffusion models. We model cross-attention training as an abstract problem of \"supervised graph alignment\" and formally quantify the impact of training data by the hardness of alignment, measured by an Alignment Difficulty (AD) metric. The higher the AD, the harder the alignment. We prove that AD increases with the number of individual prompts (or concepts) poisoned. As AD grows, the alignment task becomes increasingly difficult, yielding highly distorted outcomes that frequently map meaningful text prompts to undefined or meaningless visual representations. As a result, the generative model implodes and outputs random, incoherent images at large. We validate our analytical framework through extensive experiments, and we confirm and explain the unexpected (and unexplained) effect of model implosion while producing new, unforeseen insights. Our work provides a useful tool for studying poisoning attacks against diffusion models and their defenses.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Wenxin Ding",
        "Cathy Y. Li",
        "Shawn Shan",
        "Ben Y. Zhao",
        "Haitao Zheng"
      ],
      "url": "https://arxiv.org/abs/2409.12314",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_2b8ed5cf",
      "title": "Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection",
      "abstract": "A security threat to deep neural networks (DNN) is backdoor contamination, in which an adversary poisons the training data of a target model to inject a Trojan so that images carrying a specific trigger will always be classified into a specific label. Prior research on this problem assumes the dominance of the trigger in an image's representation, which causes any image with the trigger to be recognized as a member in the target class. Such a trigger also exhibits unique features in the representation space and can therefore be easily separated from legitimate images. Our research, however, shows that simple target contamination can cause the representation of an attack image to be less distinguishable from that of legitimate ones, thereby evading existing defenses against the backdoor infection. In our research, we show that such a contamination attack actually subtly changes the representation distribution for the target class, which can be captured by a statistic analysis. More specifically, we leverage an EM algorithm to decompose an image into its identity part (e.g., person, traffic sign) and variation part within a class (e.g., lighting, poses). Then we analyze the distribution in each class, identifying those more likely to be characterized by a mixture model resulted from adding attack samples to the legitimate image pool. Our research shows that this new technique effectively detects data contamination attacks, including the new one we propose, and is also robust against the evasion attempts made by a knowledgeable adversary.",
      "year": 2019,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Di Tang",
        "Xiaofeng Wang",
        "Haixu Tang",
        "Kehuan Zhang"
      ],
      "url": "https://openalex.org/W2965527544",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "1910.03137",
      "title": "Detecting AI Trojans Using Meta Neural Analysis",
      "abstract": "In machine learning Trojan attacks, an adversary trains a corrupted model that obtains good performance on normal data but behaves maliciously on data samples with certain trigger patterns. Several approaches have been proposed to detect such attacks, but they make undesirable assumptions about the attack strategies or require direct access to the trained models, which restricts their utility in practice.   This paper addresses these challenges by introducing a Meta Neural Trojan Detection (MNTD) pipeline that does not make assumptions on the attack strategies and only needs black-box access to models. The strategy is to train a meta-classifier that predicts whether a given target model is Trojaned. To train the meta-model without knowledge of the attack strategy, we introduce a technique called jumbo learning that samples a set of Trojaned models following a general distribution. We then dynamically optimize a query set together with the meta-classifier to distinguish between Trojaned and benign models.   We evaluate MNTD with experiments on vision, speech, tabular data and natural language text datasets, and against different Trojan attacks such as data poisoning attack, model manipulation attack, and latent attack. We show that MNTD achieves 97% detection AUC score and significantly outperforms existing detection approaches. In addition, MNTD generalizes well and achieves high detection performance against unforeseen attacks. We also propose a robust MNTD pipeline which achieves 90% detection AUC even when the attacker aims to evade the detection with full knowledge of the system.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Xiaojun Xu",
        "Qi Wang",
        "Huichen Li",
        "Nikita Borisov",
        "Carl A. Gunter",
        "Bo Li"
      ],
      "url": "https://arxiv.org/abs/1910.03137",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2108.00352",
      "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning",
      "abstract": "Self-supervised learning in computer vision aims to pre-train an image encoder using a large amount of unlabeled images or (image, text) pairs. The pre-trained image encoder can then be used as a feature extractor to build downstream classifiers for many downstream tasks with a small amount of or no labeled training data. In this work, we propose BadEncoder, the first backdoor attack to self-supervised learning. In particular, our BadEncoder injects backdoors into a pre-trained image encoder such that the downstream classifiers built based on the backdoored image encoder for different downstream tasks simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an optimization problem and we propose a gradient descent based method to solve it, which produces a backdoored image encoder from a clean one. Our extensive empirical evaluation results on multiple datasets show that our BadEncoder achieves high attack success rates while preserving the accuracy of the downstream classifiers. We also show the effectiveness of BadEncoder using two publicly available, real-world image encoders, i.e., Google's image encoder pre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training (CLIP) image encoder pre-trained on 400 million (image, text) pairs collected from the Internet. Moreover, we consider defenses including Neural Cleanse and MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our results show that these defenses are insufficient to defend against BadEncoder, highlighting the needs for new defenses against our BadEncoder. Our code is publicly available at: https://github.com/jjy1994/BadEncoder.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Jinyuan Jia",
        "Yupei Liu",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://arxiv.org/abs/2108.00352",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_bbc1dc00",
      "title": "Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features",
      "abstract": "With the prevalent use of Deep Neural Networks (DNNs) in many applications, security of these networks is of importance. Pre-trained DNNs may contain backdoors that are injected through poisoned training. These trojaned models perform well when regular inputs are provided, but misclassify to a target output label when the input is stamped with a unique pattern called trojan trigger. Recently various backdoor detection and mitigation systems for DNN based AI applications have been proposed. However, many of them are limited to trojan attacks that require a specific patch trigger. In this paper, we introduce composite attack, a more flexible and stealthy trojan attack that eludes backdoor scanners using trojan triggers composed from existing benign features of multiple labels. We show that a neural network with a composed backdoor can achieve accuracy comparable to its original version on benign data and misclassifies when the composite trigger is present in the input. Our experiments on 7 different tasks show that this attack poses a severe threat. We evaluate our attack with two state-of-the-art backdoor scanners. The results show none of the injected backdoors can be detected by either scanner. We also study in details why the scanners are not effective. In the end, we discuss the essence of our attack and propose possible defense.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Junyu Lin",
        "Lei Xu",
        "Yingqi Liu",
        "Xiangyu Zhang"
      ],
      "url": "https://openalex.org/W3106646114",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_b13c9bc7",
      "title": "LoneNeuron: a Highly-Effective Feature-Domain Neural Trojan Using Invisible and Polymorphic Watermarks",
      "abstract": "The wide adoption of deep neural networks (DNNs) in real-world applications raises increasing security concerns. Neural Trojans embedded in pre-trained neural networks are a harmful attack against the DNN model supply chain. They generate false outputs when certain stealthy triggers appear in the inputs. While data-poisoning attacks have been well studied in the literature, code-poisoning and model-poisoning backdoors only start to attract attention until recently. We present a novel model-poisoning neural Trojan, namely LoneNeuron, which responds to feature-domain patterns that transform into invisible, sample-specific, and polymorphic pixel-domain watermarks. With high attack specificity, LoneNeuron achieves a 100% attack success rate, while not affecting the main task performance. With LoneNeuron's unique watermark polymorphism property, the same feature-domain trigger is resolved to multiple watermarks in the pixel domain, which further improves watermark randomness, stealthiness, and resistance against Trojan detection. Extensive experiments show that LoneNeuron could escape state-of-the-art Trojan detectors. LoneNeuron~is also the first effective backdoor attack against vision transformers (ViTs).",
      "year": 2022,
      "venue": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
      "authors": [
        "Zeyan Liu",
        "Fengjun Li",
        "Zhu Li",
        "Bo Luo"
      ],
      "url": "https://openalex.org/W4308338624",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_e67f1280",
      "title": "ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks",
      "abstract": "Deep neural networks have achieved remarkable success on a variety of mission-critical tasks.However, recent studies show that deep neural networks are vulnerable to backdoor attacks, where the attacker releases backdoored models that behave normally on benign samples but misclassify any trigger-imposed samples to a target label.Unlike adversarial examples, backdoor attacks manipulate both the inputs and the model, perturbing samples with the trigger and injecting backdoors into the model.In this paper, we propose a novel attention-based evasive backdoor attack, dubbed ATTEQ-NN.Different from existing works that arbitrarily set the trigger mask, we carefully design an attentionbased trigger mask determination framework, which places the trigger at the crucial region with the most significant influence on the prediction results.To make the trigger-imposed samples appear more natural and imperceptible to human inspectors, we introduce a Quality-of-Experience (QoE) term into the loss function of trigger generation and carefully adjust the transparency of the trigger.During the process of iteratively optimizing the trigger generation and the backdoor injection components, we propose an alternating retraining strategy, which is shown to be effective in improving the clean data accuracy and evading some model-based defense approaches.We evaluate ATTEQ-NN with extensive experiments on VGG-Flower, CIFAR-10, GTSRB, CIFAR-100, and ImageNette datasets.The results show that ATTEQ-NN can increase the attack success rate by as much as 82% over baselines when the poison ratio is low while achieving a high QoE of the backdoored samples.We demonstrate that ATTEQ-NN reaches an attack success rate of more than 37.78% in the physical world under different lighting conditions and shooting angles.ATTEQ-NN preserves an attack success rate of more than 92.5% even if the original backdoored model is fine-tuned with clean data.It is shown that ATTEQ-NN is also effective in transfer learning scenarios.Our user studies show that the backdoored samples generated by ATTEQ-NN are indiscernible under visual inspections.ATTEQ-NN is shown to be evasive to state-of-the-art defense methods, including model pruning, NAD, STRIP, NC, and MNTD.We will open-source our codes upon publication.",
      "year": 2022,
      "venue": null,
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Jianshuo Dong",
        "Qian Wang"
      ],
      "url": "https://openalex.org/W4226550712",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2003.08904",
      "title": "RAB: Provable Robustness Against Backdoor Attacks",
      "abstract": "Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible to train the robust smoothed models efficiently for simple models such as K-nearest neighbor classifiers, and we propose an exact smooth-training algorithm that eliminates the need to sample from a noise distribution for such models. Empirically, we conduct comprehensive experiments for different machine learning (ML) models such as DNNs, support vector machines, and K-NN models on MNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark for certified robustness against backdoor attacks. In addition, we evaluate K-NN models on a spambase tabular dataset to demonstrate the advantages of the proposed exact algorithm. Both the theoretic analysis and the comprehensive evaluation on diverse ML models and datasets shed light on further robust learning strategies against general training time attacks.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Maurice Weber",
        "Xiaojun Xu",
        "Bojan Karla\u0161",
        "Ce Zhang",
        "Bo Li"
      ],
      "url": "https://arxiv.org/abs/2003.08904",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2301.01197",
      "title": "Backdoor Attacks Against Dataset Distillation",
      "abstract": "Dataset distillation has emerged as a prominent technique to improve data efficiency when training machine learning models. It encapsulates the knowledge from a large dataset into a smaller synthetic dataset. A model trained on this smaller distilled dataset can attain comparable performance to a model trained on the original training dataset. However, the existing dataset distillation techniques mainly aim at achieving the best trade-off between resource usage efficiency and model utility. The security risks stemming from them have not been explored. This study performs the first backdoor attack against the models trained on the data distilled by dataset distillation models in the image domain. Concretely, we inject triggers into the synthetic data during the distillation procedure rather than during the model training stage, where all previous attacks are performed. We propose two types of backdoor attacks, namely NAIVEATTACK and DOORPING. NAIVEATTACK simply adds triggers to the raw data at the initial distillation phase, while DOORPING iteratively updates the triggers during the entire distillation procedure. We conduct extensive evaluations on multiple datasets, architectures, and dataset distillation techniques. Empirical evaluation shows that NAIVEATTACK achieves decent attack success rate (ASR) scores in some cases, while DOORPING reaches higher ASR scores (close to 1.0) in all cases. Furthermore, we conduct a comprehensive ablation study to analyze the factors that may affect the attack performance. Finally, we evaluate multiple defense mechanisms against our backdoor attacks and show that our attacks can practically circumvent these defense mechanisms.",
      "year": 2023,
      "venue": "NDSS",
      "authors": [
        "Yugeng Liu",
        "Zheng Li",
        "Michael Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2301.01197",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2301.06241",
      "title": "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense",
      "abstract": "Deep Learning backdoor attacks have a threat model similar to traditional cyber attacks. Attack forensics, a critical counter-measure for traditional cyber attacks, is hence of importance for defending model backdoor attacks. In this paper, we propose a novel model backdoor forensics technique. Given a few attack samples such as inputs with backdoor triggers, which may represent different types of backdoors, our technique automatically decomposes them to clean inputs and the corresponding triggers. It then clusters the triggers based on their properties to allow automatic attack categorization and summarization. Backdoor scanners can then be automatically synthesized to find other instances of the same type of backdoor in other models. Our evaluation on 2,532 pre-trained models, 10 popular attacks, and comparison with 9 baselines show that our technique is highly effective. The decomposed clean inputs and triggers closely resemble the ground truth. The synthesized scanners substantially outperform the vanilla versions of existing scanners that can hardly generalize to different kinds of attacks.",
      "year": 2023,
      "venue": "NDSS",
      "authors": [
        "Siyuan Cheng",
        "Guanhong Tao",
        "Yingqi Liu",
        "Shengwei An",
        "Xiangzhe Xu",
        "Shiwei Feng",
        "Guangyu Shen",
        "Kaiyuan Zhang",
        "Qiuling Xu",
        "Shiqing Ma",
        "Xiangyu Zhang"
      ],
      "url": "https://arxiv.org/abs/2301.06241",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_2d0aee9f",
      "title": "Disguising Attacks with Explanation-Aware Backdoors",
      "abstract": "Explainable machine learning holds great potential for analyzing and understanding learning-based systems. These methods can, however, be manipulated to present unfaithful explanations, giving rise to powerful and stealthy adversaries. In this paper, we demonstrate how to fully disguise the adversarial operation of a machine learning model. Similar to neural backdoors, we change the model's prediction upon trigger presence but simultaneously fool an explanation method that is applied post-hoc for analysis. This enables an adversary to hide the presence of the trigger or point the explanation to entirely different portions of the input, throwing a red herring. We analyze different manifestations of these explanation-aware backdoors for gradient- and propagation-based explanation methods in the image domain, before we resume to conduct a red-herring attack against malware classification.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Maximilian Noppel",
        "Luk\u00e1\u0161 Peter",
        "Christian Wressnegger"
      ],
      "url": "https://openalex.org/W4385080308",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2212.04687",
      "title": "Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models",
      "abstract": "In this paper, we present a simple yet surprisingly effective technique to induce \"selective amnesia\" on a backdoored model. Our approach, called SEAM, has been inspired by the problem of catastrophic forgetting (CF), a long standing issue in continual learning. Our idea is to retrain a given DNN model on randomly labeled clean data, to induce a CF on the model, leading to a sudden forget on both primary and backdoor tasks; then we recover the primary task by retraining the randomized model on correctly labeled clean data. We analyzed SEAM by modeling the unlearning process as continual learning and further approximating a DNN using Neural Tangent Kernel for measuring CF. Our analysis shows that our random-labeling approach actually maximizes the CF on an unknown backdoor in the absence of triggered inputs, and also preserves some feature extraction in the network to enable a fast revival of the primary task. We further evaluated SEAM on both image processing and Natural Language Processing tasks, under both data contamination and training manipulation attacks, over thousands of models either trained on popular image datasets or provided by the TrojAI competition. Our experiments show that SEAM vastly outperforms the state-of-the-art unlearning techniques, achieving a high Fidelity (measuring the gap between the accuracy of the primary task and that of the backdoor) within a few minutes (about 30 times faster than training a model from scratch using the MNIST dataset), with only a small amount of clean data (0.1% of training data for TrojAI models).",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Rui Zhu",
        "Di Tang",
        "Siyuan Tang",
        "XiaoFeng Wang",
        "Haixu Tang"
      ],
      "url": "https://arxiv.org/abs/2212.04687",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_4e4fdabc",
      "title": "REDEEM MYSELF: Purifying Backdoors in Deep Learning Models using Self Attention Distillation",
      "abstract": "Recent works have revealed the vulnerability of deep neural networks to backdoor attacks, where a backdoored model orchestrates targeted or untargeted misclassification when activated by a trigger. A line of purification methods (e.g., fine-pruning, neural attention transfer, MCR [69]) have been proposed to remove the backdoor in a model. However, they either fail to reduce the attack success rate of more advanced backdoor attacks or largely degrade the prediction capacity of the model for clean samples. In this paper, we put forward a new purification defense framework, dubbed SAGE, which utilizes self-attention distillation to purge models of backdoors. Unlike traditional attention transfer mechanisms that require a teacher model to supervise the distillation process, SAGE can realize self-purification with a small number of clean samples. To enhance the defense performance, we further propose a dynamic learning rate adjustment strategy that carefully tracks the prediction accuracy of clean samples to guide the learning rate adjustment. We compare the defense performance of SAGE with 6 state-of-the-art defense approaches against 8 backdoor attacks on 4 datasets. It is shown that SAGE can reduce the attack success rate by as much as 90% with less than 3% decrease in prediction accuracy for clean samples. We will open-source our codes upon publication.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Yang Wang",
        "Qian Wang",
        "Yuzhe Gu",
        "Huayang Huang",
        "Chao Shen"
      ],
      "url": "https://openalex.org/W4385187298",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2204.05255",
      "title": "NARCISSUS: A Practical Clean-Label Backdoor Attack with Limited Information",
      "abstract": "Backdoor attacks insert malicious data into a training set so that, during inference time, it misclassifies inputs that have been patched with a backdoor trigger as the malware specified label. For backdoor attacks to bypass human inspection, it is essential that the injected data appear to be correctly labeled. The attacks with such property are often referred to as \"clean-label attacks.\" Existing clean-label backdoor attacks require knowledge of the entire training set to be effective. Obtaining such knowledge is difficult or impossible because training data are often gathered from multiple sources (e.g., face images from different users). It remains a question whether backdoor attacks still present a real threat.   This paper provides an affirmative answer to this question by designing an algorithm to mount clean-label backdoor attacks based only on the knowledge of representative examples from the target class. With poisoning equal to or less than 0.5% of the target-class data and 0.05% of the training set, we can train a model to classify test examples from arbitrary classes into the target class when the examples are patched with a backdoor trigger. Our attack works well across datasets and models, even when the trigger presents in the physical world.   We explore the space of defenses and find that, surprisingly, our attack can evade the latest state-of-the-art defenses in their vanilla form, or after a simple twist, we can adapt to the downstream defenses. We study the cause of the intriguing effectiveness and find that because the trigger synthesized by our attack contains features as persistent as the original semantic features of the target class, any attempt to remove such triggers would inevitably hurt the model accuracy first.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Yi Zeng",
        "Minzhou Pan",
        "Hoang Anh Just",
        "Lingjuan Lyu",
        "Meikang Qiu",
        "Ruoxi Jia"
      ],
      "url": "https://arxiv.org/abs/2204.05255",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_dddf388a",
      "title": "ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms",
      "abstract": "Backdoor data detection is traditionally studied in an end-to-end supervised learning (SL) setting. However, recent years have seen the proliferating adoption of self-supervised learning (SSL) and transfer learning (TL), due to their lesser need for labeled data. Successful backdoor attacks have also been demonstrated in these new settings. However, we lack a thorough understanding of the applicability of existing detection methods across a variety of learning settings. By evaluating 56 attack settings, we show that the performance of most existing detection methods varies significantly across different attacks and poison ratios, and all fail on the state-of-the-art clean-label attack. In addition, they either become inapplicable or suffer large performance losses when applied to SSL and TL. We propose a new detection method called Active Separation via Offset (ASSET), which actively induces different model behaviors between the backdoor and clean samples to promote their separation. We also provide procedures to adaptively select the number of suspicious points to remove. In the end-to-end SL setting, ASSET is superior to existing methods in terms of consistency of defensive performance across different attacks and robustness to changes in poison ratios; in particular, it is the only method that can detect the state-of-the-art clean-label attack. Moreover, ASSET's average detection rates are higher than the best existing methods in SSL and TL, respectively, by 69.3% and 33.2%, thus providing the first practical backdoor defense for these new DL settings. We open-source the project to drive further development and encourage engagement: https://github.com/ruoxi-jia-group/ASSET.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Minzhou Pan",
        "Yi Zeng",
        "Lingjuan Lyu",
        "Xue Lin",
        "Ruoxi Jia"
      ],
      "url": "https://openalex.org/W4321649939",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_27e87237",
      "title": "ODSCAN: Backdoor Scanning for Object Detection Models",
      "abstract": "A backdoor attack in deep learning inserts a hidden backdoor in the model to trigger malicious behavior upon specific input patterns. Existing detection approaches assume a metric space (for either the original inputs or their latent representations) in which normal samples and malicious samples are separable. We show that this assumption has a severe limitation by introducing a novel SSDT (Source-Specific and Dynamic-Triggers) backdoor, which obscures the difference between normal samples and malicious samples.   To overcome this limitation, we move beyond looking for a perfect metric space that would work for different deep-learning models, and instead resort to more robust topological constructs. We propose TED (Topological Evolution Dynamics) as a model-agnostic basis for robust backdoor detection. The main idea of TED is to view a deep-learning model as a dynamical system that evolves inputs to outputs. In such a dynamical system, a benign input follows a natural evolution trajectory similar to other benign inputs. In contrast, a malicious sample displays a distinct trajectory, since it starts close to benign samples but eventually shifts towards the neighborhood of attacker-specified target samples to activate the backdoor.   Extensive evaluations are conducted on vision and natural language datasets across different network architectures. The results demonstrate that TED not only achieves a high detection rate, but also significantly outperforms existing state-of-the-art detection approaches, particularly in addressing the sophisticated SSDT attack. The code to reproduce the results is made public on GitHub.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Xiaoxing Mo",
        "Yechao Zhang",
        "Leo Yu Zhang",
        "Wei Luo",
        "Nan Sun",
        "Shengshan Hu",
        "Shang Gao",
        "Yang Xiang"
      ],
      "url": "https://arxiv.org/abs/2312.02673",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2205.06900",
      "title": "MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic",
      "abstract": "Backdoor attacks are an important type of adversarial threat against deep neural network classifiers, wherein test samples from one or more source classes will be (mis)classified to the attacker's target class when a backdoor pattern is embedded. In this paper, we focus on the post-training backdoor defense scenario commonly considered in the literature, where the defender aims to detect whether a trained classifier was backdoor-attacked without any access to the training set. Many post-training detectors are designed to detect attacks that use either one or a few specific backdoor embedding functions (e.g., patch-replacement or additive attacks). These detectors may fail when the backdoor embedding function used by the attacker (unknown to the defender) is different from the backdoor embedding function assumed by the defender. In contrast, we propose a post-training defense that detects backdoor attacks with arbitrary types of backdoor embeddings, without making any assumptions about the backdoor embedding type. Our detector leverages the influence of the backdoor attack, independent of the backdoor embedding mechanism, on the landscape of the classifier's outputs prior to the softmax layer. For each class, a maximum margin statistic is estimated. Detection inference is then performed by applying an unsupervised anomaly detector to these statistics. Thus, our detector does not need any legitimate clean samples, and can efficiently detect backdoor attacks with arbitrary numbers of source classes. These advantages over several state-of-the-art methods are demonstrated on four datasets, for three different types of backdoor patterns, and for a variety of attack configurations. Finally, we propose a novel, general approach for backdoor mitigation once a detection is made. The mitigation approach was the runner-up at the first IEEE Trojan Removal Competition. The code is online available.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Hang Wang",
        "Zhen Xiang",
        "David J. Miller",
        "George Kesidis"
      ],
      "url": "https://arxiv.org/abs/2205.06900",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_8a38247d",
      "title": "Backdooring Bias (B^2) into Stable Diffusion Models",
      "abstract": "Recent advances in large text-conditional diffusion models have revolutionized image generation by enabling users to create realistic, high-quality images from textual prompts, significantly enhancing artistic creation and visual communication. However, these advancements also introduce an underexplored attack opportunity: the possibility of inducing biases by an adversary into the generated images for malicious intentions, e.g., to influence public opinion and spread propaganda. In this paper, we study an attack vector that allows an adversary to inject arbitrary bias into a target model. The attack leverages low-cost backdooring techniques using a targeted set of natural textual triggers embedded within a small number of malicious data samples produced with public generative models. An adversary could pick common sequences of words that can then be inadvertently activated by benign users during inference. We investigate the feasibility and challenges of such attacks, demonstrating how modern generative models have made this adversarial process both easier and more adaptable. On the other hand, we explore various aspects of the detectability of such attacks and demonstrate that the model's utility remains intact in the absence of the triggers. Our extensive experiments using over 200,000 generated images and against hundreds of fine-tuned models demonstrate the feasibility of the presented backdoor attack. We illustrate how these biases maintain strong text-image alignment, highlighting the challenges in detecting biased images without knowing that bias in advance. Our cost analysis confirms the low financial barrier (\\$10-\\$15) to executing such attacks, underscoring the need for robust defensive strategies against such vulnerabilities in diffusion models.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Ali Naseh",
        "Jaechul Roh",
        "Eugene Bagdasaryan",
        "Amir Houmansadr"
      ],
      "url": "https://openalex.org/W4399991166",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_ae5a938e",
      "title": "Rowhammer-Based Trojan Injection: One Bit Flip Is Sufficient for Backdooring DNNs",
      "abstract": "State-of-the-art deep neural networks (DNNs) have been proven to be vulnerable to adversarial manipulation and backdoor attacks. Backdoored models deviate from expected behavior on inputs with predefined triggers while retaining performance on clean data. Recent works focus on software simulation of backdoor injection during the inference phase by modifying network weights, which we find often unrealistic in practice due to restrictions in hardware. In contrast, in this work for the first time, we present an end-to-end backdoor injection attack realized on actual hardware on a classifier model using Rowhammer as the fault injection method. To this end, we first investigate the viability of backdoor injection attacks in real-life deployments of DNNs on hardware and address such practical issues in hardware implementation from a novel optimization perspective. We are motivated by the fact that vulnerable memory locations are very rare, device-specific, and sparsely distributed. Consequently, we propose a novel network training algorithm based on constrained optimization to achieve a realistic backdoor injection attack in hardware. By modifying parameters uniformly across the convolutional and fully-connected layers as well as optimizing the trigger pattern together, we achieve state-of-the-art attack performance with fewer bit flips. For instance, our method on a hardware-deployed ResNet-20 model trained on CIFAR-10 achieves over 89% test accuracy and 92% attack success rate by flipping only 10 out of 2.2 million bits.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "M. Caner Tol",
        "Saad Islam",
        "Andrew Adiletta",
        "Berk Sunar",
        "Ziming Zhang"
      ],
      "url": "https://openalex.org/W4286904258",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_c385cdc7",
      "title": "Revisiting Training-Inference Trigger Intensity in Backdoor Attacks",
      "abstract": "Contains fulltext : 310103.pdf (Publisher\u2019s version ) (Open Access)",
      "year": 2024,
      "venue": null,
      "authors": [
        "Gorka Abad",
        "O\u011fuzhan Ersoy",
        "Stjepan Picek",
        "Aitor Urbieta"
      ],
      "url": "https://openalex.org/W4391725253",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_48a59125",
      "title": "Persistent Backdoor Attacks in Continual Learning",
      "abstract": "Backdoor attacks pose a significant threat to neural networks, enabling adversaries to manipulate model outputs on specific inputs, often with devastating consequences, especially in critical applications. While backdoor attacks have been studied in various contexts, little attention has been given to their practicality and persistence in continual learning, particularly in understanding how the continual updates to model parameters, as new data distributions are learned and integrated, impact the effectiveness of these attacks over time. To address this gap, we introduce two persistent backdoor attacks-Blind Task Backdoor and Latent Task Backdoor-each leveraging minimal adversarial influence. Our blind task backdoor subtly alters the loss computation without direct control over the training process, while the latent task backdoor influences only a single task's training, with all other tasks trained benignly. We evaluate these attacks under various configurations, demonstrating their efficacy with static, dynamic, physical, and semantic triggers. Our results show that both attacks consistently achieve high success rates across different continual learning algorithms, while effectively evading state-of-the-art defenses, such as SentiNet and I-BAU.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Guo Zhen",
        "Abhinav Kumar",
        "Reza Tourani"
      ],
      "url": "https://openalex.org/W4403752757",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_048d0c77",
      "title": "T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification",
      "abstract": "Deep Neural Network (DNN) classifiers are known to be vulnerable to Trojan or backdoor attacks, where the classifier is manipulated such that it misclassifies any input containing an attacker-determined Trojan trigger. Backdoors compromise a model's integrity, thereby posing a severe threat to the landscape of DNN-based classification. While multiple defenses against such attacks exist for classifiers in the image domain, there have been limited efforts to protect classifiers in the text domain. We present Trojan-Miner (T-Miner) -- a defense framework for Trojan attacks on DNN-based text classifiers. T-Miner employs a sequence-to-sequence (seq-2-seq) generative model that probes the suspicious classifier and learns to produce text sequences that are likely to contain the Trojan trigger. T-Miner then analyzes the text produced by the generative model to determine if they contain trigger phrases, and correspondingly, whether the tested classifier has a backdoor. T-Miner requires no access to the training dataset or clean inputs of the suspicious classifier, and instead uses synthetically crafted \"nonsensical\" text inputs to train the generative model. We extensively evaluate T-Miner on 1100 model instances spanning 3 ubiquitous DNN model architectures, 5 different classification tasks, and a variety of trigger phrases. We show that T-Miner detects Trojan and clean models with a 98.75% overall accuracy, while achieving low false positives on clean models. We also show that T-Miner is robust against a variety of targeted, advanced attacks from an adaptive attacker.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Ahmadreza Azizi",
        "Ibrahim Asadullah Tahmid",
        "Asim Waheed",
        "Neal Mangaokar",
        "Jiameng Pu",
        "Mobin Javed",
        "Chandan K. Reddy",
        "Bimal Viswanath"
      ],
      "url": "https://openalex.org/W3135366566",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2105.00164",
      "title": "Hidden Backdoors in Human-Centric Language Models",
      "abstract": "Natural language processing (NLP) systems have been proven to be vulnerable to backdoor attacks, whereby hidden features (backdoors) are trained into a language model and may only be activated by specific inputs (called triggers), to trick the model into producing unexpected behaviors. In this paper, we create covert and natural triggers for textual backdoor attacks, \\textit{hidden backdoors}, where triggers can fool both modern language models and human inspection. We deploy our hidden backdoors through two state-of-the-art trigger embedding methods. The first approach via homograph replacement, embeds the trigger into deep neural networks through the visual spoofing of lookalike character replacement. The second approach uses subtle differences between text generated by language models and real natural text to produce trigger sentences with correct grammar and high fluency. We demonstrate that the proposed hidden backdoors can be effective across three downstream security-critical NLP tasks, representative of modern human-centric NLP systems, including toxic comment detection, neural machine translation (NMT), and question answering (QA). Our two hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at least $97\\%$ with an injection rate of only $3\\%$ in toxic comment detection, $95.1\\%$ ASR in NMT with less than $0.5\\%$ injected data, and finally $91.12\\%$ ASR against QA updated with only 27 poisoning data samples on a model previously trained with 92,024 samples (0.029\\%). We are able to demonstrate the adversary's high success rate of attacks, while maintaining functionality for regular users, with triggers inconspicuous by the human administrators.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Shaofeng Li",
        "Hui Liu",
        "Tian Dong",
        "Benjamin Zi Hao Zhao",
        "Minhui Xue",
        "Haojin Zhu",
        "Jialiang Lu"
      ],
      "url": "https://arxiv.org/abs/2105.00164",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2111.00197",
      "title": "Backdoor Pre-trained Models Can Transfer to All",
      "abstract": "Pre-trained general-purpose language models have been a dominating component in enabling real-world natural language processing (NLP) applications. However, a pre-trained model with backdoor can be a severe threat to the applications. Most existing backdoor attacks in NLP are conducted in the fine-tuning phase by introducing malicious triggers in the targeted class, thus relying greatly on the prior knowledge of the fine-tuning task. In this paper, we propose a new approach to map the inputs containing triggers directly to a predefined output representation of the pre-trained NLP models, e.g., a predefined output representation for the classification token in BERT, instead of a target label. It can thus introduce backdoor to a wide range of downstream tasks without any prior knowledge. Additionally, in light of the unique properties of triggers in NLP, we propose two new metrics to measure the performance of backdoor attacks in terms of both effectiveness and stealthiness. Our experiments with various types of triggers show that our method is widely applicable to different fine-tuning tasks (classification and named entity recognition) and to different models (such as BERT, XLNet, BART), which poses a severe threat. Furthermore, by collaborating with the popular online model repository Hugging Face, the threat brought by our method has been confirmed. Finally, we analyze the factors that may affect the attack performance and share insights on the causes of the success of our backdoor attack.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Lujia Shen",
        "Shouling Ji",
        "Xuhong Zhang",
        "Jinfeng Li",
        "Jing Chen",
        "Jie Shi",
        "Chengfang Fang",
        "Jianwei Yin",
        "Ting Wang"
      ],
      "url": "https://arxiv.org/abs/2111.00197",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_afd91528",
      "title": "Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation",
      "abstract": "Metaverse is expected to emerge as a new paradigm for the next-generation Internet, providing fully immersive and personalized experiences to socialize, work, and play in self-sustaining and hyper-spatio-temporal virtual world(s). The advancements in different technologies such as augmented reality, virtual reality, extended reality (XR), artificial intelligence (AI), and 5G/6G communication will be the key enablers behind the realization of AI-XR metaverse applications. While AI itself has many potential applications in the aforementioned technologies (e.g., avatar generation, network optimization), ensuring the security of AI in critical applications like AI-XR metaverse applications is profoundly crucial to avoid undesirable actions that could undermine users\u2019 privacy and safety, consequently putting their lives in danger. To this end, we attempt to analyze the security, privacy, and trustworthiness aspects associated with the use of various AI techniques in AI-XR metaverse applications. Specifically, we discuss numerous such challenges and present a taxonomy of potential solutions that could be leveraged to develop secure, private, robust, and trustworthy AI-XR applications. To highlight the real implications of AI-associated adversarial threats, we designed a metaverse-specific case study and analyzed it through the adversarial lens. Finally, we elaborate upon various open issues that require further research interest from the community.",
      "year": 2023,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Adnan Qayyum",
        "Muhammad Atif Butt",
        "Hassan Ali",
        "Muhammad Usman",
        "Osama Halabi",
        "Ala Al\u2010Fuqaha",
        "Qammer H. Abbasi",
        "Muhammad Ali Imran",
        "Junaid Qadir"
      ],
      "url": "https://openalex.org/W4385724403",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2311.11225",
      "title": "TextGuard: Provable Defense against Backdoor Attacks on Text Classification",
      "abstract": "Backdoor attacks have become a major security threat for deploying machine learning models in security-critical applications. Existing research endeavors have proposed many defenses against backdoor attacks. Despite demonstrating certain empirical defense efficacy, none of these techniques could provide a formal and provable security guarantee against arbitrary attacks. As a result, they can be easily broken by strong adaptive attacks, as shown in our evaluation. In this work, we propose TextGuard, the first provable defense against backdoor attacks on text classification. In particular, TextGuard first divides the (backdoored) training data into sub-training sets, achieved by splitting each training sentence into sub-sentences. This partitioning ensures that a majority of the sub-training sets do not contain the backdoor trigger. Subsequently, a base classifier is trained from each sub-training set, and their ensemble provides the final prediction. We theoretically prove that when the length of the backdoor trigger falls within a certain threshold, TextGuard guarantees that its prediction will remain unaffected by the presence of the triggers in training and testing inputs. In our evaluation, we demonstrate the effectiveness of TextGuard on three benchmark text classification tasks, surpassing the certification accuracy of existing certified defenses against backdoor attacks. Furthermore, we propose additional strategies to enhance the empirical performance of TextGuard. Comparisons with state-of-the-art empirical defenses validate the superiority of TextGuard in countering multiple backdoor attacks. Our code and data are available at https://github.com/AI-secure/TextGuard.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Hengzhi Pei",
        "Jinyuan Jia",
        "Wenbo Guo",
        "Bo Li",
        "Dawn Song"
      ],
      "url": "https://arxiv.org/abs/2311.11225",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2006.11890",
      "title": "Graph Backdoor",
      "abstract": "One intriguing property of deep neural networks (DNNs) is their inherent vulnerability to backdoor attacks -- a trojan model responds to trigger-embedded inputs in a highly predictable manner while functioning normally otherwise. Despite the plethora of prior work on DNNs for continuous data (e.g., images), the vulnerability of graph neural networks (GNNs) for discrete-structured data (e.g., graphs) is largely unexplored, which is highly concerning given their increasing use in security-sensitive domains. To bridge this gap, we present GTA, the first backdoor attack on GNNs. Compared with prior work, GTA departs in significant ways: graph-oriented -- it defines triggers as specific subgraphs, including both topological structures and descriptive features, entailing a large design spectrum for the adversary; input-tailored -- it dynamically adapts triggers to individual graphs, thereby optimizing both attack effectiveness and evasiveness; downstream model-agnostic -- it can be readily launched without knowledge regarding downstream models or fine-tuning strategies; and attack-extensible -- it can be instantiated for both transductive (e.g., node classification) and inductive (e.g., graph classification) tasks, constituting severe threats for a range of security-critical applications. Through extensive evaluation using benchmark datasets and state-of-the-art models, we demonstrate the effectiveness of GTA. We further provide analytical justification for its effectiveness and discuss potential countermeasures, pointing to several promising research directions.",
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [
        "Zhaohan Xi",
        "Ren Pang",
        "Shouling Ji",
        "Ting Wang"
      ],
      "url": "https://arxiv.org/abs/2006.11890",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2407.08935",
      "title": "Distributed Backdoor Attacks on Federated Graph Learning and Certified Defenses",
      "abstract": "Federated graph learning (FedGL) is an emerging federated learning (FL) framework that extends FL to learn graph data from diverse sources. FL for non-graph data has shown to be vulnerable to backdoor attacks, which inject a shared backdoor trigger into the training data such that the trained backdoored FL model can predict the testing data containing the trigger as the attacker desires. However, FedGL against backdoor attacks is largely unexplored, and no effective defense exists.   In this paper, we aim to address such significant deficiency. First, we propose an effective, stealthy, and persistent backdoor attack on FedGL. Our attack uses a subgraph as the trigger and designs an adaptive trigger generator that can derive the effective trigger location and shape for each graph. Our attack shows that empirical defenses are hard to detect/remove our generated triggers. To mitigate it, we further develop a certified defense for any backdoored FedGL model against the trigger with any shape at any location. Our defense involves carefully dividing a testing graph into multiple subgraphs and designing a majority vote-based ensemble classifier on these subgraphs. We then derive the deterministic certified robustness based on the ensemble classifier and prove its tightness. We extensively evaluate our attack and defense on six graph datasets. Our attack results show our attack can obtain > 90% backdoor accuracy in almost all datasets. Our defense results show, in certain cases, the certified accuracy for clean testing graphs against an arbitrary trigger with size 20 can be close to the normal accuracy under no attack, while there is a moderate gap in other cases. Moreover, the certified backdoor accuracy is always 0 for backdoored testing graphs generated by our attack, implying our defense can fully mitigate the attack. Source code is available at: https://github.com/Yuxin104/Opt-GDBA.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Yuxin Yang",
        "Qiang Li",
        "Jinyuan Jia",
        "Yuan Hong",
        "Binghui Wang"
      ],
      "url": "https://arxiv.org/abs/2407.08935",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_cc1fa34c",
      "title": "Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers",
      "abstract": "Training pipelines for machine learning (ML) based malware classification often rely on crowdsourced threat feeds, exposing a natural attack injection point. In this paper, we study the susceptibility of feature-based ML malware classifiers to backdoor poisoning attacks, specifically focusing on challenging \"clean label\" attacks where attackers do not control the sample labeling process. We propose the use of techniques from explainable machine learning to guide the selection of relevant features and values to create effective backdoor triggers in a model-agnostic fashion. Using multiple reference datasets for malware classification, including Windows PE files, PDFs, and Android applications, we demonstrate effective attacks against a diverse set of machine learning models and evaluate the effect of various constraints imposed on the attacker. To demonstrate the feasibility of our backdoor attacks in practice, we create a watermarking utility for Windows PE files that preserves the binary's functionality, and we leverage similar behavior-preserving alteration methodologies for Android and PDF files. Finally, we experiment with potential defensive strategies and show the difficulties of completely defending against these attacks, especially when the attacks blend in with the legitimate sample distribution.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Giorgio Severi",
        "Jim Meyer",
        "Scott E. Coull",
        "Alina Oprea"
      ],
      "url": "https://openalex.org/W3120073944",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_69e81ba3",
      "title": "TrojanModel: A Practical Trojan Attack against Automatic Speech Recognition Systems",
      "abstract": "While deep learning techniques have achieved great success in modern digital products, researchers have shown that deep learning models are susceptible to Trojan attacks. In a Trojan attack, an adversary stealthily modifies a deep learning model such that the model will output a predefined label whenever a trigger is present in the input. In this paper, we present TrojanModel, a practical Trojan attack against Automatic Speech Recognition (ASR) systems. ASR systems aim to transcribe voice input into text, which is easier for subsequent downstream applications to process. We consider a practical attack scenario in which an adversary inserts a Trojan into the acoustic model of a target ASR system. Unlike existing work that uses noise-like triggers that will easily arouse user suspicion, the work in this paper focuses on the use of unsuspicious sounds as a trigger, e.g., a piece of music playing in the background. In addition, TrojanModel does not require the retraining of a target model. Experimental results show that TrojanModel can achieve high attack success rates with negligible effect on the target model's performance. We also demonstrate that the attack is effective in an over-the-air attack scenario, where audio is played over a physical speaker and received by a microphone.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Wei Zong",
        "Yang-Wai Chow",
        "Willy Susilo",
        "Kien Do",
        "Svetha Venkatesh"
      ],
      "url": "https://openalex.org/W4385080316",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_92c470d3",
      "title": "MagBackdoor: Beware of Your Loudspeaker as Backdoor of Magnetic Attack for Malicious Command Injection",
      "abstract": "Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named &#x201C;DirtyFlipping&#x201D;, which uses dirty label techniques, &#x2018;label-on-label&#x2018;, to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Orson Mengara"
      ],
      "url": "https://openalex.org/W4393285751",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2302.06279",
      "title": "Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data",
      "abstract": "Deep neural networks (DNNs) have demonstrated remarkable performance across various tasks, including image and speech recognition. However, maximizing the effectiveness of DNNs requires meticulous optimization of numerous hyperparameters and network parameters through training. Moreover, high-performance DNNs entail many parameters, which consume significant energy during training. In order to overcome these challenges, researchers have turned to spiking neural networks (SNNs), which offer enhanced energy efficiency and biologically plausible data processing capabilities, rendering them highly suitable for sensory data tasks, particularly in neuromorphic data. Despite their advantages, SNNs, like DNNs, are susceptible to various threats, including adversarial examples and backdoor attacks. Yet, the field of SNNs still needs to be explored in terms of understanding and countering these attacks.   This paper delves into backdoor attacks in SNNs using neuromorphic datasets and diverse triggers. Specifically, we explore backdoor triggers within neuromorphic data that can manipulate their position and color, providing a broader scope of possibilities than conventional triggers in domains like images. We present various attack strategies, achieving an attack success rate of up to 100% while maintaining a negligible impact on clean accuracy. Furthermore, we assess these attacks' stealthiness, revealing that our most potent attacks possess significant stealth capabilities. Lastly, we adapt several state-of-the-art defenses from the image domain, evaluating their efficacy on neuromorphic data and uncovering instances where they fall short, leading to compromised performance.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Gorka Abad",
        "Oguzhan Ersoy",
        "Stjepan Picek",
        "Aitor Urbieta"
      ],
      "url": "https://arxiv.org/abs/2302.06279",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_60201f2f",
      "title": "Blind Backdoors in Deep Learning Models",
      "abstract": "We investigate a new method for injecting backdoors into machine learning models, based compromising the loss-value computation in the model-training code. We use it to demonstrate new classes of backdoors strictly more powerful than those in the prior literature: single-pixel and physical backdoors in ImageNet models, backdoors that switch the model to a covert, privacy-violating task, and backdoors that do not require inference-time input modifications. \r\nOur attack is blind: the attacker cannot modify the training data, nor observe the execution of his code, nor access the resulting model. The attack code creates poisoned training inputs on the fly, as the model is training, and uses multi-objective optimization to achieve high accuracy both the main and backdoor tasks. We show how a blind attack can evade any known defense and propose new ones.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Eugene Bagdasaryan",
        "Vitaly Shmatikov"
      ],
      "url": "https://openalex.org/W3195462295",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_b8ef1770",
      "title": "Experimental Analyses of the Physical Surveillance Risks in Client-Side Content Scanning",
      "abstract": "Content scanning systems employ perceptual hashing algorithms to scan user content for illicit material, such as child pornography or terrorist recruitment flyers.Perceptual hashing algorithms help determine whether two images are visually similar while preserving the privacy of the input images.Several efforts from industry and academia propose scanning on client devices such as smartphones due to the impending rollout of end-to-end encryption that will make server-side scanning difficult.These proposals have met with strong criticism because of the potential for the technology to be misused for censorship.However, the risks of this technology in the context of surveillance are not well understood.Our work informs this conversation by experimentally characterizing the potential for one type of misuse -attackers manipulating the content scanning system to perform physical surveillance on target locations.Our contributions are threefold: (1) we offer a definition of physical surveillance in the context of client-side image scanning systems; (2) we experimentally characterize this risk and create a surveillance algorithm that achieves physical surveillance rates more than 30% by poisoning 0.2% of the perceptual hash database; (3) we experimentally study the trade-off between the robustness of client-side image scanning systems and surveillance, showing that more robust detection of illicit material leads to an increased potential for physical surveillance in most settings.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Ashish Hooda",
        "Andrey Labunets",
        "Tadayoshi Kohno",
        "Earlence Fernandes"
      ],
      "url": "https://openalex.org/W4391725331",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_e8a2ea2f",
      "title": "On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts",
      "abstract": "Malicious or manipulated prompts are known to exploit text-to-image models to generate unsafe images. Existing studies, however, focus on the passive exploitation of such harmful capabilities. In this paper, we investigate the proactive generation of unsafe images from benign prompts (e.g., a photo of a cat) through maliciously modified text-to-image models. Our preliminary investigation demonstrates that poisoning attacks are a viable method to achieve this goal but uncovers significant side effects, where unintended spread to non-targeted prompts compromises attack stealthiness. Root cause analysis identifies conceptual similarity as an important contributing factor to these side effects. To address this, we propose a stealthy poisoning attack method that balances covertness and performance. Our findings highlight the potential risks of adopting text-to-image models in real-world scenarios, thereby calling for future research and safety measures in this space.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yixin Wu",
        "Ning Yu",
        "Michael Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W4387964003",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2306.11924",
      "title": "Deep perceptual hashing algorithms with hidden dual purpose: when client-side scanning does facial recognition",
      "abstract": "End-to-end encryption (E2EE) provides strong technical protections to individuals from interferences. Governments and law enforcement agencies around the world have however raised concerns that E2EE also allows illegal content to be shared undetected. Client-side scanning (CSS), using perceptual hashing (PH) to detect known illegal content before it is shared, is seen as a promising solution to prevent the diffusion of illegal content while preserving encryption. While these proposals raise strong privacy concerns, proponents of the solutions have argued that the risk is limited as the technology has a limited scope: detecting known illegal content. In this paper, we show that modern perceptual hashing algorithms are actually fairly flexible pieces of technology and that this flexibility could be used by an adversary to add a secondary hidden feature to a client-side scanning system. More specifically, we show that an adversary providing the PH algorithm can ``hide\" a secondary purpose of face recognition of a target individual alongside its primary purpose of image copy detection. We first propose a procedure to train a dual-purpose deep perceptual hashing model by jointly optimizing for both the image copy detection and the targeted facial recognition task. Second, we extensively evaluate our dual-purpose model and show it to be able to reliably identify a target individual 67% of the time while not impacting its performance at detecting illegal content. We also show that our model is neither a general face detection nor a facial recognition model, allowing its secondary purpose to be hidden. Finally, we show that the secondary purpose can be enabled by adding a single illegal looking image to the database. Taken together, our results raise concerns that a deep perceptual hashing-based CSS system could turn billions of user devices into tools to locate targeted individuals.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Shubham Jain",
        "Ana-Maria Cretu",
        "Antoine Cully",
        "Yves-Alexandre de Montjoye"
      ],
      "url": "https://arxiv.org/abs/2306.11924",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2308.13904",
      "title": "LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors",
      "abstract": "Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inverting the triggers, LMSanitator aims to invert the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prompt-tuning's property of freezing the pretrained model to perform accurate and fast output monitoring and input purging during the inference phase. Extensive experiments on multiple language models and NLP tasks illustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves 92.8% backdoor detection accuracy on 960 models and decreases the attack success rate to less than 1% in most scenarios.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Chengkun Wei",
        "Wenlong Meng",
        "Zhikun Zhang",
        "Min Chen",
        "Minghu Zhao",
        "Wenjing Fang",
        "Lei Wang",
        "Zihui Zhang",
        "Wenzhi Chen"
      ],
      "url": "https://arxiv.org/abs/2308.13904",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_156f5616",
      "title": "Make Agent Defeat Agent: Automatic Detection of Taint-Style Vulnerabilities in LLM-based Agents",
      "abstract": "Language Models (LMs) are becoming increasingly popular in real-world applications. Outsourcing model training and data hosting to third-party platforms has become a standard method for reducing costs. In such a situation, the attacker can manipulate the training process or data to inject a backdoor into models. Backdoor attacks are a serious threat where malicious behavior is activated when triggers are present, otherwise, the model operates normally. However, there is still no systematic and comprehensive review of LMs from the attacker's capabilities and purposes on different backdoor attack surfaces. Moreover, there is a shortage of analysis and comparison of the diverse emerging backdoor countermeasures. Therefore, this work aims to provide the NLP community with a timely review of backdoor attacks and countermeasures. According to the attackers' capability and affected stage of the LMs, the attack surfaces are formalized into four categorizations: attacking the pre-trained model with fine-tuning (APMF) or parameter-efficient fine-tuning (APMP), attacking the final model with training (AFMT), and attacking Large Language Models (ALLM). Thus, attacks under each categorization are combed. The countermeasures are categorized into two general classes: sample inspection and model inspection. Thus, we review countermeasures and analyze their advantages and disadvantages. Also, we summarize the benchmark datasets and provide comparable evaluations for representative attacks and defenses. Drawing the insights from the review, we point out the crucial areas for future research on the backdoor, especially soliciting more efficient and practical countermeasures.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Pengzhou Cheng",
        "Zongru Wu",
        "Wei Du",
        "Gongshen Liu"
      ],
      "url": "https://openalex.org/W4386754881",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2506.24033",
      "title": "Poisoning Attacks to Local Differential Privacy for Ranking Estimation",
      "abstract": "Local differential privacy (LDP) involves users perturbing their inputs to provide plausible deniability of their data. However, this also makes LDP vulnerable to poisoning attacks. In this paper, we first introduce novel poisoning attacks for ranking estimation. These attacks are intricate, as fake attackers do not merely adjust the frequency of target items. Instead, they leverage a limited number of fake users to precisely modify frequencies, effectively altering item rankings to maximize gains. To tackle this challenge, we introduce the concepts of attack cost and optimal attack item (set), and propose corresponding strategies for kRR, OUE, and OLH protocols. For kRR, we iteratively select optimal attack items and allocate suitable fake users. For OUE, we iteratively determine optimal attack item sets and consider the incremental changes in item frequencies across different sets. Regarding OLH, we develop a harmonic cost function based on the pre-image of a hash to select that supporting a larger number of effective attack items. Lastly, we present an attack strategy based on confidence levels to quantify the probability of a successful attack and the number of attack iterations more precisely. We demonstrate the effectiveness of our attacks through theoretical and empirical evidence, highlighting the necessity for defenses against these attacks. The source code and data have been made available at https://github.com/LDP-user/LDP-Ranking.git.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Pei Zhan",
        "Peng Tang",
        "Yangzhuo Li",
        "Puwen Wei",
        "Shanqing Guo"
      ],
      "url": "https://arxiv.org/abs/2506.24033",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2103.14991",
      "title": "Graph Unlearning",
      "abstract": "Machine unlearning is a process of removing the impact of some training data from the machine learning (ML) models upon receiving removal requests. While straightforward and legitimate, retraining the ML model from scratch incurs a high computational overhead. To address this issue, a number of approximate algorithms have been proposed in the domain of image and text data, among which SISA is the state-of-the-art solution. It randomly partitions the training set into multiple shards and trains a constituent model for each shard. However, directly applying SISA to the graph data can severely damage the graph structural information, and thereby the resulting ML model utility. In this paper, we propose GraphEraser, a novel machine unlearning framework tailored to graph data. Its contributions include two novel graph partition algorithms and a learning-based aggregation method. We conduct extensive experiments on five real-world graph datasets to illustrate the unlearning efficiency and model utility of GraphEraser. It achieves 2.06$\\times$ (small dataset) to 35.94$\\times$ (large dataset) unlearning time improvement. On the other hand, GraphEraser achieves up to $62.5\\%$ higher F1 score and our proposed learning-based aggregation method achieves up to $112\\%$ higher F1 score.\\footnote{Our code is available at \\url{https://github.com/MinChen00/Graph-Unlearning}.}",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Min Chen",
        "Zhikun Zhang",
        "Tianhao Wang",
        "Michael Backes",
        "Mathias Humbert",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2103.14991",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_7adf2996",
      "title": "Data Duplication: A Novel Multi-Purpose Attack Paradigm in Machine Unlearning",
      "abstract": "Duplication is a prevalent issue within datasets. Existing research has demonstrated that the presence of duplicated data in training datasets can significantly influence both model performance and data privacy. However, the impact of data duplication on the unlearning process remains largely unexplored. This paper addresses this gap by pioneering a comprehensive investigation into the role of data duplication, not only in standard machine unlearning but also in federated and reinforcement unlearning paradigms. Specifically, we propose an adversary who duplicates a subset of the target model's training set and incorporates it into the training set. After training, the adversary requests the model owner to unlearn this duplicated subset, and analyzes the impact on the unlearned model. For example, the adversary can challenge the model owner by revealing that, despite efforts to unlearn it, the influence of the duplicated subset remains in the model. Moreover, to circumvent detection by de-duplication techniques, we propose three novel near-duplication methods for the adversary, each tailored to a specific unlearning paradigm. We then examine their impacts on the unlearning process when de-duplication techniques are applied. Our findings reveal several crucial insights: 1) the gold standard unlearning method, retraining from scratch, fails to effectively conduct unlearning under certain conditions; 2) unlearning duplicated data can lead to significant model degradation in specific scenarios; and 3) meticulously crafted duplicates can evade detection by de-duplication methods.",
      "year": 2025,
      "venue": "CISPA Helmholtz Center",
      "authors": [
        "Ye, Dayong",
        "Zhu, Tianqing",
        "Li Jiayang",
        "Gao Kun",
        "Liu Bo",
        "Yu Zhang, Leo",
        "Zhou, Wanlei",
        "Zhang Yang"
      ],
      "url": "https://openalex.org/W7104649343",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2208.12348",
      "title": "SNAP: Efficient Extraction of Private Properties with Poisoning",
      "abstract": "Property inference attacks allow an adversary to extract global properties of the training dataset from a machine learning model. Such attacks have privacy implications for data owners sharing their datasets to train machine learning models. Several existing approaches for property inference attacks against deep neural networks have been proposed, but they all rely on the attacker training a large number of shadow models, which induces a large computational overhead.   In this paper, we consider the setting of property inference attacks in which the attacker can poison a subset of the training dataset and query the trained target model. Motivated by our theoretical analysis of model confidences under poisoning, we design an efficient property inference attack, SNAP, which obtains higher attack success and requires lower amounts of poisoning than the state-of-the-art poisoning-based property inference attack by Mahloujifar et al. For example, on the Census dataset, SNAP achieves 34% higher success rate than Mahloujifar et al. while being 56.5x faster. We also extend our attack to infer whether a certain property was present at all during training and estimate the exact proportion of a property of interest efficiently. We evaluate our attack on several properties of varying proportions from four datasets and demonstrate SNAP's generality and effectiveness. An open-source implementation of SNAP can be found at https://github.com/johnmath/snap-sp23.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Harsh Chaudhari",
        "John Abascal",
        "Alina Oprea",
        "Matthew Jagielski",
        "Florian Tram\u00e8r",
        "Jonathan Ullman"
      ],
      "url": "https://arxiv.org/abs/2208.12348",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_0b75ddff",
      "title": "ActiveDaemon: Unconscious DNN Dormancy and Waking Up via User-specific Invisible Token",
      "abstract": "Well-trained deep neural network (DNN) models can be treated as commodities for commercial transactions and generate significant revenues, raising the urgent need for intellectual property (IP) protection against illegitimate reproducing.Emerging studies on IP protection often aim at inserting watermarks into DNNs, allowing owners to passively verify the ownership of target models after counterfeit models appear and commercial benefits are infringed, while active authentication against unauthorized queries of DNN-based applications is still neglected.In this paper, we propose a novel approach to protect model intellectual property, called ActiveDaemon, which incorporates a built-in access control function in DNNs to safeguard against commercial piracy.Specifically, our approach enables DNNs to predict correct outputs only for authorized users with user-specific tokens while producing poor accuracy for unauthorized users.In ActiveDaemon, the user-specific tokens are generated by a specially designed U-Net style encoder-decoder network, which can map strings and input images into numerous noise images to address identity management with large-scale user capacity.Compared to existing studies, these user-specific tokens are invisible, dynamic and more perceptually concealed, enhancing the stealthiness and reliability of model IP protection.To automatically wake up the model accuracy, we utilize the data poisoning-based training technique to unconsciously embed the ActiveDaemon into the neuron's function.We conduct experiments to compare the protection performance of ActiveDaemon with four state-of-the-art approaches over four datasets.The experimental results show that ActiveDaemon can reduce the accuracy of unauthorized queries by as much as 81% with less than a 1.4% decrease in that of authorized queries.Meanwhile, our approach can also reduce the LPIPS scores of the authorized tokens to 0.0027 on CIFAR10 and 0.0368 on ImageNet 1 .",
      "year": 2024,
      "venue": null,
      "authors": [
        "Ge Ren",
        "Gaolei Li",
        "Shenghong Li",
        "Libo Chen",
        "Kui Ren"
      ],
      "url": "https://openalex.org/W4391724751",
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2406.19466",
      "title": "Data Poisoning Attacks to Locally Differentially Private Frequent Itemset Mining Protocols",
      "abstract": "Local differential privacy (LDP) provides a way for an untrusted data collector to aggregate users' data without violating their privacy. Various privacy-preserving data analysis tasks have been studied under the protection of LDP, such as frequency estimation, frequent itemset mining, and machine learning. Despite its privacy-preserving properties, recent research has demonstrated the vulnerability of certain LDP protocols to data poisoning attacks. However, existing data poisoning attacks are focused on basic statistics under LDP, such as frequency estimation and mean/variance estimation. As an important data analysis task, the security of LDP frequent itemset mining has yet to be thoroughly examined. In this paper, we aim to address this issue by presenting novel and practical data poisoning attacks against LDP frequent itemset mining protocols. By introducing a unified attack framework with composable attack operations, our data poisoning attack can successfully manipulate the state-of-the-art LDP frequent itemset mining protocols and has the potential to be adapted to other protocols with similar structures. We conduct extensive experiments on three datasets to compare the proposed attack with four baseline attacks. The results demonstrate the severity of the threat and the effectiveness of the proposed attack.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Wei Tong",
        "Haoyu Chen",
        "Jiacheng Niu",
        "Sheng Zhong"
      ],
      "url": "https://arxiv.org/abs/2406.19466",
      "classification_confidence": "HIGH"
    }
  ]
}