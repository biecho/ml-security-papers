{
  "papers": {
    "seed_aa77bed7": {
      "paper_id": "seed_aa77bed7",
      "title": "Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries",
      "abstract": "We study adversarial examples in a black-box setting where the adversary only has API access to the target model and each query is expensive. Prior work on black-box adversarial examples follows one of two main strategies: (1) transfer attacks use white-box attacks on local models to find candidate adversarial examples that transfer to the target model, and (2) optimization-based attacks use queries to the target model and apply optimization techniques to search for adversarial examples. We propose hybrid attacks that combine both strategies, using candidate adversarial examples from local models as starting points for optimization-based attacks and using labels learned in optimization-based attacks to tune local models for finding transfer candidates. We empirically demonstrate on the MNIST, CIFAR10, and ImageNet datasets that our hybrid attack strategy reduces cost and improves success rates. We also introduce a seed prioritization strategy which enables attackers to focus their resources on the most promising seeds. Combining hybrid attacks with our seed prioritization strategy enables batch attacks that can reliably find adversarial examples with only a handful of queries.",
      "year": 2019,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Fnu Suya",
        "Jianfeng Chi",
        "David Evans",
        "Yuan Tian"
      ],
      "url": "https://openalex.org/W2969333443",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773471",
      "fetched_at": "2026-01-09T13:51:01.410676",
      "classified_at": "2026-01-09T13:55:13.731419",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2969333443",
      "doi": "https://doi.org/10.48550/arxiv.1908.07000"
    },
    "seed_132b0814": {
      "paper_id": "seed_132b0814",
      "title": "Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning",
      "abstract": null,
      "year": 2020,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec20fall_quiring_prepub.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773485",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "1904.02144": {
      "paper_id": "1904.02144",
      "title": "HopSkipJumpAttack: A Query-Efficient Decision-Based Attack",
      "abstract": "The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for $\\ell_2$ and $\\ell_\\infty$ similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than Boundary Attack. It also achieves competitive performance in attacking several widely-used defense mechanisms. (HopSkipJumpAttack was named Boundary Attack++ in a previous version of the preprint.)",
      "year": 2020,
      "venue": "IEEE S&P",
      "authors": [
        "Jianbo Chen",
        "Michael I. Jordan",
        "Martin J. Wainwright"
      ],
      "url": "https://arxiv.org/abs/1904.02144",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773491",
      "fetched_at": "2026-01-09T12:14:35.773492",
      "classified_at": "2026-01-09T13:21:23.571162",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_eb74b357": {
      "paper_id": "seed_eb74b357",
      "title": "PatchGuard: A Provably Robust Defense against Adversarial Patches via Small Receptive Fields and Masking",
      "abstract": "Localized adversarial patches aim to induce misclassification in machine learning models by arbitrarily modifying pixels within a restricted region of an image. Such attacks can be realized in the physical world by attaching the adversarial patch to the object to be misclassified, and defending against such attacks is an unsolved/open problem. In this paper, we propose a general defense framework called PatchGuard that can achieve high provable robustness while maintaining high clean accuracy against localized adversarial patches. The cornerstone of PatchGuard involves the use of CNNs with small receptive fields to impose a bound on the number of features corrupted by an adversarial patch. Given a bounded number of corrupted features, the problem of designing an adversarial patch defense reduces to that of designing a secure feature aggregation mechanism. Towards this end, we present our robust masking defense that robustly detects and masks corrupted features to recover the correct prediction. Notably, we can prove the robustness of our defense against any adversary within our threat model. Our extensive evaluation on ImageNet, ImageNette (a 10-class subset of ImageNet), and CIFAR-10 datasets demonstrates that our defense achieves state-of-the-art performance in terms of both provable robust accuracy and clean accuracy.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Chong Xiang",
        "Arjun Nitin Bhagoji",
        "Vikash Sehwag",
        "Prateek Mittal"
      ],
      "url": "https://openalex.org/W3093131568",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773496",
      "fetched_at": "2026-01-09T13:51:04.347284",
      "classified_at": "2026-01-09T13:55:15.623704",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3093131568",
      "doi": "https://doi.org/10.48550/arxiv.2005.10884"
    },
    "seed_0db57677": {
      "paper_id": "seed_0db57677",
      "title": "Gotta Catch'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks",
      "abstract": "Deep neural networks (DNN) are known to be vulnerable to adversarial attacks. Numerous efforts either try to patch weaknesses in trained models, or try to make it difficult or costly to compute adversarial examples that exploit them. In our work, we explore a new \"honeypot\" approach to protect DNN models. We intentionally inject trapdoors, honeypot weaknesses in the classification manifold that attract attackers searching for adversarial examples. Attackers' optimization algorithms gravitate towards trapdoors, leading them to produce attacks similar to trapdoors in the feature space. Our defense then identifies attacks by comparing neuron activation signatures of inputs to those of trapdoors. In this paper, we introduce trapdoors and describe an implementation of a trapdoor-enabled defense. First, we analytically prove that trapdoors shape the computation of adversarial attacks so that attack inputs will have feature representations very similar to those of trapdoors. Second, we experimentally show that trapdoor-protected models can detect, with high accuracy, adversarial examples generated by state-of-the-art attacks (PGD, optimization-based CW, Elastic Net, BPDA), with negligible impact on normal classification. These results generalize across classification domains, including image, facial, and traffic-sign recognition. We also present significant results measuring trapdoors' robustness against customized adaptive attacks (countermeasures).",
      "year": 2020,
      "venue": null,
      "authors": [
        "Shawn Shan",
        "Emily Wenger",
        "Bolun Wang",
        "Bo Li",
        "Hai-Tao Zheng",
        "Ben Y. Zhao"
      ],
      "url": "https://openalex.org/W3088733693",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773500",
      "fetched_at": "2026-01-09T13:51:48.297535",
      "classified_at": "2026-01-09T13:55:17.492207",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3088733693",
      "doi": "https://doi.org/10.1145/3372297.3417231"
    },
    "1911.01559": {
      "paper_id": "1911.01559",
      "title": "A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models",
      "abstract": "Despite their tremendous success in a range of domains, deep learning systems are inherently susceptible to two types of manipulations: adversarial inputs -- maliciously crafted samples that deceive target deep neural network (DNN) models, and poisoned models -- adversely forged DNNs that misbehave on pre-defined inputs. While prior work has intensively studied the two attack vectors in parallel, there is still a lack of understanding about their fundamental connections: what are the dynamic interactions between the two attack vectors? what are the implications of such interactions for optimizing existing attacks? what are the potential countermeasures against the enhanced attacks? Answering these key questions is crucial for assessing and mitigating the holistic vulnerabilities of DNNs deployed in realistic settings.   Here we take a solid step towards this goal by conducting the first systematic study of the two attack vectors within a unified framework. Specifically, (i) we develop a new attack model that jointly optimizes adversarial inputs and poisoned models; (ii) with both analytical and empirical evidence, we reveal that there exist intriguing \"mutual reinforcement\" effects between the two attack vectors -- leveraging one vector significantly amplifies the effectiveness of the other; (iii) we demonstrate that such effects enable a large design spectrum for the adversary to enhance the existing attacks that exploit both vectors (e.g., backdoor attacks), such as maximizing the attack evasiveness with respect to various detection methods; (iv) finally, we discuss potential countermeasures against such optimized attacks and their technical challenges, pointing to several promising research directions.",
      "year": 2020,
      "venue": "ACM CCS",
      "authors": [
        "Ren Pang",
        "Hua Shen",
        "Xinyang Zhang",
        "Shouling Ji",
        "Yevgeniy Vorobeychik",
        "Xiapu Luo",
        "Alex Liu",
        "Ting Wang"
      ],
      "url": "https://arxiv.org/abs/1911.01559",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773504",
      "fetched_at": "2026-01-09T12:14:35.773505",
      "classified_at": "2026-01-09T13:21:24.383178",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_835f9fe4": {
      "paper_id": "seed_835f9fe4",
      "title": "Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled Defense",
      "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial attacks. A great effort has been directed to developing effective defenses against adversarial attacks and finding vulnerabilities of proposed defenses. A recently proposed defense called Trapdoor-enabled Detection (TeD) deliberately injects trapdoors into DNN models to trap and detect adversarial examples targeting categories protected by TeD. TeD can effectively detect existing state-of-the-art adversarial attacks. In this paper, we propose a novel black-box adversarial attack on TeD, called Feature-Indistinguishable Attack (FIA). It circumvents TeD by crafting adversarial examples indistinguishable in the feature (i.e., neuron-activation) space from benign examples in the target category. To achieve this goal, FIA jointly minimizes the distance to the expectation of feature representations of benign samples in the target category and maximizes the distances to positive adversarial examples generated to query TeD in the preparation phase. A constraint is used to ensure that the feature vector of a generated adversarial example is within the distribution of feature vectors of benign examples in the target category. Our extensive empirical evaluation with different configurations and variants of TeD indicates that our proposed FIA can effectively circumvent TeD. FIA opens a door for developing much more powerful adversarial attacks. The FIA code is available at: https://github.com/CGCL-codes/FeatureIndistinguishableAttack.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Chaoxiang He",
        "Bin Zhu",
        "Xiaojing Ma",
        "Hai Jin",
        "Shengshan Hu"
      ],
      "url": "https://openalex.org/W3213785680",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773508",
      "fetched_at": "2026-01-09T13:51:50.314269",
      "classified_at": "2026-01-09T13:55:19.960145",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3213785680",
      "doi": "https://doi.org/10.1145/3460120.3485378"
    },
    "2102.02956": {
      "paper_id": "2102.02956",
      "title": "DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks",
      "abstract": "State-of-the-art object detectors are vulnerable to localized patch hiding attacks, where an adversary introduces a small adversarial patch to make detectors miss the detection of salient objects. The patch attacker can carry out a physical-world attack by printing and attaching an adversarial patch to the victim object. In this paper, we propose DetectorGuard as the first general framework for building provably robust object detectors against localized patch hiding attacks. DetectorGuard is inspired by recent advancements in robust image classification research; we ask: can we adapt robust image classifiers for robust object detection? Unfortunately, due to their task difference, an object detector naively adapted from a robust image classifier 1) may not necessarily be robust in the adversarial setting or 2) even maintain decent performance in the clean setting. To build a high-performance robust object detector, we propose an objectness explaining strategy: we adapt a robust image classifier to predict objectness for every image location and then explain each objectness using the bounding boxes predicted by a conventional object detector. If all objectness is well explained, we output the predictions made by the conventional object detector; otherwise, we issue an attack alert. Notably, 1) in the adversarial setting, we formally prove the end-to-end robustness of DetectorGuard on certified objects, i.e., it either detects the object or triggers an alert, against any patch hiding attacker within our threat model; 2) in the clean setting, we have almost the same performance as state-of-the-art object detectors. Our evaluation on the PASCAL VOC, MS COCO, and KITTI datasets further demonstrates that DetectorGuard achieves the first provable robustness against localized patch hiding attacks at a negligible cost (<1%) of clean performance.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Chong Xiang",
        "Prateek Mittal"
      ],
      "url": "https://arxiv.org/abs/2102.02956",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773512",
      "fetched_at": "2026-01-09T12:14:35.773513",
      "classified_at": "2026-01-09T13:21:25.242488",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_34c9601a": {
      "paper_id": "seed_34c9601a",
      "title": "It's Not What It Looks Like: Manipulating Perceptual Hashing based Applications",
      "abstract": "Perceptual hashing is widely used to search or match similar images for digital forensics and cybercrime study. Unfortunately, the robustness of perceptual hashing algorithms is not well understood in these contexts. In this paper, we examine the robustness of perceptual hashing and its dependent security applications both experimentally and empirically. We first develop a series of attack algorithms to subvert perceptual hashing based image search. This is done by generating attack images that effectively enlarge the hash distance to the original image while introducing minimal visual changes. To make the attack practical, we design the attack algorithms under a black-box setting, augmented with novel designs (e.g., grayscale initialization) to improve the attack efficiency and transferability. We then evaluate our attack against the standard pHash as well as its robust variant using three different datasets. After confirming the attack effectiveness experimentally, we then empirically test against real-world reverse image search engines including TinEye, Google, Microsoft Bing, and Yandex. We find that our attack is highly successful on TinEye and Bing, and is moderately successful on Google and Yandex. Based on our findings, we discuss possible countermeasures and recommendations.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Qingying Hao",
        "Licheng Luo",
        "Steve T.K. Jan",
        "Gang Wang"
      ],
      "url": "https://openalex.org/W3211902004",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773517",
      "fetched_at": "2026-01-09T13:51:51.143999",
      "classified_at": "2026-01-09T13:55:21.905025",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3211902004",
      "doi": "https://doi.org/10.1145/3460120.3484559"
    },
    "seed_3dd86804": {
      "paper_id": "seed_3dd86804",
      "title": "RamBoAttack: A Robust and Query Efficient Deep Neural Network Decision Exploit",
      "abstract": "Machine learning models are critically susceptible to evasion attacks from adversarial examples. Generally, adversarial examples, modified inputs deceptively similar to the original input, are constructed under whitebox settings by adversaries with full access to the model. However, recent attacks have shown a remarkable reduction in query numbers to craft adversarial examples using blackbox attacks. Particularly, alarming is the ability to exploit the classification decision from the access interface of a trained model provided by a growing number of Machine Learning as a Service providers including Google, Microsoft, IBM and used by a plethora of applications incorporating these models. The ability of an adversary to exploit only the predicted label from a model to craft adversarial examples is distinguished as a decision-based attack. In our study, we first deep dive into recent state-of-the-art decision-based attacks in ICLR and SP to highlight the costly nature of discovering low distortion adversarial employing gradient estimation methods. We develop a robust query efficient attack capable of avoiding entrapment in a local minimum and misdirection from noisy gradients seen in gradient estimation methods. The attack method we propose, RamBoAttack, exploits the notion of Randomized Block Coordinate Descent to explore the hidden classifier manifold, targeting perturbations to manipulate only localized input features to address the issues of gradient estimation methods. Importantly, the RamBoAttack is more robust to the different sample inputs available to an adversary and the targeted class. Overall, for a given target class, RamBoAttack is demonstrated to be more robust at achieving a lower distortion within a given query budget. We curate our extensive results using the large-scale high-resolution ImageNet dataset and open-source our attack, test samples and artifacts on GitHub.",
      "year": 2021,
      "venue": "NDSS",
      "authors": [
        "Viet Quoc Vo",
        "Ehsan Abbasnejad",
        "Damith C. Ranasinghe"
      ],
      "url": "https://arxiv.org/abs/2112.05282",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773519",
      "fetched_at": "2026-01-09T13:51:07.155531",
      "classified_at": "2026-01-09T13:55:23.766954",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "2112.05282"
    },
    "2201.09650": {
      "paper_id": "2201.09650",
      "title": "What You See is Not What the Network Infers: Detecting Adversarial Examples Based on Semantic Contradiction",
      "abstract": "Adversarial examples (AEs) pose severe threats to the applications of deep neural networks (DNNs) to safety-critical domains, e.g., autonomous driving. While there has been a vast body of AE defense solutions, to the best of our knowledge, they all suffer from some weaknesses, e.g., defending against only a subset of AEs or causing a relatively high accuracy loss for legitimate inputs. Moreover, most existing solutions cannot defend against adaptive attacks, wherein attackers are knowledgeable about the defense mechanisms and craft AEs accordingly. In this paper, we propose a novel AE detection framework based on the very nature of AEs, i.e., their semantic information is inconsistent with the discriminative features extracted by the target DNN model. To be specific, the proposed solution, namely ContraNet, models such contradiction by first taking both the input and the inference result to a generator to obtain a synthetic output and then comparing it against the original input. For legitimate inputs that are correctly inferred, the synthetic output tries to reconstruct the input. On the contrary, for AEs, instead of reconstructing the input, the synthetic output would be created to conform to the wrong label whenever possible. Consequently, by measuring the distance between the input and the synthetic output with metric learning, we can differentiate AEs from legitimate inputs. We perform comprehensive evaluations under various AE attack scenarios, and experimental results show that ContraNet outperforms existing solutions by a large margin, especially under adaptive attacks. Moreover, our analysis shows that successful AEs that can bypass ContraNet tend to have much-weakened adversarial semantics. We have also shown that ContraNet can be easily combined with adversarial training techniques to achieve further improved AE defense capabilities.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Yijun Yang",
        "Ruiyuan Gao",
        "Yu Li",
        "Qiuxia Lai",
        "Qiang Xu"
      ],
      "url": "https://arxiv.org/abs/2201.09650",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773524",
      "fetched_at": "2026-01-09T12:14:35.773525",
      "classified_at": "2026-01-09T13:21:26.078486",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_9082ba7d": {
      "paper_id": "seed_9082ba7d",
      "title": "AutoDA: Automated Decision-based Iterative Adversarial Attacks",
      "abstract": "In the rapidly evolving field of machine learning, adversarial attacks\\npresent a significant challenge to model robustness and security.\\nDecision-based attacks, which only require feedback on the decision of a model\\nrather than detailed probabilities or scores, are particularly insidious and\\ndifficult to defend against. This work introduces L-AutoDA (Large Language\\nModel-based Automated Decision-based Adversarial Attacks), a novel approach\\nleveraging the generative capabilities of Large Language Models (LLMs) to\\nautomate the design of these attacks. By iteratively interacting with LLMs in\\nan evolutionary framework, L-AutoDA automatically designs competitive attack\\nalgorithms efficiently without much human effort. We demonstrate the efficacy\\nof L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline\\nmethods in both success rate and computational efficiency. Our findings\\nunderscore the potential of language models as tools for adversarial attack\\ngeneration and highlight new avenues for the development of robust AI systems.\\n",
      "year": 2024,
      "venue": "Proceedings of the Genetic and Evolutionary Computation Conference Companion",
      "authors": [
        "Ping Guo",
        "Fei Liu",
        "Xi Lin",
        "Qingchuan Zhao",
        "Qingfu Zhang"
      ],
      "url": "https://openalex.org/W4391418171",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773528",
      "fetched_at": "2026-01-09T13:51:08.488403",
      "classified_at": "2026-01-09T13:55:25.727316",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391418171",
      "doi": "https://doi.org/10.1145/3638530.3664121"
    },
    "seed_02670011": {
      "paper_id": "seed_02670011",
      "title": "Blacklight: Scalable Defense for Neural Networks against Query-Based Black-Box Attacks",
      "abstract": "Deep learning systems are known to be vulnerable to adversarial examples. In particular, query-based black-box attacks do not require knowledge of the deep learning model, but can compute adversarial examples over the network by submitting queries and inspecting returns. Recent work largely improves the efficiency of those attacks, demonstrating their practicality on today's ML-as-a-service platforms. We propose Blacklight, a new defense against query-based black-box adversarial attacks. The fundamental insight driving our design is that, to compute adversarial examples, these attacks perform iterative optimization over the network, producing image queries highly similar in the input space. Blacklight detects query-based black-box attacks by detecting highly similar queries, using an efficient similarity engine operating on probabilistic content fingerprints. We evaluate Blacklight against eight state-of-the-art attacks, across a variety of models and image classification tasks. Blacklight identifies them all, often after only a handful of queries. By rejecting all detected queries, Blacklight prevents any attack to complete, even when attackers persist to submit queries after account ban or query rejection. Blacklight is also robust against several powerful countermeasures, including an optimal black-box attack that approximates white-box attacks in efficiency. Finally, we illustrate how Blacklight generalizes to other domains like text classification.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Huiying Li",
        "Shawn Shan",
        "Emily Wenger",
        "Jiayun Zhang",
        "Hai-Tao Zheng",
        "Ben Y. Zhao"
      ],
      "url": "https://openalex.org/W4287752693",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773531",
      "fetched_at": "2026-01-09T13:51:09.075715",
      "classified_at": "2026-01-09T13:55:27.637111",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4287752693",
      "doi": "https://doi.org/10.48550/arxiv.2006.14042"
    },
    "seed_75866050": {
      "paper_id": "seed_75866050",
      "title": "Physical Hijacking Attacks against Object Trackers",
      "abstract": "Modern autonomous systems rely on both object detection and object tracking in their visual perception pipelines. Although many recent works have attacked the object detection component of autonomous vehicles, these attacks do not work on full pipelines that integrate object tracking to enhance the object detector\u2019s accuracy. Meanwhile, existing attacks against object tracking either lack real-world applicability or do not work against a powerful class of object trackers, Siamese trackers. In this paper, we present AttrackZone, a new physically-realizable tracker hijacking attack against Siamese trackers that systematically determines valid regions in an environment that can be used for physical perturbations. AttrackZone exploits the heatmap generation process of Siamese Region Proposal Networks in order to take control of an object\u2019s bounding box, resulting in physical consequences including vehicle collisions and masked intrusion of pedestrians into unauthorized areas. Evaluations in both the digital and physical domain show that AttrackZone achieves its attack goals 92% of the time, requiring only 0.3-3 seconds on average.",
      "year": 2022,
      "venue": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
      "authors": [
        "Raymond J. Muller",
        "Yanmao Man",
        "Z. Berkay Celik",
        "Ming Li",
        "Ryan Gerdes"
      ],
      "url": "https://openalex.org/W4308411153",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773534",
      "fetched_at": "2026-01-09T13:51:09.911577",
      "classified_at": "2026-01-09T13:55:29.513866",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4308411153",
      "doi": "https://doi.org/10.1145/3548606.3559390"
    },
    "2205.10686": {
      "paper_id": "2205.10686",
      "title": "Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models",
      "abstract": "Server breaches are an unfortunate reality on today's Internet. In the context of deep neural network (DNN) models, they are particularly harmful, because a leaked model gives an attacker \"white-box\" access to generate adversarial examples, a threat model that has no practical robust defenses. For practitioners who have invested years and millions into proprietary DNNs, e.g. medical imaging, this seems like an inevitable disaster looming on the horizon.   In this paper, we consider the problem of post-breach recovery for DNN models. We propose Neo, a new system that creates new versions of leaked models, alongside an inference time filter that detects and removes adversarial examples generated on previously leaked models. The classification surfaces of different model versions are slightly offset (by introducing hidden distributions), and Neo detects the overfitting of attacks to the leaked model used in its generation. We show that across a variety of tasks and attack methods, Neo is able to filter out attacks from leaked models with very high accuracy, and provides strong protection (7--10 recoveries) against attackers who repeatedly breach the server. Neo performs well against a variety of strong adaptive attacks, dropping slightly in # of breaches recoverable, and demonstrates potential as a complement to DNN defenses in the wild.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Shawn Shan",
        "Wenxin Ding",
        "Emily Wenger",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "url": "https://arxiv.org/abs/2205.10686",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773537",
      "fetched_at": "2026-01-09T12:14:35.773539",
      "classified_at": "2026-01-09T13:21:27.117087",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_d32928b7": {
      "paper_id": "seed_d32928b7",
      "title": "Squint Hard Enough: Attacking Perceptual Hashing with Adversarial Machine Learning",
      "abstract": "PhotoDNA is a widely utilized hash designed to counteract Child Sexual Abuse Material (CSAM). However, there has been a scarcity of detailed information regarding its performance. In this paper, we present a comprehensive analysis of its robustness and susceptibility to false positives, along with fundamental insights into its structure. Our findings reveal its resilience to common image processing techniques like lossy compression. Conversely, its robustness is limited when confronted with cropping. Additionally, we propose recommendations for enhancing the algorithm or optimizing its application. This work is an extension on our paper [21].",
      "year": 2024,
      "venue": "Journal of Cyber Security and Mobility",
      "authors": [
        "Martin Steinebach"
      ],
      "url": "https://openalex.org/W4395083462",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773542",
      "fetched_at": "2026-01-09T13:51:11.026980",
      "classified_at": "2026-01-09T13:55:31.377860",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4395083462",
      "doi": "https://doi.org/10.13052/jcsm2245-1439.1339"
    },
    "seed_fe4dcffc": {
      "paper_id": "seed_fe4dcffc",
      "title": "The Space of Adversarial Strategies",
      "abstract": "Adversarial examples, inputs designed to induce worst-case behavior in machine learning models, have been extensively studied over the past decade. Yet, our understanding of this phenomenon stems from a rather fragmented pool of knowledge; at present, there are a handful of attacks, each with disparate assumptions in threat models and incomparable definitions of optimality. In this paper, we propose a systematic approach to characterize worst-case (i.e., optimal) adversaries. We first introduce an extensible decomposition of attacks in adversarial machine learning by atomizing attack components into surfaces and travelers. With our decomposition, we enumerate over components to create 576 attacks (568 of which were previously unexplored). Next, we propose the Pareto Ensemble Attack (PEA): a theoretical attack that upper-bounds attack performance. With our new attacks, we measure performance relative to the PEA on: both robust and non-robust models, seven datasets, and three extended lp-based threat models incorporating compute costs, formalizing the Space of Adversarial Strategies. From our evaluation we find that attack performance to be highly contextual: the domain, model robustness, and threat model can have a profound influence on attack efficacy. Our investigation suggests that future studies measuring the security of machine learning should: (1) be contextualized to the domain &amp; threat models, and (2) go beyond the handful of known attacks used today.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Ryan Sheatsley",
        "Blaine Hoak",
        "Eric Pauley",
        "Patrick McDaniel"
      ],
      "url": "https://openalex.org/W4295698813",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773545",
      "fetched_at": "2026-01-09T13:51:12.380515",
      "classified_at": "2026-01-09T13:55:33.275627",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4295698813",
      "doi": "https://doi.org/10.48550/arxiv.2209.04521"
    },
    "2303.06280": {
      "paper_id": "2303.06280",
      "title": "Stateful Defenses for Machine Learning Models Are Not Yet Secure Against Black-box Attacks",
      "abstract": "Recent work has proposed stateful defense models (SDMs) as a compelling strategy to defend against a black-box attacker who only has query access to the model, as is common for online machine learning platforms. Such stateful defenses aim to defend against black-box attacks by tracking the query history and detecting and rejecting queries that are \"similar\" and thus preventing black-box attacks from finding useful gradients and making progress towards finding adversarial attacks within a reasonable query budget. Recent SDMs (e.g., Blacklight and PIHA) have shown remarkable success in defending against state-of-the-art black-box attacks. In this paper, we show that SDMs are highly vulnerable to a new class of adaptive black-box attacks. We propose a novel adaptive black-box attack strategy called Oracle-guided Adaptive Rejection Sampling (OARS) that involves two stages: (1) use initial query patterns to infer key properties about an SDM's defense; and, (2) leverage those extracted properties to design subsequent query patterns to evade the SDM's defense while making progress towards finding adversarial inputs. OARS is broadly applicable as an enhancement to existing black-box attacks - we show how to apply the strategy to enhance six common black-box attacks to be more effective against current class of SDMs. For example, OARS-enhanced versions of black-box attacks improved attack success rate against recent stateful defenses from almost 0% to to almost 100% for multiple datasets within reasonable query budgets.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Ryan Feng",
        "Ashish Hooda",
        "Neal Mangaokar",
        "Kassem Fawaz",
        "Somesh Jha",
        "Atul Prakash"
      ],
      "url": "https://arxiv.org/abs/2303.06280",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773548",
      "fetched_at": "2026-01-09T12:14:35.773549",
      "classified_at": "2026-01-09T13:21:27.955492",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_0e1a0e07": {
      "paper_id": "seed_0e1a0e07",
      "title": "BounceAttack: A Query-Efficient Decision-based Adversarial Attack by Bouncing into the Wild",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a068/1RjEaEvldVS",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773552",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_634fcd8a": {
      "paper_id": "seed_634fcd8a",
      "title": "Sabre: Cutting through Adversarial Noise with Adaptive Spectral Filtering and Input Reconstruction",
      "abstract": "<p class=\"MsoNormal\">The adoption of neural networks (NNs) across critical sectors including transportation, medicine, communications infrastructure, etc.is inexorable. However, NNs remain highly susceptible to adversarial perturbations, whereby seemingly minimal or imperceptible changes to their inputs cause gross misclassifications, which questions their practical use.Although a growing body of work focuses on defending against such attacks,adversarial robustness remains an open challenge, especially as the effectiveness of existing solutions against increasingly sophisticated input manipulations comes at the cost of degrading ability to recognize benign samples, as we reveal. In this work we introduce SABRE, an adversarial defense framework that closes the gap between benign and robust accuracy in NN classification tasks, without sacrificing benign sample recognition performance. In particular, through spectral decomposition of the input and selective energy-based filtering, SABRE extracts robust features that serve in input reconstruction prior to feeding existing NN architectures. We demonstrate the performance of our approach across multiple domains, by evaluating it on image classification, network intrusion detection, and speech command recognition tasks, showing that SABRE not only outperforms existing defense mechanisms, but also behaves consistently with different neural architectures,data types, (un)known attacks, and adversarial perturbation strengths. Through these extensive experiments, we make the case for SABRE\u2019s adoption in deploying robust and reliable neural classifiers.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Alec F. Diallo",
        "Paul Patras"
      ],
      "url": "https://openalex.org/W4402264076",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773556",
      "fetched_at": "2026-01-09T13:51:52.438414",
      "classified_at": "2026-01-09T13:55:35.810497",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4402264076",
      "doi": "https://doi.org/10.1109/sp54263.2024.00076"
    },
    "2307.07873": {
      "paper_id": "2307.07873",
      "title": "Why Does Little Robustness Help? A Further Step Towards Understanding Adversarial Transferability",
      "abstract": "Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in adversarial training explains the degradation of gradient similarity. Building on these insights, we explore the impacts of data augmentation and gradient regularization on transferability and identify that the trade-off generally exists in the various training mechanisms, thus building a comprehensive blueprint for the regulation mechanism behind transferability. Finally, we provide a general route for constructing better surrogates to boost transferability which optimizes both model smoothness and gradient similarity simultaneously, e.g., the combination of input gradient regularization and sharpness-aware minimization (SAM), validated by extensive experiments. In summary, we call for attention to the united impacts of these two factors for launching effective transfer attacks, rather than optimizing one while ignoring the other, and emphasize the crucial role of manipulating surrogate models.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Yechao Zhang",
        "Shengshan Hu",
        "Leo Yu Zhang",
        "Junyu Shi",
        "Minghui Li",
        "Xiaogeng Liu",
        "Wei Wan",
        "Hai Jin"
      ],
      "url": "https://arxiv.org/abs/2307.07873",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML07",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773560",
      "fetched_at": "2026-01-09T12:14:35.773561",
      "classified_at": "2026-01-09T13:21:28.906192",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2306.16614": {
      "paper_id": "2306.16614",
      "title": "Group-based Robustness: A General Framework for Customized Robustness in the Real World",
      "abstract": "Machine-learning models are known to be vulnerable to evasion attacks that perturb model inputs to induce misclassifications. In this work, we identify real-world scenarios where the true threat cannot be assessed accurately by existing attacks. Specifically, we find that conventional metrics measuring targeted and untargeted robustness do not appropriately reflect a model's ability to withstand attacks from one set of source classes to another set of target classes. To address the shortcomings of existing methods, we formally define a new metric, termed group-based robustness, that complements existing metrics and is better-suited for evaluating model performance in certain attack scenarios. We show empirically that group-based robustness allows us to distinguish between models' vulnerability against specific threat models in situations where traditional robustness metrics do not apply. Moreover, to measure group-based robustness efficiently and accurately, we 1) propose two loss functions and 2) identify three new attack strategies. We show empirically that with comparable success rates, finding evasive samples using our new loss functions saves computation by a factor as large as the number of targeted classes, and finding evasive samples using our new attack strategies saves time by up to 99\\% compared to brute-force search methods. Finally, we propose a defense method that increases group-based robustness by up to 3.52$\\times$.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Weiran Lin",
        "Keane Lucas",
        "Neo Eyal",
        "Lujo Bauer",
        "Michael K. Reiter",
        "Mahmood Sharif"
      ],
      "url": "https://arxiv.org/abs/2306.16614",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773564",
      "fetched_at": "2026-01-09T12:14:35.773565",
      "classified_at": "2026-01-09T13:21:29.764405",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_c3a24a00": {
      "paper_id": "seed_c3a24a00",
      "title": "DorPatch: Distributed and Occlusion-Robust Adversarial Patch to Evade Certifiable Defenses",
      "abstract": "Adversarial patch attacks are among the most practical adversarial attacks.Recent efforts focus on providing a certifiable guarantee on correct predictions in the presence of white-box adversarial patch attacks.In this paper, we propose DorPatch, an effective adversarial patch attack to evade both certifiably robust defenses and empirical defenses.DorPatch employs group lasso on a patch's mask, image dropout, density regularization, and structural loss to generate a fully optimized, distributed, occlusion-robust, and inconspicuous adversarial patch that can be deployed in physical-world adversarial patch attacks.Our extensive experimental evaluation with both digitaldomain and physical-world tests indicates that DorPatch can effectively evade PatchCleanser [64], the state-of-the-art certifiable defense, and empirical defenses against adversarial patch attacks.More critically, mispredicted results of adversarially patched examples generated by DorPatch can receive certification from PatchCleanser, producing a false trust in guaranteed predictions.DorPatch achieves state-of-the-art attacking performance and perceptual quality among all adversarial patch attacks.DorPatch poses a significant threat to real-world applications of DNN models and calls for developing effective defenses to thwart the attack.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Chaoxiang He",
        "Xiaojing Ma",
        "Bin Zhu",
        "Yimiao Zeng",
        "Hanqing Hu",
        "Xiaofan Bai",
        "Hai Jin",
        "Dongmei Zhang"
      ],
      "url": "https://openalex.org/W4391725251",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773569",
      "fetched_at": "2026-01-09T13:51:53.267562",
      "classified_at": "2026-01-09T13:55:37.970641",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391725251",
      "doi": "https://doi.org/10.14722/ndss.2024.24920"
    },
    "seed_b2597a38": {
      "paper_id": "seed_b2597a38",
      "title": "UniID: Spoofing Face Authentication System by Universal Identity",
      "abstract": "Face authentication systems are widely employed in access control systems to ensure the security of confidential facilities.Recent works have demonstrated their vulnerabilities to adversarial attacks.However, such attacks typically require adversaries to wear disguises such as glasses or hats during every authentication, which may raise suspicion and reduce their attack impacts.In this paper, we propose the UniID attack, which allows multiple adversaries to perform face spoofing attacks without any additional disguise by enabling an insider to register a universal identity into the face authentication database by wearing an adversarial patch.To achieve it, we first select appropriate adversaries through feature engineering, then generate the desired adversarial patch with a multi-target joint-optimization approach, and finally overcome practical challenges such as improving the transferability of the adversarial patch towards black-box systems and enhancing its robustness in the physical world.We implement UniID in laboratory setups and evaluate its effectiveness with six face recognition models (FaceNet, Mobile-FaceNet, ArcFace-18/50, and MagFace-18/50) and two commercial face authentication systems (ArcSoft and Face++).Simulation and real-world experimental results demonstrate that UniID can achieve a max attack success rate of 100% and 79% in 3-user scenarios under the white-box setting and black-box setting respectively, and it can be extended to more than 8 users.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Zhihao Wu",
        "Yushi Cheng",
        "Shibo Zhang",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "url": "https://openalex.org/W4391724743",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773571",
      "fetched_at": "2026-01-09T13:51:54.026212",
      "classified_at": "2026-01-09T13:55:39.764386",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391724743",
      "doi": "https://doi.org/10.14722/ndss.2024.241036"
    },
    "seed_3d23af29": {
      "paper_id": "seed_3d23af29",
      "title": "Enhance Stealthiness and Transferability of Adversarial Attacks with Class Activation Mapping Ensemble Attack",
      "abstract": "Although there has been extensive research on the transferability of adversarial attacks, existing methods for generating adversarial examples suffer from two significant drawbacks: poor stealthiness and low attack efficacy under low-round attacks.To address the above issues, we creatively propose an adversarial example generation method that ensembles the class activation maps of multiple models, called class activation mapping ensemble attack.We first use the class activation mapping method to discover the relationship between the decision of the Deep Neural Network and the image region.Then we calculate the class activation score for each pixel and use it as the weight for perturbation to enhance the stealthiness of adversarial examples and improve attack performance under low attack rounds.In the optimization process, we also ensemble class activation maps of multiple models to ensure the transferability of the adversarial attack algorithm.Experimental results show that our method generates adversarial examples with high perceptibility, transferability, attack performance under low-round attacks, and evasiveness.Specifically, when our attack capability is comparable to the most potent attack (VMIFGSM), our perceptibility is close to the best-performing attack (TPGD).For non-targeted attacks, our method outperforms the VMIFGSM by an average of 11.69% in attack capability against 13 target models and outperforms the TPGD by an average of 37.15%.For targeted attacks, our method achieves the fastest convergence, the most potent attack efficacy, and significantly outperforms the eight baseline methods in lowround attacks.Furthermore, our method can evade defenses and be used to assess the robustness of models 1 .",
      "year": 2024,
      "venue": null,
      "authors": [
        "Hui Xia",
        "Rui Zhang",
        "Zi Kang",
        "Shuliang Jiang",
        "Shuo Xu"
      ],
      "url": "https://openalex.org/W4391725334",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773574",
      "fetched_at": "2026-01-09T13:51:54.795798",
      "classified_at": "2026-01-09T13:55:41.651275",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391725334",
      "doi": "https://doi.org/10.14722/ndss.2024.23164"
    },
    "seed_d13bd23e": {
      "paper_id": "seed_d13bd23e",
      "title": "Neural Invisibility Cloak: Concealing Adversary in Images via Compromised AI-driven Image Signal Processing",
      "abstract": "Autonomous vehicles (AV) are game-changing innovations that promise a safer, more convenient, and environmentally friendly mode of transportation than traditional vehicles. Therefore, understanding AV technologies and their impact on society is critical as we continue this revolutionary journey. Generally, there needs to be a detailed study available to assist a researcher in understanding AV and its challenges. This research presents a comprehensive survey encompassing various aspects of AVs, such as public adoption, driverless city planning, traffic management, environmental impact, public health, social implications, international standards, safety, and security. Furthermore, it presents emerging technologies such as artificial intelligence (AI), integration of cloud computing, and solar power usage in automated vehicles. It also presents forensics approaches, tools used, standards involved, and challenges associated with conducting digital forensics in the context of autonomous vehicles. Moreover, this research provides an overview of cyber attacks affecting autonomous vehicles, attack management, traditional security devices, threat modeling, authentication schemes, over-the-air updates, zero-trust architectures, data privacy, and the corresponding defensive strategies to mitigate such risks. It also presents international standards, guidelines, and best practices for AVs. Finally, it outlines the future directions of AVs and the challenges that must be addressed to achieve widespread adoption.",
      "year": 2023,
      "venue": "Technologies",
      "authors": [
        "Memoona Sadaf",
        "Zafar Iqbal",
        "Abdul Rehman Javed",
        "Irum Saba",
        "Moez Krichen",
        "Sajid Majeed",
        "Arooj Raza"
      ],
      "url": "https://openalex.org/W4386415974",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773577",
      "fetched_at": "2026-01-09T13:51:19.303226",
      "classified_at": "2026-01-09T13:55:43.560283",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4386415974",
      "doi": "https://doi.org/10.3390/technologies11050117"
    },
    "seed_46951381": {
      "paper_id": "seed_46951381",
      "title": "Self-interpreting Adversarial Images",
      "abstract": "We introduce a new type of indirect, cross-modal injection attacks against visual language models that enable creation of self-interpreting images. These images contain hidden \"meta-instructions\" that control how models answer users' questions about the image and steer models' outputs to express an adversary-chosen style, sentiment, or point of view. Self-interpreting images act as soft prompts, conditioning the model to satisfy the adversary's (meta-)objective while still producing answers based on the image's visual content. Meta-instructions are thus a stronger form of prompt injection. Adversarial images look natural and the model's answers are coherent and plausible, yet they also follow the adversary-chosen interpretation, e.g., political spin, or even objectives that are not achievable with explicit text instructions. We evaluate the efficacy of self-interpreting images for a variety of models, interpretations, and user prompts. We describe how these attacks could cause harm by enabling creation of self-interpreting content that carries spam, misinformation, or spin. Finally, we discuss defenses.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Tingwei Zhang",
        "Collin Zhang",
        "John X. Morris",
        "Eugene Bagdasaryan",
        "Vitaly Shmatikov"
      ],
      "url": "https://openalex.org/W4400667173",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773580",
      "fetched_at": "2026-01-09T13:51:19.901181",
      "classified_at": "2026-01-09T13:55:45.390533",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4400667173",
      "doi": "https://doi.org/10.48550/arxiv.2407.08970"
    },
    "seed_7b350e01": {
      "paper_id": "seed_7b350e01",
      "title": "TextShield: Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation",
      "abstract": null,
      "year": 2020,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec20-li-jinfeng.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773583",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2106.09898": {
      "paper_id": "2106.09898",
      "title": "Bad Characters: Imperceptible NLP Attacks",
      "abstract": "Several years of research have shown that machine-learning systems are vulnerable to adversarial examples, both in theory and in practice. Until now, such attacks have primarily targeted visual models, exploiting the gap between human and machine perception. Although text-based models have also been attacked with adversarial examples, such attacks struggled to preserve semantic meaning and indistinguishability. In this paper, we explore a large class of adversarial examples that can be used to attack text-based models in a black-box setting without making any human-perceptible visual modification to inputs. We use encoding-specific perturbations that are imperceptible to the human eye to manipulate the outputs of a wide range of Natural Language Processing (NLP) systems from neural machine-translation pipelines to web search engines. We find that with a single imperceptible encoding injection -- representing one invisible character, homoglyph, reordering, or deletion -- an attacker can significantly reduce the performance of vulnerable models, and with three injections most models can be functionally broken. Our attacks work against currently-deployed commercial systems, including those produced by Microsoft and Google, in addition to open source models published by Facebook, IBM, and HuggingFace. This novel series of attacks presents a significant threat to many language processing systems: an attacker can affect systems in a targeted manner without any assumptions about the underlying model. We conclude that text-based NLP systems require careful input sanitization, just like conventional applications, and that given such systems are now being deployed rapidly at scale, the urgent attention of architects and operators is required.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Nicholas Boucher",
        "Ilia Shumailov",
        "Ross Anderson",
        "Nicolas Papernot"
      ],
      "url": "https://arxiv.org/abs/2106.09898",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773586",
      "fetched_at": "2026-01-09T12:14:35.773587",
      "classified_at": "2026-01-09T13:21:30.656202",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2209.06506": {
      "paper_id": "2209.06506",
      "title": "Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models",
      "abstract": "Neural text ranking models have witnessed significant advancement and are increasingly being deployed in practice. Unfortunately, they also inherit adversarial vulnerabilities of general neural models, which have been detected but remain underexplored by prior studies. Moreover, the inherit adversarial vulnerabilities might be leveraged by blackhat SEO to defeat better-protected search engines. In this study, we propose an imitation adversarial attack on black-box neural passage ranking models. We first show that the target passage ranking model can be transparentized and imitated by enumerating critical queries/candidates and then train a ranking imitation model. Leveraging the ranking imitation model, we can elaborately manipulate the ranking results and transfer the manipulation attack to the target ranking model. For this purpose, we propose an innovative gradient-based attack method, empowered by the pairwise objective function, to generate adversarial triggers, which causes premeditated disorderliness with very few tokens. To equip the trigger camouflages, we add the next sentence prediction loss and the language model fluency constraint to the objective function. Experimental results on passage ranking demonstrate the effectiveness of the ranking imitation attack model and adversarial triggers against various SOTA neural ranking models. Furthermore, various mitigation analyses and human evaluation show the effectiveness of camouflages when facing potential mitigation approaches. To motivate other scholars to further investigate this novel and important problem, we make the experiment data and code publicly available.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Jiawei Liu",
        "Yangyang Kang",
        "Di Tang",
        "Kaisong Song",
        "Changlong Sun",
        "Xiaofeng Wang",
        "Wei Lu",
        "Xiaozhong Liu"
      ],
      "url": "https://arxiv.org/abs/2209.06506",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773590",
      "fetched_at": "2026-01-09T12:14:35.773591",
      "classified_at": "2026-01-09T13:21:31.514424",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2303.14443": {
      "paper_id": "2303.14443",
      "title": "No more Reviewer #2: Subverting Automatic Paper-Reviewer Assignment using Adversarial Learning",
      "abstract": "The number of papers submitted to academic conferences is steadily rising in many scientific disciplines. To handle this growth, systems for automatic paper-reviewer assignments are increasingly used during the reviewing process. These systems use statistical topic models to characterize the content of submissions and automate the assignment to reviewers. In this paper, we show that this automation can be manipulated using adversarial learning. We propose an attack that adapts a given paper so that it misleads the assignment and selects its own reviewers. Our attack is based on a novel optimization strategy that alternates between the feature space and problem space to realize unobtrusive changes to the paper. To evaluate the feasibility of our attack, we simulate the paper-reviewer assignment of an actual security conference (IEEE S&P) with 165 reviewers on the program committee. Our results show that we can successfully select and remove reviewers without access to the assignment system. Moreover, we demonstrate that the manipulated papers remain plausible and are often indistinguishable from benign submissions.",
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [
        "Thorsten Eisenhofer",
        "Erwin Quiring",
        "Jonas M\u00f6ller",
        "Doreen Riepel",
        "Thorsten Holz",
        "Konrad Rieck"
      ],
      "url": "https://arxiv.org/abs/2303.14443",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773595",
      "fetched_at": "2026-01-09T12:14:35.773596",
      "classified_at": "2026-01-09T13:22:46.707897",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_851c5dcb": {
      "paper_id": "seed_851c5dcb",
      "title": "WaveGuard: Understanding and Mitigating Audio Adversarial Examples",
      "abstract": "There has been a recent surge in adversarial attacks on deep learning based automatic speech recognition (ASR) systems. These attacks pose new challenges to deep learning security and have raised significant concerns in deploying ASR systems in safety-critical applications. In this work, we introduce WaveGuard: a framework for detecting adversarial inputs that are crafted to attack ASR systems. Our framework incorporates audio transformation functions and analyses the ASR transcriptions of the original and transformed audio to detect adversarial inputs. We demonstrate that our defense framework is able to reliably detect adversarial examples constructed by four recent audio adversarial attacks, with a variety of audio transformation functions. With careful regard for best practices in defense evaluations, we analyze our proposed defense and its strength to withstand adaptive and robust attacks in the audio domain. We empirically demonstrate that audio transformations that recover audio from perceptually informed representations can lead to a strong defense that is robust against an adaptive adversary even in a complete white-box setting. Furthermore, WaveGuard can be used out-of-the box and integrated directly with any ASR model to efficiently detect audio adversarial examples, without the need for model retraining.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Shlomo Dubnov",
        "Julian McAuley",
        "Farinaz Koushanfar"
      ],
      "url": "https://openalex.org/W3135197931",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773599",
      "fetched_at": "2026-01-09T13:51:56.062140",
      "classified_at": "2026-01-09T13:55:47.259859",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3135197931",
      "doi": "https://doi.org/10.48550/arxiv.2103.03344"
    },
    "seed_6945ef9b": {
      "paper_id": "seed_6945ef9b",
      "title": "Dompteur: Taming Audio Adversarial Examples",
      "abstract": "Adversarial examples seem to be inevitable. These specifically crafted inputs allow attackers to arbitrarily manipulate machine learning systems. Even worse, they often seem harmless to human observers. In our digital society, this poses a significant threat. For example, Automatic Speech Recognition (ASR) systems, which serve as hands-free interfaces to many kinds of systems, can be attacked with inputs incomprehensible for human listeners. The research community has unsuccessfully tried several approaches to tackle this problem. In this paper we propose a different perspective: We accept the presence of adversarial examples against ASR systems, but we require them to be perceivable by human listeners. By applying the principles of psychoacoustics, we can remove semantically irrelevant information from the ASR input and train a model that resembles human perception more closely. We implement our idea in a tool named DOMPTEUR and demonstrate that our augmented system, in contrast to an unmodified baseline, successfully focuses on perceptible ranges of the input signal. This change forces adversarial examples into the audible range, while using minimal computational overhead and preserving benign performance. To evaluate our approach, we construct an adaptive attacker that actively tries to avoid our augmentations and demonstrate that adversarial examples from this attacker remain clearly perceivable. Finally, we substantiate our claims by performing a hearing test with crowd-sourced human listeners.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Thorsten Eisenhofer",
        "Lea Sch\u00f6nherr",
        "J. Howard Frank",
        "Lars Speckemeier",
        "Dorothea Kolossa",
        "Thorsten Holz"
      ],
      "url": "https://openalex.org/W3127994681",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773601",
      "fetched_at": "2026-01-09T13:51:57.577312",
      "classified_at": "2026-01-09T13:55:49.075176",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3127994681",
      "doi": "https://doi.org/10.48550/arxiv.2102.05431"
    },
    "seed_c668d459": {
      "paper_id": "seed_c668d459",
      "title": "EarArray: Defending against DolphinAttack via Acoustic Attenuation",
      "abstract": "DolphinAttacks (i.e., inaudible voice commands) modulate audible voices over ultrasounds to inject malicious commands silently into voice assistants and manipulate controlled systems (e.g., doors or smart speakers).Eliminating DolphinAttacks is challenging if ever possible since it requires to modify the microphone hardware.In this paper, we design EarArray, a lightweight method that can not only detect such attacks but also identify the direction of attackers without requiring any extra hardware or hardware modification.Essentially, inaudible voice commands are modulated on ultrasounds that inherently attenuate faster than the one of audible sounds.By inspecting the command sound signals via the built-in multiple microphones on smart devices, EarArray is able to estimate the attenuation rate and thus detect the attacks.We propose a model of the propagation of audible sounds and ultrasounds from the sound source to a voice assistant, e.g., a smart speaker, and illustrate the underlying principle and its feasibility.We implemented EarArray using two specially-designed microphone arrays and our experiments show that EarArray can detect inaudible voice commands with an accuracy of 99% and recognize the direction of the attackers with an accuracy of 97.89%.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Guoming Zhang",
        "Xiaoyu Ji",
        "Xinfeng Li",
        "Gang Qu",
        "Wenyuan Xu"
      ],
      "url": "https://openalex.org/W3138076532",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773604",
      "fetched_at": "2026-01-09T13:51:58.331888",
      "classified_at": "2026-01-09T13:55:50.901383",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3138076532",
      "doi": "https://doi.org/10.14722/ndss.2021.24551"
    },
    "1911.01840": {
      "paper_id": "1911.01840",
      "title": "Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems",
      "abstract": "Speaker recognition (SR) is widely used in our daily life as a biometric authentication or identification mechanism. The popularity of SR brings in serious security concerns, as demonstrated by recent adversarial attacks. However, the impacts of such threats in the practical black-box setting are still open, since current attacks consider the white-box setting only. In this paper, we conduct the first comprehensive and systematic study of the adversarial attacks on SR systems (SRSs) to understand their security weakness in the practical blackbox setting. For this purpose, we propose an adversarial attack, named FAKEBOB, to craft adversarial samples. Specifically, we formulate the adversarial sample generation as an optimization problem, incorporated with the confidence of adversarial samples and maximal distortion to balance between the strength and imperceptibility of adversarial voices. One key contribution is to propose a novel algorithm to estimate the score threshold, a feature in SRSs, and use it in the optimization problem to solve the optimization problem. We demonstrate that FAKEBOB achieves 99% targeted attack success rate on both open-source and commercial systems. We further demonstrate that FAKEBOB is also effective on both open-source and commercial systems when playing over the air in the physical world. Moreover, we have conducted a human study which reveals that it is hard for human to differentiate the speakers of the original and adversarial voices. Last but not least, we show that four promising defense methods for adversarial attack from the speech recognition domain become ineffective on SRSs against FAKEBOB, which calls for more effective defense methods. We highlight that our study peeks into the security implications of adversarial attacks on SRSs, and realistically fosters to improve the security robustness of SRSs.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Guangke Chen",
        "Sen Chen",
        "Lingling Fan",
        "Xiaoning Du",
        "Zhe Zhao",
        "Fu Song",
        "Yang Liu"
      ],
      "url": "https://arxiv.org/abs/1911.01840",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773607",
      "fetched_at": "2026-01-09T12:14:35.773608",
      "classified_at": "2026-01-09T13:22:47.671334",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "1910.05262": {
      "paper_id": "1910.05262",
      "title": "Hear \"No Evil\", See \"Kenansville\": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems",
      "abstract": "Automatic speech recognition and voice identification systems are being deployed in a wide array of applications, from providing control mechanisms to devices lacking traditional interfaces, to the automatic transcription of conversations and authentication of users. Many of these applications have significant security and privacy considerations. We develop attacks that force mistranscription and misidentification in state of the art systems, with minimal impact on human comprehension. Processing pipelines for modern systems are comprised of signal preprocessing and feature extraction steps, whose output is fed to a machine-learned model. Prior work has focused on the models, using white-box knowledge to tailor model-specific attacks. We focus on the pipeline stages before the models, which (unlike the models) are quite similar across systems. As such, our attacks are black-box and transferable, and demonstrably achieve mistranscription and misidentification rates as high as 100% by modifying only a few frames of audio. We perform a study via Amazon Mechanical Turk demonstrating that there is no statistically significant difference between human perception of regular and perturbed audio. Our findings suggest that models may learn aspects of speech that are generally not perceived by human subjects, but that are crucial for model accuracy. We also find that certain English language phonemes (in particular, vowels) are significantly more susceptible to our attack. We show that the attacks are effective when mounted over cellular networks, where signals are subject to degradation due to transcoding, jitter, and packet loss.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Hadi Abdullah",
        "Muhammad Sajidur Rahman",
        "Washington Garcia",
        "Logan Blue",
        "Kevin Warren",
        "Anurag Swarnim Yadav",
        "Tom Shrimpton",
        "Patrick Traynor"
      ],
      "url": "https://arxiv.org/abs/1910.05262",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773611",
      "fetched_at": "2026-01-09T12:14:35.773613",
      "classified_at": "2026-01-09T13:22:48.315989",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2007.06622": {
      "paper_id": "2007.06622",
      "title": "SoK: The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems",
      "abstract": "Speech and speaker recognition systems are employed in a variety of applications, from personal assistants to telephony surveillance and biometric authentication. The wide deployment of these systems has been made possible by the improved accuracy in neural networks. Like other systems based on neural networks, recent research has demonstrated that speech and speaker recognition systems are vulnerable to attacks using manipulated inputs. However, as we demonstrate in this paper, the end-to-end architecture of speech and speaker systems and the nature of their inputs make attacks and defenses against them substantially different than those in the image space. We demonstrate this first by systematizing existing research in this space and providing a taxonomy through which the community can evaluate future work. We then demonstrate experimentally that attacks against these models almost universally fail to transfer. In so doing, we argue that substantial additional work is required to provide adequate mitigations in this space.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Hadi Abdullah",
        "Kevin Warren",
        "Vincent Bindschaedler",
        "Nicolas Papernot",
        "Patrick Traynor"
      ],
      "url": "https://arxiv.org/abs/2007.06622",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773616",
      "fetched_at": "2026-01-09T12:14:35.773617",
      "classified_at": "2026-01-09T13:22:49.084044",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_c331271c": {
      "paper_id": "seed_c331271c",
      "title": "AdvPulse: Universal, Synchronization-free, and Targeted Audio Adversarial Attacks via Subsecond Perturbations",
      "abstract": "Existing efforts in audio adversarial attacks only focus on the scenarios where an adversary has prior knowledge of the entire speech input so as to generate an adversarial example by aligning and mixing the audio input with corresponding adversarial perturbation. In this work we consider a more practical and challenging attack scenario where the intelligent audio system takes streaming audio inputs (e.g., live human speech) and the adversary can deceive the system by playing adversarial perturbations simultaneously. This change in attack behavior brings great challenges, preventing existing adversarial perturbation generation methods from being applied directly. In practice, (1) the adversary cannot anticipate what the victim will say: the adversary cannot rely on their prior knowledge of the speech signal to guide how to generate adversarial perturbations; and (2) the adversary cannot control when the victim will speak: the synchronization between the adversarial perturbation and the speech cannot be guaranteed. To address these challenges, in this paper we propose AdvPulse, a systematic approach to generate subsecond audio adversarial perturbations, that achieves the capability to alter the recognition results of streaming audio inputs in a targeted and synchronization-free manner. To circumvent the constraints on speech content and time, we exploit penalty-based universal adversarial perturbation generation algorithm and incorporate the varying time delay into the optimization process. We further tailor the adversarial perturbation according to environmental sounds to make it inconspicuous to humans. Additionally, by considering the sources of distortions occurred during the physical playback, we are able to generate more robust audio adversarial perturbations that can remain effective even under over-the-air propagation. Extensive experiments on two representative types of intelligent audio systems (i.e., speaker recognition and speech command recognition) are conducted in various realistic environments. The results show that our attack can achieve an average attack success rate of over 89.6% in indoor environments and 76.0% in inside-vehicle scenarios even with loud engine and road noises.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Zhuohang Li",
        "Yi Wu",
        "Jian Liu",
        "Yingying Chen",
        "Bo Yuan"
      ],
      "url": "https://openalex.org/W3109668151",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773619",
      "fetched_at": "2026-01-09T13:51:59.102800",
      "classified_at": "2026-01-09T13:55:52.708835",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3109668151",
      "doi": "https://doi.org/10.1145/3372297.3423348"
    },
    "2110.09714": {
      "paper_id": "2110.09714",
      "title": "Black-box Adversarial Attacks on Commercial Speech Platforms with Minimal Information",
      "abstract": "Adversarial attacks against commercial black-box speech platforms, including cloud speech APIs and voice control devices, have received little attention until recent years. The current \"black-box\" attacks all heavily rely on the knowledge of prediction/confidence scores to craft effective adversarial examples, which can be intuitively defended by service providers without returning these messages. In this paper, we propose two novel adversarial attacks in more practical and rigorous scenarios. For commercial cloud speech APIs, we propose Occam, a decision-only black-box adversarial attack, where only final decisions are available to the adversary. In Occam, we formulate the decision-only AE generation as a discontinuous large-scale global optimization problem, and solve it by adaptively decomposing this complicated problem into a set of sub-problems and cooperatively optimizing each one. Our Occam is a one-size-fits-all approach, which achieves 100% success rates of attacks with an average SNR of 14.23dB, on a wide range of popular speech and speaker recognition APIs, including Google, Alibaba, Microsoft, Tencent, iFlytek, and Jingdong, outperforming the state-of-the-art black-box attacks. For commercial voice control devices, we propose NI-Occam, the first non-interactive physical adversarial attack, where the adversary does not need to query the oracle and has no access to its internal information and training data. We combine adversarial attacks with model inversion attacks, and thus generate the physically-effective audio AEs with high transferability without any interaction with target devices. Our experimental results show that NI-Occam can successfully fool Apple Siri, Microsoft Cortana, Google Assistant, iFlytek and Amazon Echo with an average SRoA of 52% and SNR of 9.65dB, shedding light on non-interactive physical attacks against voice control devices.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Baolin Zheng",
        "Peipei Jiang",
        "Qian Wang",
        "Qi Li",
        "Chao Shen",
        "Cong Wang",
        "Yunjie Ge",
        "Qingyang Teng",
        "Shenyi Zhang"
      ],
      "url": "https://arxiv.org/abs/2110.09714",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773622",
      "fetched_at": "2026-01-09T12:14:35.773623",
      "classified_at": "2026-01-09T13:22:49.759980",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2207.13192": {
      "paper_id": "2207.13192",
      "title": "Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception",
      "abstract": "Recently, adversarial machine learning attacks have posed serious security threats against practical audio signal classification systems, including speech recognition, speaker recognition, and music copyright detection. Previous studies have mainly focused on ensuring the effectiveness of attacking an audio signal classifier via creating a small noise-like perturbation on the original signal. It is still unclear if an attacker is able to create audio signal perturbations that can be well perceived by human beings in addition to its attack effectiveness. This is particularly important for music signals as they are carefully crafted with human-enjoyable audio characteristics.   In this work, we formulate the adversarial attack against music signals as a new perception-aware attack framework, which integrates human study into adversarial attack design. Specifically, we conduct a human study to quantify the human perception with respect to a change of a music signal. We invite human participants to rate their perceived deviation based on pairs of original and perturbed music signals, and reverse-engineer the human perception process by regression analysis to predict the human-perceived deviation given a perturbed signal. The perception-aware attack is then formulated as an optimization problem that finds an optimal perturbation signal to minimize the prediction of perceived deviation from the regressed human perception model. We use the perception-aware framework to design a realistic adversarial music attack against YouTube's copyright detector. Experiments show that the perception-aware attack produces adversarial music with significantly better perceptual quality than prior work.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Rui Duan",
        "Zhe Qu",
        "Shangqing Zhao",
        "Leah Ding",
        "Yao Liu",
        "Zhuo Lu"
      ],
      "url": "https://arxiv.org/abs/2207.13192",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773626",
      "fetched_at": "2026-01-09T12:14:35.773627",
      "classified_at": "2026-01-09T13:22:50.439253",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_ca879b84": {
      "paper_id": "seed_ca879b84",
      "title": "SpecPatch: Human-in-the-Loop Adversarial Audio Spectrogram Patch Attack on Speech Recognition",
      "abstract": "The rapid development of deep neural networks and generative AI has catalyzed growth in realistic speech synthesis. While this technology has great potential to improve lives, it also leads to the emergence of ''DeepFake'' where synthesized speech can be misused to deceive humans and machines for nefarious purposes. In response to this evolving threat, there has been a significant amount of interest in mitigating this threat by DeepFake detection.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Zhiyuan Yu",
        "Shixuan Zhai",
        "Ning Zhang"
      ],
      "url": "https://openalex.org/W4388856757",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773631",
      "fetched_at": "2026-01-09T13:51:59.619846",
      "classified_at": "2026-01-09T13:55:54.699084",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4388856757",
      "doi": "https://doi.org/10.1145/3576915.3623209"
    },
    "seed_b28bffd0": {
      "paper_id": "seed_b28bffd0",
      "title": "Learning Normality is Enough: A Software-based Mitigation against Inaudible Voice Attacks",
      "abstract": "Automatic speech recognition (ASR) systems have been shown to be vulnerable to adversarial examples (AEs).Recent success all assumes that users will not notice or disrupt the attack process despite the existence of music/noise-like sounds and spontaneous responses from voice assistants.Nonetheless, in practical user-present scenarios, user awareness may nullify existing attack attempts that launch unexpected sounds or ASR usage.In this paper, we seek to bridge the gap in existing research and extend the attack to user-present scenarios.We propose VRIFLE, an inaudible adversarial perturbation (IAP) attack via ultrasound delivery that can manipulate ASRs as a user speaks.The inherent differences between audible sounds and ultrasounds make IAP delivery face unprecedented challenges such as distortion, noise, and instability.In this regard, we design a novel ultrasonic transformation model to enhance the crafted perturbation to be physically effective and even survive long-distance delivery.We further enable VRIFLE's robustness by adopting a series of augmentation on user and real-world variations during the generation process.In this way, VRIFLE features an effective real-time manipulation of the ASR output from different distances and under any speech of users, with an alter-and-mute strategy that suppresses the impact of user disruption.Our extensive experiments in both digital and physical worlds verify VRIFLE's effectiveness under various configurations, robustness against six kinds of defenses, and universality in a targeted manner.We also show that VRIFLE can be delivered with a portable attack device and even everyday-life loudspeakers.Receiver UTM \u2460",
      "year": 2024,
      "venue": null,
      "authors": [
        "Xinfeng Li",
        "Yan Chen",
        "Xuancun Lu",
        "Zihan Zeng",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "url": "https://openalex.org/W4391725296",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773634",
      "fetched_at": "2026-01-09T13:52:00.472332",
      "classified_at": "2026-01-09T13:55:56.537331",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391725296",
      "doi": "https://doi.org/10.14722/ndss.2024.23030"
    },
    "seed_5442d202": {
      "paper_id": "seed_5442d202",
      "title": "Understanding and Benchmarking the Commonality of Adversarial Examples",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a111/1Ub23jYBBHa",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773637",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_062923da": {
      "paper_id": "seed_062923da",
      "title": "ALIF: Low-Cost Adversarial Audio Attacks on Black-Box Speech Platforms using Linguistic Features",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a056/1RjEav0Daa4",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773641",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2311.07780": {
      "paper_id": "2311.07780",
      "title": "Parrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models",
      "abstract": "Audio adversarial examples (AEs) have posed significant security challenges to real-world speaker recognition systems. Most black-box attacks still require certain information from the speaker recognition model to be effective (e.g., keeping probing and requiring the knowledge of similarity scores). This work aims to push the practicality of the black-box attacks by minimizing the attacker's knowledge about a target speaker recognition model. Although it is not feasible for an attacker to succeed with completely zero knowledge, we assume that the attacker only knows a short (or a few seconds) speech sample of a target speaker. Without any probing to gain further knowledge about the target model, we propose a new mechanism, called parrot training, to generate AEs against the target model. Motivated by recent advancements in voice conversion (VC), we propose to use the one short sentence knowledge to generate more synthetic speech samples that sound like the target speaker, called parrot speech. Then, we use these parrot speech samples to train a parrot-trained(PT) surrogate model for the attacker. Under a joint transferability and perception framework, we investigate different ways to generate AEs on the PT model (called PT-AEs) to ensure the PT-AEs can be generated with high transferability to a black-box target model with good human perceptual quality. Real-world experiments show that the resultant PT-AEs achieve the attack success rates of 45.8% - 80.8% against the open-source models in the digital-line scenario and 47.9% - 58.3% against smart devices, including Apple HomePod (Siri), Amazon Echo, and Google Home, in the over-the-air scenario.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Rui Duan",
        "Zhe Qu",
        "Leah Ding",
        "Yao Liu",
        "Zhuo Lu"
      ],
      "url": "https://arxiv.org/abs/2311.07780",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773644",
      "fetched_at": "2026-01-09T12:14:35.773645",
      "classified_at": "2026-01-09T13:22:51.208431",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_7166cede": {
      "paper_id": "seed_7166cede",
      "title": "When Translators Refuse to Translate: A Novel Attack to Speech Translation Systems",
      "abstract": "\"Prince of Networks is the first treatment of Bruno Latour specifically as a philosopher. Part One covers four key works that display Latours underrated contributions to metaphysics: Irreductions, Science in Action, We Have Never Been Modern, and Pandoras Hope. Harman contends that Latour is one of the central figures of contemporary philosophy, with a highly original ontology centered in four key concepts: actants, irreduction, translation, and alliance. In Part Two, Harman summarizes Latours most important philosophical insights, including his status as the first secular occasionalist. Working from his own object-oriented perspective, Harman also criticizes the Latourian focus on the relational character of actors at the expense of their cryptic autonomous reality. This book forms a remarkable interface between Latours Actor-Network Theory and the Speculative Realism of Harman and his confederates.\" -- Book cover.",
      "year": 2009,
      "venue": "Digital Library - Books MetaLibrary",
      "authors": [
        "Graham Harman"
      ],
      "url": "https://openalex.org/W2165673991",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773647",
      "fetched_at": "2026-01-09T13:52:04.868900",
      "classified_at": "2026-01-09T13:55:58.514440",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2165673991"
    },
    "2107.04284": {
      "paper_id": "2107.04284",
      "title": "Universal 3-Dimensional Perturbations for Black-Box Attacks on Video Recognition Systems",
      "abstract": "Widely deployed deep neural network (DNN) models have been proven to be vulnerable to adversarial perturbations in many applications (e.g., image, audio and text classifications). To date, there are only a few adversarial perturbations proposed to deviate the DNN models in video recognition systems by simply injecting 2D perturbations into video frames. However, such attacks may overly perturb the videos without learning the spatio-temporal features (across temporal frames), which are commonly extracted by DNN models for video recognition. To our best knowledge, we propose the first black-box attack framework that generates universal 3-dimensional (U3D) perturbations to subvert a variety of video recognition systems. U3D has many advantages, such as (1) as the transfer-based attack, U3D can universally attack multiple DNN models for video recognition without accessing to the target DNN model; (2) the high transferability of U3D makes such universal black-box attack easy-to-launch, which can be further enhanced by integrating queries over the target model when necessary; (3) U3D ensures human-imperceptibility; (4) U3D can bypass the existing state-of-the-art defense schemes; (5) U3D can be efficiently generated with a few pre-learned parameters, and then immediately injected to attack real-time DNN-based video recognition systems. We have conducted extensive experiments to evaluate U3D on multiple DNN models and three large-scale video datasets. The experimental results demonstrate its superiority and practicality.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Shangyu Xie",
        "Han Wang",
        "Yu Kong",
        "Yuan Hong"
      ],
      "url": "https://arxiv.org/abs/2107.04284",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773650",
      "fetched_at": "2026-01-09T12:14:35.773651",
      "classified_at": "2026-01-09T13:22:51.848729",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2203.16000": {
      "paper_id": "2203.16000",
      "title": "StyleFool: Fooling Video Classification Systems via Style Transfer",
      "abstract": "Video classification systems are vulnerable to adversarial attacks, which can create severe security problems in video verification. Current black-box attacks need a large number of queries to succeed, resulting in high computational overhead in the process of attack. On the other hand, attacks with restricted perturbations are ineffective against defenses such as denoising or adversarial training. In this paper, we focus on unrestricted perturbations and propose StyleFool, a black-box video adversarial attack via style transfer to fool the video classification system. StyleFool first utilizes color theme proximity to select the best style image, which helps avoid unnatural details in the stylized videos. Meanwhile, the target class confidence is additionally considered in targeted attacks to influence the output distribution of the classifier by moving the stylized video closer to or even across the decision boundary. A gradient-free method is then employed to further optimize the adversarial perturbations. We carry out extensive experiments to evaluate StyleFool on two standard datasets, UCF-101 and HMDB-51. The experimental results demonstrate that StyleFool outperforms the state-of-the-art adversarial attacks in terms of both the number of queries and the robustness against existing defenses. Moreover, 50% of the stylized videos in untargeted attacks do not need any query since they can already fool the video classification model. Furthermore, we evaluate the indistinguishability through a user study to show that the adversarial samples of StyleFool look imperceptible to human eyes, despite unrestricted perturbations.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Yuxin Cao",
        "Xi Xiao",
        "Ruoxi Sun",
        "Derui Wang",
        "Minhui Xue",
        "Sheng Wen"
      ],
      "url": "https://arxiv.org/abs/2203.16000",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773665",
      "fetched_at": "2026-01-09T12:14:35.773666",
      "classified_at": "2026-01-09T13:22:52.440799",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2108.09513": {
      "paper_id": "2108.09513",
      "title": "A Hard Label Black-box Adversarial Attack Against Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph structure related tasks such as node classification and graph classification. However, GNNs are vulnerable to adversarial attacks. Existing works mainly focus on attacking GNNs for node classification; nevertheless, the attacks against GNNs for graph classification have not been well explored.   In this work, we conduct a systematic study on adversarial attacks against GNNs for graph classification via perturbing the graph structure. In particular, we focus on the most challenging attack, i.e., hard label black-box attack, where an attacker has no knowledge about the target GNN model and can only obtain predicted labels through querying the target model.To achieve this goal, we formulate our attack as an optimization problem, whose objective is to minimize the number of edges to be perturbed in a graph while maintaining the high attack success rate. The original optimization problem is intractable to solve, and we relax the optimization problem to be a tractable one, which is solved with theoretical convergence guarantee. We also design a coarse-grained searching algorithm and a query-efficient gradient computation algorithm to decrease the number of queries to the target GNN model. Our experimental results on three real-world datasets demonstrate that our attack can effectively attack representative GNNs for graph classification with less queries and perturbations. We also evaluate the effectiveness of our attack under two defenses: one is well-designed adversarial graph detector and the other is that the target GNN model itself is equipped with a defense to prevent adversarial graph generation. Our experimental results show that such defenses are not effective enough, which highlights more advanced defenses.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Jiaming Mu",
        "Binghui Wang",
        "Qi Li",
        "Kun Sun",
        "Mingwei Xu",
        "Zhuotao Liu"
      ],
      "url": "https://arxiv.org/abs/2108.09513",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773668",
      "fetched_at": "2026-01-09T12:14:35.773669",
      "classified_at": "2026-01-09T13:22:53.193578",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "1705.07535": {
      "paper_id": "1705.07535",
      "title": "Evading Classifiers by Morphing in the Dark",
      "abstract": "Learning-based systems have been shown to be vulnerable to evasion through adversarial data manipulation. These attacks have been studied under assumptions that the adversary has certain knowledge of either the target model internals, its training dataset or at least classification scores it assigns to input samples. In this paper, we investigate a much more constrained and realistic attack scenario wherein the target classifier is minimally exposed to the adversary, revealing on its final classification decision (e.g., reject or accept an input sample). Moreover, the adversary can only manipulate malicious samples using a blackbox morpher. That is, the adversary has to evade the target classifier by morphing malicious samples \"in the dark\". We present a scoring mechanism that can assign a real-value score which reflects evasion progress to each sample based on the limited information available. Leveraging on such scoring mechanism, we propose an evasion method -- EvadeHC -- and evaluate it against two PDF malware detectors, namely PDFRate and Hidost. The experimental evaluation demonstrates that the proposed evasion attacks are effective, attaining $100\\%$ evasion rate on the evaluation dataset. Interestingly, EvadeHC outperforms the known classifier evasion technique that operates based on classification scores output by the classifiers. Although our evaluations are conducted on PDF malware classifier, the proposed approaches are domain-agnostic and is of wider application to other learning-based systems.",
      "year": 2017,
      "venue": "ACM CCS",
      "authors": [
        "Hung Dang",
        "Yue Huang",
        "Ee-Chien Chang"
      ],
      "url": "https://arxiv.org/abs/1705.07535",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773672",
      "fetched_at": "2026-01-09T12:14:35.773674",
      "classified_at": "2026-01-09T13:22:53.979422",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "1905.12386": {
      "paper_id": "1905.12386",
      "title": "Misleading Authorship Attribution of Source Code using Adversarial Learning",
      "abstract": "In this paper, we present a novel attack against authorship attribution of source code. We exploit that recent attribution methods rest on machine learning and thus can be deceived by adversarial examples of source code. Our attack performs a series of semantics-preserving code transformations that mislead learning-based attribution but appear plausible to a developer. The attack is guided by Monte-Carlo tree search that enables us to operate in the discrete domain of source code. In an empirical evaluation with source code from 204 programmers, we demonstrate that our attack has a substantial effect on two recent attribution methods, whose accuracy drops from over 88% to 1% under attack. Furthermore, we show that our attack can imitate the coding style of developers with high accuracy and thereby induce false attributions. We conclude that current approaches for authorship attribution are inappropriate for practical application and there is a need for resilient analysis techniques.",
      "year": 2019,
      "venue": "USENIX Security",
      "authors": [
        "Erwin Quiring",
        "Alwin Maier",
        "Konrad Rieck"
      ],
      "url": "https://arxiv.org/abs/1905.12386",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773678",
      "fetched_at": "2026-01-09T12:14:35.773679",
      "classified_at": "2026-01-09T13:22:54.779977",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_2b933416": {
      "paper_id": "seed_2b933416",
      "title": "Intriguing Properties of Adversarial ML Attacks in the Problem Space",
      "abstract": "Recent research efforts on adversarial machine learning (ML) have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored. This article makes three major contributions. Firstly, we propose a general formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, absent artifacts, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the by-product of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. Secondly, building on our general formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations in terms of semantics and artifacts. We have tested our approach on a dataset with 150K Android apps from 2016 and 2018 which show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Thirdly, we explore the effectiveness of adversarial training as a possible approach to enforce robustness against adversarial samples, evaluating its effectiveness on the considered machine learning models under different scenarios. Our results demonstrate that \"adversarial-malware as a service\" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial instance.",
      "year": 2019,
      "venue": "IEEE S&P",
      "authors": [
        "Jacopo Cortellazzi",
        "Feargus Pendlebury",
        "Daniel Arp",
        "Erwin Quiring",
        "Fabio Pierazzi",
        "Lorenzo Cavallaro"
      ],
      "url": "https://arxiv.org/abs/1911.02142",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773681",
      "fetched_at": "2026-01-09T13:52:05.401916",
      "classified_at": "2026-01-09T13:56:00.596282",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "1911.02142"
    },
    "seed_52fb0e9c": {
      "paper_id": "seed_52fb0e9c",
      "title": "Structural Attack against Graph Based Android Malware Detection",
      "abstract": "Malware detection techniques achieve great success with deeper insight into the semantics of malware. Among existing detection techniques, function call graph (FCG) based methods achieve promising performance due to their prominent representations of malware's functionalities. Meanwhile, recent adversarial attacks not only perturb feature vectors to deceive classifiers (i.e., feature-space attacks) but also investigate how to generate real evasive malware (i.e., problem-space attacks). However, existing problem-space attacks are limited due to their inconsistent transformations between feature space and problem space.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Kaifa Zhao",
        "Hao Zhou",
        "Yulin Zhu",
        "Xian Zhan",
        "Kai Zhou",
        "Jianfeng Li",
        "Le Yu",
        "Wei Yuan",
        "Xiapu Luo"
      ],
      "url": "https://openalex.org/W3212677680",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773684",
      "fetched_at": "2026-01-09T13:52:06.198932",
      "classified_at": "2026-01-09T13:56:02.480782",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3212677680",
      "doi": "https://doi.org/10.1145/3460120.3485387"
    },
    "seed_a3e33bfd": {
      "paper_id": "seed_a3e33bfd",
      "title": "URET: Universal Robustness Evaluation Toolkit (for Evasion)",
      "abstract": "Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functional adversarial input. We demonstrate the generality of our approach on several diverse machine learning tasks with various input representations. We also show the importance of generating adversarial examples as they enable the deployment of mitigation techniques.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Kevin Eykholt",
        "Taesung Lee",
        "Douglas Lee Schales",
        "Jiyong Jang",
        "Ian Molloy",
        "Masha Zorin"
      ],
      "url": "https://openalex.org/W4385964791",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773687",
      "fetched_at": "2026-01-09T13:52:06.986676",
      "classified_at": "2026-01-09T13:56:04.409237",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385964791",
      "doi": "https://doi.org/10.48550/arxiv.2308.01840"
    },
    "seed_c40e42d5": {
      "paper_id": "seed_c40e42d5",
      "title": "Adversarial Training for Raw-Binary Malware Classifiers",
      "abstract": "Learning on execution behaviour, i.e., sequences of API calls, is proven to be effective in malware detection. In this paper, we present CruParamer, a deep neural network based malware detection approach for Windows platform that performs learning on sequences of parameter-augmented APIs. It first employs rule-based and clustering-based classification to assess the sensitivity of a parameter to malicious behaviour, and further labels the API following the run-time parameters with varying degrees of sensitivities. Then, it encodes the APIs by concatenating the native embedding and the sensitive embedding of labelled APIs, for characterizing the relationship between successive labelled APIs and their correspondence in terms of security semantics. Finally, it feeds the sequences of API embedding into the deep neural network for training a binary classifier to detect malware. In addition to presenting the design, we have implemented CruParamer and evaluated it on two datasets. The results demonstrate that CruParamer outperforms na\u00efve models when taking raw APIs as input, proving the effectiveness of CruParamer. Moreover, we have evaluated the impact of <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">mimicry</i> and adversarial attacks on our model, and the results verify the robustness of CruParamer.",
      "year": 2022,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xiaohui Chen",
        "Zhiyu Hao",
        "Lun Li",
        "Lei Cui",
        "Yiran Zhu",
        "Zhenquan Ding",
        "Yongji Liu"
      ],
      "url": "https://openalex.org/W4212847133",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773691",
      "fetched_at": "2026-01-09T13:52:07.780716",
      "classified_at": "2026-01-09T13:56:07.101932",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4212847133",
      "doi": "https://doi.org/10.1109/tifs.2022.3152360"
    },
    "seed_1fdb3170": {
      "paper_id": "seed_1fdb3170",
      "title": "PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis",
      "abstract": "This proposal discusses the growing challenges in reverse engineering modern software binaries, particularly those compiled from newer system programming languages such as Rust, Go, and Mojo. Traditional reverse engineering techniques, developed with a focus on C and C++, fall short when applied to these newer languages due to their reliance on outdated heuristics and failure to fully utilize the rich semantic information embedded in binary programs. These challenges are exacerbated by the limitations of current data-driven methods, which are susceptible to generating inaccurate results, commonly referred to as hallucinations. To overcome these limitations, we propose a novel approach that integrates probabilistic binary analysis with fine-tuned large language models (LLMs). Our method systematically models the uncertainties inherent in reverse engineering, enabling more accurate reasoning about incomplete or ambiguous information. By incorporating LLMs, we extend the analysis beyond traditional heuristics, allowing for more creative and context-aware inferences, particularly for binaries from diverse programming languages. This hybrid approach not only enhances the robustness and accuracy of reverse engineering efforts but also offers a scalable solution adaptable to the rapidly evolving landscape of software development.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Zhuo Zhuo",
        "Xiangyu Zhang"
      ],
      "url": "https://openalex.org/W4416072512",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773694",
      "fetched_at": "2026-01-09T13:52:08.350347",
      "classified_at": "2026-01-09T13:56:09.696244",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4416072512",
      "doi": "https://doi.org/10.48550/arxiv.2506.03504"
    },
    "2303.08509": {
      "paper_id": "2303.08509",
      "title": "Black-box Adversarial Example Attack towards FCG Based Android Malware Detection under Incomplete Feature Information",
      "abstract": "The function call graph (FCG) based Android malware detection methods have recently attracted increasing attention due to their promising performance. However, these methods are susceptible to adversarial examples (AEs). In this paper, we design a novel black-box AE attack towards the FCG based malware detection system, called BagAmmo. To mislead its target system, BagAmmo purposefully perturbs the FCG feature of malware through inserting \"never-executed\" function calls into malware code. The main challenges are two-fold. First, the malware functionality should not be changed by adversarial perturbation. Second, the information of the target system (e.g., the graph feature granularity and the output probabilities) is absent.   To preserve malware functionality, BagAmmo employs the try-catch trap to insert function calls to perturb the FCG of malware. Without the knowledge about feature granularity and output probabilities, BagAmmo adopts the architecture of generative adversarial network (GAN), and leverages a multi-population co-evolution algorithm (i.e., Apoem) to generate the desired perturbation. Every population in Apoem represents a possible feature granularity, and the real feature granularity can be achieved when Apoem converges.   Through extensive experiments on over 44k Android apps and 32 target models, we evaluate the effectiveness, efficiency and resilience of BagAmmo. BagAmmo achieves an average attack success rate of over 99.9% on MaMaDroid, APIGraph and GCN, and still performs well in the scenario of concept drift and data imbalance. Moreover, BagAmmo outperforms the state-of-the-art attack SRL in attack success rate.",
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [
        "Heng Li",
        "Zhang Cheng",
        "Bang Wu",
        "Liheng Yuan",
        "Cuiying Gao",
        "Wei Yuan",
        "Xiapu Luo"
      ],
      "url": "https://arxiv.org/abs/2303.08509",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773697",
      "fetched_at": "2026-01-09T12:14:35.773698",
      "classified_at": "2026-01-09T13:22:55.490510",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2309.01866": {
      "paper_id": "2309.01866",
      "title": "Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting",
      "abstract": "The widespread adoption of the Android operating system has made malicious Android applications an appealing target for attackers. Machine learning-based (ML-based) Android malware detection (AMD) methods are crucial in addressing this problem; however, their vulnerability to adversarial examples raises concerns. Current attacks against ML-based AMD methods demonstrate remarkable performance but rely on strong assumptions that may not be realistic in real-world scenarios, e.g., the knowledge requirements about feature space, model parameters, and training dataset. To address this limitation, we introduce AdvDroidZero, an efficient query-based attack framework against ML-based AMD methods that operates under the zero knowledge setting. Our extensive evaluation shows that AdvDroidZero is effective against various mainstream ML-based AMD methods, in particular, state-of-the-art such methods and real-world antivirus solutions.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Ping He",
        "Yifan Xia",
        "Xuhong Zhang",
        "Shouling Ji"
      ],
      "url": "https://arxiv.org/abs/2309.01866",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773700",
      "fetched_at": "2026-01-09T12:14:35.773701",
      "classified_at": "2026-01-09T13:22:56.109773",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_4d703ac5": {
      "paper_id": "seed_4d703ac5",
      "title": "Make a Feint to the East While Attacking in the West: Blinding LLM-Based Code Auditors with Flashboom Attacks",
      "abstract": null,
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://ieeexplore.ieee.org/document/11023369",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773704",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2208.12897": {
      "paper_id": "2208.12897",
      "title": "ATTRITION: Attacking Static Hardware Trojan Detection Techniques Using Reinforcement Learning",
      "abstract": "Stealthy hardware Trojans (HTs) inserted during the fabrication of integrated circuits can bypass the security of critical infrastructures. Although researchers have proposed many techniques to detect HTs, several limitations exist, including: (i) a low success rate, (ii) high algorithmic complexity, and (iii) a large number of test patterns. Furthermore, the most pertinent drawback of prior detection techniques stems from an incorrect evaluation methodology, i.e., they assume that an adversary inserts HTs randomly. Such inappropriate adversarial assumptions enable detection techniques to claim high HT detection accuracy, leading to a \"false sense of security.\" Unfortunately, to the best of our knowledge, despite more than a decade of research on detecting HTs inserted during fabrication, there have been no concerted efforts to perform a systematic evaluation of HT detection techniques.   In this paper, we play the role of a realistic adversary and question the efficacy of HT detection techniques by developing an automated, scalable, and practical attack framework, ATTRITION, using reinforcement learning (RL). ATTRITION evades eight detection techniques across two HT detection categories, showcasing its agnostic behavior. ATTRITION achieves average attack success rates of $47\\times$ and $211\\times$ compared to randomly inserted HTs against state-of-the-art HT detection techniques. We demonstrate ATTRITION's ability to evade detection techniques by evaluating designs ranging from the widely-used academic suites to larger designs such as the open-source MIPS and mor1kx processors to AES and a GPS module. Additionally, we showcase the impact of ATTRITION-generated HTs through two case studies (privilege escalation and kill switch) on the mor1kx processor. We envision that our work, along with our released HT benchmarks and models, fosters the development of better HT detection techniques.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Vasudev Gohil",
        "Hao Guo",
        "Satwik Patnaik",
        " Jeyavijayan",
        " Rajendran"
      ],
      "url": "https://arxiv.org/abs/2208.12897",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773707",
      "fetched_at": "2026-01-09T12:14:35.773708",
      "classified_at": "2026-01-09T13:22:56.745587",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_fd5d9be1": {
      "paper_id": "seed_fd5d9be1",
      "title": "DeepShuffle: A Lightweight Defense Framework against Adversarial Fault Injection Attacks on Deep Neural Networks in Multi-Tenant Cloud-FPGA",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a034/1RjEa9WUlPi",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773711",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_31676987": {
      "paper_id": "seed_31676987",
      "title": "Interpretable Deep Learning under Fire",
      "abstract": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing \\imlses are highly vulnerable to adversarial manipulations. Specifically, we present ADV^2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV^2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability -- a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV^2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions.",
      "year": 2018,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Xinyang Zhang",
        "Ningfei Wang",
        "Hua Shen",
        "Shouling Ji",
        "Xiapu Luo",
        "Ting Wang"
      ],
      "url": "https://openalex.org/W2903544706",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773714",
      "fetched_at": "2026-01-09T13:52:10.701815",
      "classified_at": "2026-01-09T13:56:11.860850",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2903544706",
      "doi": "https://doi.org/10.48550/arxiv.1812.00891"
    },
    "seed_25e677aa": {
      "paper_id": "seed_25e677aa",
      "title": "\u201cIs your explanation stable?\u201d: A Robustness Evaluation Framework for Feature Attribution",
      "abstract": "Understanding the decision process of neural networks is hard. One vital method for explanation is to attribute its decision to pivotal features. Although many algorithms are proposed, most of them solely improve the faithfulness to the model. However, the real environment contains many random noises, which may leads to great fluctuations in the explanations. More seriously, recent works show that explanation algorithms are vulnerable to adversarial attacks. All of these make the explanation hard to trust in real scenarios.   To bridge this gap, we propose a model-agnostic method \\emph{Median Test for Feature Attribution} (MeTFA) to quantify the uncertainty and increase the stability of explanation algorithms with theoretical guarantees. MeTFA has the following two functions: (1) examine whether one feature is significantly important or unimportant and generate a MeTFA-significant map to visualize the results; (2) compute the confidence interval of a feature attribution score and generate a MeTFA-smoothed map to increase the stability of the explanation. Experiments show that MeTFA improves the visual quality of explanations and significantly reduces the instability while maintaining the faithfulness. To quantitatively evaluate the faithfulness of an explanation under different noise settings, we further propose several robust faithfulness metrics. Experiment results show that the MeTFA-smoothed explanation can significantly increase the robust faithfulness. In addition, we use two scenarios to show MeTFA's potential in the applications. First, when applied to the SOTA explanation method to locate context bias for semantic segmentation models, MeTFA-significant explanations use far smaller regions to maintain 99\\%+ faithfulness. Second, when tested with different explanation-oriented attacks, MeTFA can help defend vanilla, as well as adaptive, adversarial attacks against explanations.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Yuyou Gan",
        "Yuhao Mao",
        "Xuhong Zhang",
        "Shouling Ji",
        "Yuwen Pu",
        "Meng Han",
        "Jianwei Yin",
        "Ting Wang"
      ],
      "url": "https://arxiv.org/abs/2209.01782",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773720",
      "fetched_at": "2026-01-09T13:52:11.432812",
      "classified_at": "2026-01-09T13:56:15.356734",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "2209.01782"
    },
    "seed_1e31a394": {
      "paper_id": "seed_1e31a394",
      "title": "AIRS: Explanation for Deep Reinforcement Learning based Security Applications",
      "abstract": "This perspective paper is based on several sessions by the members of the Round Table AI at FIRM 1 , with input from a number of external and international speakers. Its particular focus lies on the management of the model risk of productive models in banks and other financial institutions. The models in view range from simple rules-based approaches to Artificial Intelligence (AI) or Machine learning (ML) models with a high level of sophistication. The typical applications of those models are related to predictions and decision making around the value chain of credit risk (including accounting side under IFRS9 or related national GAAP approaches), insurance risk or other financial risk types. We expect more models of higher complexity in the space of anti-money laundering, fraud detection and transaction monitoring as well as a rise of AI/ML models as alternatives to current methods in solving some of the more intricate stochastic differential equations needed for the pricing and/or valuation of derivatives. The same type of model is also successful in areas unrelated to risk management, such as sales optimization, customer lifetime value considerations, robo-advisory, and other fields of applications. The paper refers to recent related publications from central banks, financial supervisors and regulators as well as other relevant sources and working groups. It aims to give practical advice for establishing a risk-based governance and testing framework for the mentioned model types and discusses the use of recent technologies, approaches, and platforms to support the establishment of responsible, trustworthy, explainable, auditable, and manageable AI/ML in production. In view of the recent EU publication on AI, also referred to as the EU Artificial Intelligence Act (AIA), we also see a certain added value for this paper as an instigator of further thinking outside of the financial services sector, in particular where \u201cHigh Risk\u201d models according to the mentioned EU consultation are concerned.",
      "year": 2022,
      "venue": "Frontiers in Artificial Intelligence",
      "authors": [
        "Sebastian Fritz-Morgenthal",
        "Bernhard Hein",
        "Jochen Papenbrock"
      ],
      "url": "https://openalex.org/W4225496733",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773723",
      "fetched_at": "2026-01-09T13:52:12.199283",
      "classified_at": "2026-01-09T13:56:17.182220",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4225496733",
      "doi": "https://doi.org/10.3389/frai.2022.779799"
    },
    "seed_ac5526aa": {
      "paper_id": "seed_ac5526aa",
      "title": "SoK: Explainable Machine Learning in Adversarial Environments",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": null,
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773725",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_377680e8": {
      "paper_id": "seed_377680e8",
      "title": "SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations",
      "abstract": "Research into adversarial examples (AE) has developed rapidly, yet static adversarial patches are still the main technique for conducting attacks in the real world, despite being obvious, semi-permanent and unmodifiable once deployed. In this paper, we propose Short-Lived Adversarial Perturbations (SLAP), a novel technique that allows adversaries to realize physically robust real-world AE by using a light projector. Attackers can project a specifically crafted adversarial perturbation onto a real-world object, transforming it into an AE. This allows the adversary greater control over the attack compared to adversarial patches: (i) projections can be dynamically turned on and off or modified at will, (ii) projections do not suffer from the locality constraint imposed by patches, making them harder to detect. We study the feasibility of SLAP in the self-driving scenario, targeting both object detector and traffic sign recognition tasks, focusing on the detection of stop signs. We conduct experiments in a variety of ambient light conditions, including outdoors, showing how in non-bright settings the proposed method generates AE that are extremely robust, causing misclassifications on state-of-the-art networks with up to 99% success rate for a variety of angles and distances. We also demostrate that SLAP-generated AE do not present detectable behaviours seen in adversarial patches and therefore bypass SentiNet, a physical AE detection method. We evaluate other defences including an adaptive defender using adversarial learning which is able to thwart the attack effectiveness up to 80% even in favourable attacker conditions.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Giulio Lovisotto",
        "Henry Turner",
        "Ivo Sluganovic",
        "Martin Strohmeier",
        "Ivan Martinovi\u0107"
      ],
      "url": "https://openalex.org/W3042075786",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773728",
      "fetched_at": "2026-01-09T13:52:13.806763",
      "classified_at": "2026-01-09T13:56:19.083693",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3042075786",
      "doi": "https://doi.org/10.48550/arxiv.2007.04137"
    },
    "2209.09577": {
      "paper_id": "2209.09577",
      "title": "Understanding Real-world Threats to Deep Learning Models in Android Apps",
      "abstract": "Famous for its superior performance, deep learning (DL) has been popularly used within many applications, which also at the same time attracts various threats to the models. One primary threat is from adversarial attacks. Researchers have intensively studied this threat for several years and proposed dozens of approaches to create adversarial examples (AEs). But most of the approaches are only evaluated on limited models and datasets (e.g., MNIST, CIFAR-10). Thus, the effectiveness of attacking real-world DL models is not quite clear. In this paper, we perform the first systematic study of adversarial attacks on real-world DNN models and provide a real-world model dataset named RWM. Particularly, we design a suite of approaches to adapt current AE generation algorithms to the diverse real-world DL models, including automatically extracting DL models from Android apps, capturing the inputs and outputs of the DL models in apps, generating AEs and validating them by observing the apps' execution. For black-box DL models, we design a semantic-based approach to build suitable datasets and use them for training substitute models when performing transfer-based attacks. After analyzing 245 DL models collected from 62,583 real-world apps, we have a unique opportunity to understand the gap between real-world DL models and contemporary AE generation algorithms. To our surprise, the current AE generation algorithms can only directly attack 6.53% of the models. Benefiting from our approach, the success rate upgrades to 47.35%.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Zizhuang Deng",
        "Kai Chen",
        "Guozhu Meng",
        "Xiaodong Zhang",
        "Ke Xu",
        "Yao Cheng"
      ],
      "url": "https://arxiv.org/abs/2209.09577",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773731",
      "fetched_at": "2026-01-09T12:14:35.773732",
      "classified_at": "2026-01-09T13:22:57.417919",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2302.09491": {
      "paper_id": "2302.09491",
      "title": "X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection",
      "abstract": "Adversarial attacks are valuable for evaluating the robustness of deep learning models. Existing attacks are primarily conducted on the visible light spectrum (e.g., pixel-wise texture perturbation). However, attacks targeting texture-free X-ray images remain underexplored, despite the widespread application of X-ray imaging in safety-critical scenarios such as the X-ray detection of prohibited items. In this paper, we take the first step toward the study of adversarial attacks targeted at X-ray prohibited item detection, and reveal the serious threats posed by such attacks in this safety-critical scenario. Specifically, we posit that successful physical adversarial attacks in this scenario should be specially designed to circumvent the challenges posed by color/texture fading and complex overlapping. To this end, we propose X-adv to generate physically printable metals that act as an adversarial agent capable of deceiving X-ray detectors when placed in luggage. To resolve the issues associated with color/texture fading, we develop a differentiable converter that facilitates the generation of 3D-printable objects with adversarial shapes, using the gradients of a surrogate model rather than directly generating adversarial textures. To place the printed 3D adversarial objects in luggage with complex overlapped instances, we design a policy-based reinforcement learning strategy to find locations eliciting strong attack performance in worst-case scenarios whereby the prohibited items are heavily occluded by other items. To verify the effectiveness of the proposed X-Adv, we conduct extensive experiments in both the digital and the physical world (employing a commercial X-ray security inspection system for the latter case). Furthermore, we present the physical-world X-ray adversarial attack dataset XAD.",
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [
        "Aishan Liu",
        "Jun Guo",
        "Jiakai Wang",
        "Siyuan Liang",
        "Renshuai Tao",
        "Wenbo Zhou",
        "Cong Liu",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "url": "https://arxiv.org/abs/2302.09491",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773734",
      "fetched_at": "2026-01-09T12:14:35.773735",
      "classified_at": "2026-01-09T13:22:58.393539",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_41f18639": {
      "paper_id": "seed_41f18639",
      "title": "That Person Moves Like A Car: Misclassification Attack Detection for Autonomous Systems Using Spatiotemporal Consistency",
      "abstract": "Smart cities are being developed worldwide with the use of technology to improve the quality of life of citizens and enhance their safety. Video surveillance is a key component of smart city infrastructure, as it involves the installation of cameras at strategic locations throughout the city for monitoring public spaces and providing real-time surveillance footage to law enforcement and other city representatives. Video surveillance systems have evolved rapidly in recent years, and are now integrated with advanced technologies like deep learning, blockchain, edge computing, and cloud computing. This study provides a comprehensive overview of video surveillance systems in smart cities, as well as the functions and challenges of those systems. The aim of this paper is to highlight the importance of video surveillance systems in smart cities and to provide insights into how they could be used to enhance safety, security, and the overall quality of life for citizens.",
      "year": 2023,
      "venue": "Electronics",
      "authors": [
        "Yanjinlkham Myagmar-Ochir",
        "Wooseong Kim"
      ],
      "url": "https://openalex.org/W4386141924",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773738",
      "fetched_at": "2026-01-09T13:52:14.617101",
      "classified_at": "2026-01-09T13:56:21.501255",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4386141924",
      "doi": "https://doi.org/10.3390/electronics12173567"
    },
    "seed_6697838b": {
      "paper_id": "seed_6697838b",
      "title": "You Can't See Me: Physical Removal Attacks on LiDAR-based Autonomous Vehicles Driving Frameworks",
      "abstract": "Autonomous Vehicles (AVs) increasingly use LiDAR-based object detection systems to perceive other vehicles and pedestrians on the road. While existing attacks on LiDAR-based autonomous driving architectures focus on lowering the confidence score of AV object detection models to induce obstacle misdetection, our research discovers how to leverage laser-based spoofing techniques to selectively remove the LiDAR point cloud data of genuine obstacles at the sensor level before being used as input to the AV perception. The ablation of this critical LiDAR information causes autonomous driving obstacle detectors to fail to identify and locate obstacles and, consequently, induces AVs to make dangerous automatic driving decisions. In this paper, we present a method invisible to the human eye that hides objects and deceives autonomous vehicles' obstacle detectors by exploiting inherent automatic transformation and filtering processes of LiDAR sensor data integrated with autonomous driving frameworks. We call such attacks Physical Removal Attacks (PRA), and we demonstrate their effectiveness against three popular AV obstacle detectors (Apollo, Autoware, PointPillars), and we achieve 45\u00b0 attack capability. We evaluate the attack impact on three fusion models (Frustum-ConvNet, AVOD, and Integrated-Semantic Level Fusion) and the consequences on the driving decision using LGSVL, an industry-grade simulator. In our moving vehicle scenarios, we achieve a 92.7% success rate removing 90\\% of a target obstacle's cloud points. Finally, we demonstrate the attack's success against two popular defenses against spoofing and object hiding attacks and discuss two enhanced defense strategies to mitigate our attack.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yulong Cao",
        "S. Hrushikesh Bhupathiraju",
        "Pirouz Naghavi",
        "Takeshi Sugawara",
        "Z. Morley Mao",
        "Sara Rampazzi"
      ],
      "url": "https://openalex.org/W4306887028",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773741",
      "fetched_at": "2026-01-09T13:52:15.510665",
      "classified_at": "2026-01-09T13:56:23.491339",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4306887028",
      "doi": "https://doi.org/10.48550/arxiv.2210.09482"
    },
    "seed_59ad97fd": {
      "paper_id": "seed_59ad97fd",
      "title": "CAPatch: Physical Adversarial Patch against Image Captioning Systems",
      "abstract": null,
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec23fall-prepub-121-zhang-shibo.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773744",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_7c3a5f4d": {
      "paper_id": "seed_7c3a5f4d",
      "title": "Exorcising \"Wraith\": Protecting LiDAR-based Object Detector in Automated Driving System from Appearing Attacks",
      "abstract": "Automated driving systems rely on 3D object detectors to recognize possible obstacles from LiDAR point clouds. However, recent works show the adversary can forge non-existent cars in the prediction results with a few fake points (i.e., appearing attack). By removing statistical outliers, existing defenses are however designed for specific attacks or biased by predefined heuristic rules. Towards more comprehensive mitigation, we first systematically inspect the mechanism of recent appearing attacks: Their common weaknesses are observed in crafting fake obstacles which (i) have obvious differences in the local parts compared with real obstacles and (ii) violate the physical relation between depth and point density. In this paper, we propose a novel plug-and-play defensive module which works by side of a trained LiDAR-based object detector to eliminate forged obstacles where a major proportion of local parts have low objectness, i.e., to what degree it belongs to a real object. At the core of our module is a local objectness predictor, which explicitly incorporates the depth information to model the relation between depth and point density, and predicts each local part of an obstacle with an objectness score. Extensive experiments show, our proposed defense eliminates at least 70% cars forged by three known appearing attacks in most cases, while, for the best previous defense, less than 30% forged cars are eliminated. Meanwhile, under the same circumstance, our defense incurs less overhead for AP/precision on cars compared with existing defenses. Furthermore, We validate the effectiveness of our proposed defense on simulation-based closed-loop control driving tests in the open-source system of Baidu's Apollo.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Qi\u2010Fan Xiao",
        "Xudong Pan",
        "Yifan Lu",
        "Mi Zhang",
        "Jiarun Dai",
        "Min Yang"
      ],
      "url": "https://openalex.org/W4353012846",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773747",
      "fetched_at": "2026-01-09T13:52:16.766185",
      "classified_at": "2026-01-09T13:56:25.439900",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4353012846",
      "doi": "https://doi.org/10.48550/arxiv.2303.09731"
    },
    "2401.03582": {
      "paper_id": "2401.03582",
      "title": "Invisible Reflections: Leveraging Infrared Laser Reflections to Target Traffic Sign Perception",
      "abstract": "All vehicles must follow the rules that govern traffic behavior, regardless of whether the vehicles are human-driven or Connected Autonomous Vehicles (CAVs). Road signs indicate locally active rules, such as speed limits and requirements to yield or stop. Recent research has demonstrated attacks, such as adding stickers or projected colored patches to signs, that cause CAV misinterpretation, resulting in potential safety issues. Humans can see and potentially defend against these attacks. But humans can not detect what they can not observe. We have developed an effective physical-world attack that leverages the sensitivity of filterless image sensors and the properties of Infrared Laser Reflections (ILRs), which are invisible to humans. The attack is designed to affect CAV cameras and perception, undermining traffic sign recognition by inducing misclassification. In this work, we formulate the threat model and requirements for an ILR-based traffic sign perception attack to succeed. We evaluate the effectiveness of the ILR attack with real-world experiments against two major traffic sign recognition architectures on four IR-sensitive cameras. Our black-box optimization methodology allows the attack to achieve up to a 100% attack success rate in indoor, static scenarios and a >80.5% attack success rate in our outdoor, moving vehicle scenarios. We find the latest state-of-the-art certifiable defense is ineffective against ILR attacks as it mis-certifies >33.5% of cases. To address this, we propose a detection strategy based on the physical properties of IR laser reflections which can detect 96% of ILR attacks.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Takami Sato",
        "Sri Hrushikesh Varma Bhupathiraju",
        "Michael Clifford",
        "Takeshi Sugawara",
        "Qi Alfred Chen",
        "Sara Rampazzi"
      ],
      "url": "https://arxiv.org/abs/2401.03582",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773750",
      "fetched_at": "2026-01-09T12:14:35.773751",
      "classified_at": "2026-01-09T13:22:59.064293",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_02261740": {
      "paper_id": "seed_02261740",
      "title": "Avara: A Uniform Evaluation System for Perceptibility Analysis Against Adversarial Object Evasion Attacks",
      "abstract": null,
      "year": 2024,
      "venue": "CCS",
      "authors": [],
      "url": "https://drive.google.com/file/d/16qfqZpOED2W3wXmGibdDOIK5ctboend7/view",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773754",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_de1f4a5e": {
      "paper_id": "seed_de1f4a5e",
      "title": "VisionGuard: Secure and Robust Visual Perception of Autonomous Vehicles in Practice",
      "abstract": null,
      "year": 2024,
      "venue": "CCS",
      "authors": [],
      "url": "hhttps://tianweiz07.github.io/Papers/24-ccs1.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773756",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_f78c1c68": {
      "paper_id": "seed_f78c1c68",
      "title": "Invisible but Detected: Physical Adversarial Shadow Attack and Defense on LiDAR Object Detection",
      "abstract": "A convolutional neural network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention from both industry and academia in the past few years. The existing reviews mainly focus on CNN's applications in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide some novel ideas and prospects in this fast-growing field. Besides, not only 2-D convolution but also 1-D and multidimensional ones are involved. First, this review introduces the history of CNN. Second, we provide an overview of various convolutions. Third, some classic and advanced CNN models are introduced; especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for functions and hyperparameter selection. Fifth, the applications of 1-D, 2-D, and multidimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed as guidelines for future work.",
      "year": 2021,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Zewen Li",
        "Fan Liu",
        "Wenjie Yang",
        "Shouheng Peng",
        "Jun Zhou"
      ],
      "url": "https://openalex.org/W3168997536",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773759",
      "fetched_at": "2026-01-09T13:52:18.777205",
      "classified_at": "2026-01-09T13:56:27.343086",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3168997536",
      "doi": "https://doi.org/10.1109/tnnls.2021.3084827"
    },
    "seed_1f7c2241": {
      "paper_id": "seed_1f7c2241",
      "title": "From Threat to Trust: Exploiting Attention Mechanisms for Attacks and Defenses in Cooperative Perception",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-wang-chenyi.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773761",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_caf294a5": {
      "paper_id": "seed_caf294a5",
      "title": "Adversarial Policy Training against Deep Reinforcement Learning",
      "abstract": null,
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec21summer_wu-xian.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773764",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2402.03741": {
      "paper_id": "2402.03741",
      "title": "SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems",
      "abstract": "Recent advancements in multi-agent reinforcement learning (MARL) have opened up vast application prospects, such as swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent research reveals that attackers can rapidly exploit the victim's vulnerabilities, generating adversarial policies that result in the failure of specific tasks. For instance, reducing the winning rate of a superhuman-level Go AI to around 20%. Existing studies predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY) that incorporates the concept of constructing multiple subgames to mitigate the impact of partial observability and suggests sharing transitions among subpolicies to improve attackers' exploitative ability. Extensive evaluations demonstrate the effectiveness of SUB-PLAY under three typical partial observability limitations. Visualization results indicate that adversarial policies induce significantly different activations of the victims' policy networks. Furthermore, we evaluate three potential defenses aimed at exploring ways to mitigate security threats posed by adversarial policies, providing constructive recommendations for deploying MARL in competitive environments.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Oubo Ma",
        "Yuwen Pu",
        "Linkang Du",
        "Yang Dai",
        "Ruo Wang",
        "Xiaolei Liu",
        "Yingcai Wu",
        "Shouling Ji"
      ],
      "url": "https://arxiv.org/abs/2402.03741",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773767",
      "fetched_at": "2026-01-09T12:14:35.773768",
      "classified_at": "2026-01-09T13:22:59.667721",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_2f948b9d": {
      "paper_id": "seed_2f948b9d",
      "title": "CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization",
      "abstract": "Deep reinforcement learning (DRL) has gained widespread adoption in control and decision-making tasks due to its strong performance in dynamic environments. However, DRL agents are vulnerable to noisy observations and adversarial attacks, and concerns about the adversarial robustness of DRL systems have emerged. Recent efforts have focused on addressing these robustness issues by establishing rigorous theoretical guarantees for the returns achieved by DRL agents in adversarial settings. Among these approaches, policy smoothing has proven to be an effective and scalable method for certifying the robustness of DRL agents. Nevertheless, existing certifiably robust DRL relies on policies trained with simple Gaussian augmentations, resulting in a suboptimal trade-off between certified robustness and certified return. To address this issue, we introduce a novel paradigm dubbed \\texttt{C}ertified-r\\texttt{A}dius-\\texttt{M}aximizing \\texttt{P}olicy (\\texttt{CAMP}) training. \\texttt{CAMP} is designed to enhance DRL policies, achieving better utility without compromising provable robustness. By leveraging the insight that the global certified radius can be derived from local certified radii based on training-time statistics, \\texttt{CAMP} formulates a surrogate loss related to the local certified radius and optimizes the policy guided by this surrogate loss. We also introduce \\textit{policy imitation} as a novel technique to stabilize \\texttt{CAMP} training. Experimental results demonstrate that \\texttt{CAMP} significantly improves the robustness-return trade-off across various tasks. Based on the results, \\texttt{CAMP} can achieve up to twice the certified expected return compared to that of baselines. Our code is available at https://github.com/NeuralSec/camp-robust-rl.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Derui Wang",
        "Kristen Moore",
        "Diksha Goel",
        "Minjune Kim",
        "Gang Li",
        "Yang Li",
        "Robin Doss",
        "Minhui Xue",
        "Bo Li",
        "Seyit Camtepe",
        "Liming Zhu"
      ],
      "url": "https://openalex.org/W4406975609",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773770",
      "fetched_at": "2026-01-09T13:52:20.927453",
      "classified_at": "2026-01-09T13:56:29.165782",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4406975609",
      "doi": "https://doi.org/10.48550/arxiv.2501.17667"
    },
    "seed_815d0a56": {
      "paper_id": "seed_815d0a56",
      "title": "Cost-Aware Robust Tree Ensembles for Security Applications",
      "abstract": "There are various costs for attackers to manipulate the features of security classifiers. The costs are asymmetric across features and to the directions of changes, which cannot be precisely captured by existing cost models based on $L_p$-norm robustness. In this paper, we utilize such domain knowledge to increase the attack cost of evading classifiers, specifically, tree ensemble models that are widely used by security tasks. We propose a new cost modeling method to capture the feature manipulation cost as constraint, and then we integrate the cost-driven constraint into the node construction process to train robust tree ensembles. During the training process, we use the constraint to find data points that are likely to be perturbed given the feature manipulation cost, and we use a new robust training algorithm to optimize the quality of the trees. Our cost-aware training method can be applied to different types of tree ensembles, including gradient boosted decision trees and random forest models. Using Twitter spam detection as the case study, our evaluation results show that we can increase the attack cost by 10.6X compared to the baseline. Moreover, our robust training method using cost-driven constraint can achieve higher accuracy, lower false positive rate, and stronger cost-aware robustness than the state-of-the-art training method using $L_\\infty$-norm cost model. Our code is available at https://github.com/surrealyz/growtrees.",
      "year": 2019,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yizheng Chen",
        "Shiqi Wang",
        "Weifan Jiang",
        "Asaf Cidon",
        "Suman Jana"
      ],
      "url": "https://openalex.org/W3026606204",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773775",
      "fetched_at": "2026-01-09T13:52:21.701940",
      "classified_at": "2026-01-09T13:56:30.976450",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3026606204",
      "doi": "https://doi.org/10.48550/arxiv.1912.01149"
    },
    "seed_af0214b4": {
      "paper_id": "seed_af0214b4",
      "title": "CADE: Detecting and Explaining Concept Drift Samples for Security Applications",
      "abstract": null,
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec21-yang-limin.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773778",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2105.11363": {
      "paper_id": "2105.11363",
      "title": "Learning Security Classifiers with Verified Global Robustness Properties",
      "abstract": "Many recent works have proposed methods to train classifiers with local robustness properties, which can provably eliminate classes of evasion attacks for most inputs, but not all inputs. Since data distribution shift is very common in security applications, e.g., often observed for malware detection, local robustness cannot guarantee that the property holds for unseen inputs at the time of deploying the classifier. Therefore, it is more desirable to enforce global robustness properties that hold for all inputs, which is strictly stronger than local robustness.   In this paper, we present a framework and tools for training classifiers that satisfy global robustness properties. We define new notions of global robustness that are more suitable for security classifiers. We design a novel booster-fixer training framework to enforce global robustness properties. We structure our classifier as an ensemble of logic rules and design a new verifier to verify the properties. In our training algorithm, the booster increases the classifier's capacity, and the fixer enforces verified global robustness properties following counterexample guided inductive synthesis.   We show that we can train classifiers to satisfy different global robustness properties for three security datasets, and even multiple properties at the same time, with modest impact on the classifier's performance. For example, we train a Twitter spam account classifier to satisfy five global robustness properties, with 5.4% decrease in true positive rate, and 0.1% increase in false positive rate, compared to a baseline XGBoost model that doesn't satisfy any property.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Yizheng Chen",
        "Shiqi Wang",
        "Yue Qin",
        "Xiaojing Liao",
        "Suman Jana",
        "David Wagner"
      ],
      "url": "https://arxiv.org/abs/2105.11363",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773780",
      "fetched_at": "2026-01-09T12:14:35.773781",
      "classified_at": "2026-01-09T13:23:00.263824",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2105.08619": {
      "paper_id": "2105.08619",
      "title": "On the Robustness of Domain Constraints",
      "abstract": "Machine learning is vulnerable to adversarial examples-inputs designed to cause models to perform poorly. However, it is unclear if adversarial examples represent realistic inputs in the modeled domains. Diverse domains such as networks and phishing have domain constraints-complex relationships between features that an adversary must satisfy for an attack to be realized (in addition to any adversary-specific goals). In this paper, we explore how domain constraints limit adversarial capabilities and how adversaries can adapt their strategies to create realistic (constraint-compliant) examples. In this, we develop techniques to learn domain constraints from data, and show how the learned constraints can be integrated into the adversarial crafting process. We evaluate the efficacy of our approach in network intrusion and phishing datasets and find: (1) up to 82% of adversarial examples produced by state-of-the-art crafting algorithms violate domain constraints, (2) domain constraints are robust to adversarial examples; enforcing constraints yields an increase in model accuracy by up to 34%. We observe not only that adversaries must alter inputs to satisfy domain constraints, but that these constraints make the generation of valid adversarial examples far more challenging.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Ryan Sheatsley",
        "Blaine Hoak",
        "Eric Pauley",
        "Yohan Beugin",
        "Michael J. Weisman",
        "Patrick McDaniel"
      ],
      "url": "https://arxiv.org/abs/2105.08619",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773784",
      "fetched_at": "2026-01-09T12:14:35.773785",
      "classified_at": "2026-01-09T13:23:01.350013",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_56b12302": {
      "paper_id": "seed_56b12302",
      "title": "Cert-RNN: Towards Certifying the Robustness of Recurrent Neural Networks",
      "abstract": "Certifiable robustness, the functionality of verifying whether the given region surrounding a data point admits any adversarial example, provides guaranteed security for neural networks deployed in adversarial environments. A plethora of work has been proposed to certify the robustness of feed-forward networks, e.g., FCNs and CNNs. Yet, most existing methods cannot be directly applied to recurrent neural networks (RNNs), due to their sequential inputs and unique operations.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Tianyu Du",
        "Shouling Ji",
        "Lujia Shen",
        "Yao Zhang",
        "Jinfeng Li",
        "Jie Shi",
        "Chengfang Fang",
        "Jianwei Yin",
        "Raheem Beyah",
        "Ting Wang"
      ],
      "url": "https://openalex.org/W3214321642",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773787",
      "fetched_at": "2026-01-09T13:52:24.198608",
      "classified_at": "2026-01-09T13:56:32.790703",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3214321642",
      "doi": "https://doi.org/10.1145/3460120.3484538"
    },
    "2002.12398": {
      "paper_id": "2002.12398",
      "title": "TSS: Transformation-Specific Smoothing for Robustness Certification",
      "abstract": "As machine learning (ML) systems become pervasive, safeguarding their security is critical. However, recently it has been demonstrated that motivated adversaries are able to mislead ML systems by perturbing test data using semantic transformations. While there exists a rich body of research providing provable robustness guarantees for ML models against $\\ell_p$ norm bounded adversarial perturbations, guarantees against semantic perturbations remain largely underexplored. In this paper, we provide TSS -- a unified framework for certifying ML robustness against general adversarial semantic transformations. First, depending on the properties of each transformation, we divide common transformations into two categories, namely resolvable (e.g., Gaussian blur) and differentially resolvable (e.g., rotation) transformations. For the former, we propose transformation-specific randomized smoothing strategies and obtain strong robustness certification. The latter category covers transformations that involve interpolation errors, and we propose a novel approach based on stratified sampling to certify the robustness. Our framework TSS leverages these certification strategies and combines with consistency-enhanced training to provide rigorous certification of robustness. We conduct extensive experiments on over ten types of challenging semantic transformations and show that TSS significantly outperforms the state of the art. Moreover, to the best of our knowledge, TSS is the first approach that achieves nontrivial certified robustness on the large-scale ImageNet dataset. For instance, our framework achieves 30.4% certified robust accuracy against rotation attack (within $\\pm 30^\\circ$) on ImageNet. Moreover, to consider a broader range of transformations, we show TSS is also robust against adaptive attacks and unforeseen image corruptions such as CIFAR-10-C and ImageNet-C.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Linyi Li",
        "Maurice Weber",
        "Xiaojun Xu",
        "Luka Rimanic",
        "Bhavya Kailkhura",
        "Tao Xie",
        "Ce Zhang",
        "Bo Li"
      ],
      "url": "https://arxiv.org/abs/2002.12398",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773790",
      "fetched_at": "2026-01-09T12:14:35.773791",
      "classified_at": "2026-01-09T13:23:02.039417",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_ea304a74": {
      "paper_id": "seed_ea304a74",
      "title": "Transcend: Detecting Concept Drift in Malware Classification Models",
      "abstract": "Building machine learning models of malware behavior is widely accepted as a panacea towards effective malware classification. A crucial requirement for building sustainable learning models, though, is to train on a wide variety of malware samples. Unfortunately, malware evolves rapidly and it thus becomes hard\u2014if not impossible\u2014to generalize learning models to reflect future, previously-unseen behaviors. Consequently, most malware classifiers become unsustainable in the long run, becoming rapidly antiquated as malware continues to evolve. In this work, we propose Transcend, a framework to identify aging classification models in vivo during deployment, much before the machine learning model\u2019s performance starts to degrade. This is a significant departure from conventional approaches that retrain aging models retrospectively when poor performance is observed. Our approach uses a statistical comparison of samples seen during deployment with those used to train the model, thereby building metrics for prediction quality. We show how Transcend can be used to identify concept drift based on two separate case studies on Android andWindows malware, raising a red flag before the model starts making consistently poor decisions due to out-of-date training.",
      "year": 2017,
      "venue": "UCL Discovery (University College London)",
      "authors": [
        "Roberto Jordaney",
        "Kumar Sharad",
        "Santanu Kumar Dash",
        "Zhi Wang",
        "Davide Papini",
        "Ilia Nouretdinov",
        "Lorenzo Cavallaro"
      ],
      "url": "https://openalex.org/W2753594008",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773795",
      "fetched_at": "2026-01-09T13:52:24.945973",
      "classified_at": "2026-01-09T13:56:34.834874",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2753594008"
    },
    "seed_d8fa3e95": {
      "paper_id": "seed_d8fa3e95",
      "title": "Transcending Transcend: Revisiting Malware Classification in the Presence of Concept Drift",
      "abstract": "Machine learning for malware classification shows encouraging results, but real deployments suffer from performance degradation as malware authors adapt their techniques to evade detection. This phenomenon, known as concept drift, occurs as new malware examples evolve and become less and less like the original training examples. One promising method to cope with concept drift is classification with rejection in which examples that are likely to be misclassified are instead quarantined until they can be expertly analyzed. We propose TRANSCENDENT, a rejection framework built on Transcend, a recently proposed strategy based on conformal prediction theory. In particular, we provide a formal treatment of Transcend, enabling us to refine conformal evaluation theory -- its underlying statistical engine -- and gain a better understanding of the theoretical reasons for its effectiveness. In the process, we develop two additional conformal evaluators that match or surpass the performance of the original while significantly decreasing the computational overhead. We evaluate TRANSCENDENT on a malware dataset spanning 5 years that removes sources of experimental bias present in the original evaluation. TRANSCENDENT outperforms state-of-the-art approaches while generalizing across different malware domains and classifiers. To further assist practitioners, we determine the optimal operational settings for a TRANSCENDENT deployment and show how it can be applied to many popular learning algorithms. These insights support both old and new empirical findings, making Transcend a sound and practical solution for the first time. To this end, we release TRANSCENDENT as open source, to aid the adoption of rejection strategies by the security community.",
      "year": 2022,
      "venue": "2022 IEEE Symposium on Security and Privacy (SP)",
      "authors": [
        "Federico Barbero",
        "Feargus Pendlebury",
        "Fabio Pierazzi",
        "Lorenzo Cavallaro"
      ],
      "url": "https://openalex.org/W3110824252",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773797",
      "fetched_at": "2026-01-09T13:52:26.465502",
      "classified_at": "2026-01-09T13:56:36.909675",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3110824252",
      "doi": "https://doi.org/10.1109/sp46214.2022.9833659"
    },
    "seed_cd05f758": {
      "paper_id": "seed_cd05f758",
      "title": "Transferring Adversarial Robustness Through Robust Representation Matching",
      "abstract": "With the widespread use of machine learning, concerns over its security and reliability have become prevalent. As such, many have developed defenses to harden neural networks against adversarial examples, imperceptibly perturbed inputs that are reliably misclassified. Adversarial training in which adversarial examples are generated and used during training is one of the few known defenses able to reliably withstand such attacks against neural networks. However, adversarial training imposes a significant training overhead and scales poorly with model complexity and input dimension. In this paper, we propose Robust Representation Matching (RRM), a low-cost method to transfer the robustness of an adversarially trained model to a new model being trained for the same task irrespective of architectural differences. Inspired by student-teacher learning, our method introduces a novel training loss that encourages the student to learn the teacher's robust representations. Compared to prior works, RRM is superior with respect to both model performance and adversarial training time. On CIFAR-10, RRM trains a robust model $\\sim 1.8\\times$ faster than the state-of-the-art. Furthermore, RRM remains effective on higher-dimensional datasets. On Restricted-ImageNet, RRM trains a ResNet50 model $\\sim 18\\times$ faster than standard adversarial training.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "P. Vaishnavi",
        "Kevin Eykholt",
        "Amir Rahmati"
      ],
      "url": "https://openalex.org/W4221166176",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML07",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773800",
      "fetched_at": "2026-01-09T13:52:27.292986",
      "classified_at": "2026-01-09T13:56:38.760511",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4221166176",
      "doi": "https://doi.org/10.48550/arxiv.2202.09994"
    },
    "seed_2a9592ec": {
      "paper_id": "seed_2a9592ec",
      "title": "DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing",
      "abstract": "Diffusion models have been leveraged to perform adversarial purification and thus provide both empirical and certified robustness for a standard model. On the other hand, different robustly trained smoothed models have been studied to improve the certified robustness. Thus, it raises a natural question: Can diffusion model be used to achieve improved certified robustness on those robustly trained smoothed models? In this work, we first theoretically show that recovered instances by diffusion models are in the bounded neighborhood of the original instance with high probability; and the \"one-shot\" denoising diffusion probabilistic models (DDPM) can approximate the mean of the generated distribution of a continuous-time diffusion model, which approximates the original instance under mild conditions. Inspired by our analysis, we propose a certifiably robust pipeline DiffSmooth, which first performs adversarial purification via diffusion models and then maps the purified instances to a common region via a simple yet effective local smoothing strategy. We conduct extensive experiments on different datasets and show that DiffSmooth achieves SOTA-certified robustness compared with eight baselines. For instance, DiffSmooth improves the SOTA-certified accuracy from $36.0\\%$ to $53.0\\%$ under $\\ell_2$ radius $1.5$ on ImageNet. The code is available at [https://github.com/javyduck/DiffSmooth].",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jiawei Zhang",
        "Zhong\u2010Zhu Chen",
        "Huan Zhang",
        "Chaowei Xiao",
        "Bo Li"
      ],
      "url": "https://openalex.org/W4386272826",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773803",
      "fetched_at": "2026-01-09T13:52:27.811422",
      "classified_at": "2026-01-09T13:56:40.833370",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4386272826",
      "doi": "https://doi.org/10.48550/arxiv.2308.14333"
    },
    "seed_5a511665": {
      "paper_id": "seed_5a511665",
      "title": "Anomaly Detection in the Open World: Normality Shift Detection, Explanation, and Adaptation",
      "abstract": "Concept drift is one of the most frustrating challenges for learning-based security applications built on the closeworld assumption of identical distribution between training and deployment.Anomaly detection, one of the most important tasks in security domains, is instead immune to the drift of abnormal behavior due to the training without any abnormal data (known as zero-positive), which however comes at the cost of more severe impacts when normality shifts.However, existing studies mainly focus on concept drift of abnormal behaviour and/or supervised learning, leaving the normality shift for zero-positive anomaly detection largely unexplored.In this work, we are the first to explore the normality shift for deep learning-based anomaly detection in security applications, and propose OWAD, a general framework to detect, explain, and adapt to normality shift in practice.In particular, OWAD outperforms prior work by detecting shift in an unsupervised fashion, reducing the overhead of manual labeling, and providing better adaptation performance through distribution-level tackling.We demonstrate the effectiveness of OWAD through several realistic experiments on three security-related anomaly detection applications with long-term practical data.Results show that OWAD can provide better adaptation performance of normality shift with less labeling overhead.We provide case studies to analyze the normality shift and provide operational recommendations for security applications.We also conduct an initial real-world deployment on a SCADA security system.1 Normality shift intuitively refers to the change of distribution of normal data (detailed definition is in \u00a7II-C).In this paper, we interchangeably use terms \"drift\" and \"shift\".We tend to use \"normality shift\" as a whole term.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Dongqi Han",
        "Zhiliang Wang",
        "Wenqi Chen",
        "Kai Wang",
        "Rui Yu",
        "Su Wang",
        "Han Zhang",
        "Zhihua Wang",
        "Minghui Jin",
        "Jiahai Yang",
        "Xingang Shi",
        "Xia Yin"
      ],
      "url": "https://openalex.org/W4324007053",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773807",
      "fetched_at": "2026-01-09T13:52:28.617834",
      "classified_at": "2026-01-09T13:56:42.860998",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4324007053",
      "doi": "https://doi.org/10.14722/ndss.2023.24830"
    },
    "seed_4a39fd44": {
      "paper_id": "seed_4a39fd44",
      "title": "BARS: Local Robustness Certification for Deep Learning based Traffic Analysis Systems",
      "abstract": "Deep learning (DL) performs well in many traffic analysis tasks.Nevertheless, the vulnerability of deep learning weakens the real-world performance of these traffic analyzers (e.g., suffering from evasion attack).Many studies in recent years focused on robustness certification for DL-based models.But existing methods perform far from perfectly in the traffic analysis domain.In this paper, we try to match three attributes of DL-based traffic analysis systems at the same time: (1) highly heterogeneous features, (2) varied model designs, (3) adversarial operating environments.Therefore, we propose BARS, a general robustness certification framework for DL-based traffic analysis systems based on boundary-adaptive randomized smoothing.To obtain tighter robustness guarantee, BARS uses optimized smoothing noise converging on the classification boundary.We firstly propose the Distribution Transformer for generating optimized smoothing noise.Then to optimize the smoothing noise, we propose some special distribution functions and two gradient based searching algorithms for noise shape and noise scale.We implement and evaluate BARS in three practical DL-based traffic analysis systems.Experiment results show that BARS can achieve tighter robustness guarantee than baseline methods.Furthermore, we illustrate the practicability of BARS through five application cases (e.g., quantitatively evaluating robustness).",
      "year": 2023,
      "venue": null,
      "authors": [
        "Kai Wang",
        "Zhiliang Wang",
        "Dongqi Han",
        "Wenqi Chen",
        "Jiahai Yang",
        "Xingang Shi",
        "Xia Yin"
      ],
      "url": "https://openalex.org/W4324007187",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773810",
      "fetched_at": "2026-01-09T13:52:29.430533",
      "classified_at": "2026-01-09T13:56:44.966791",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4324007187",
      "doi": "https://doi.org/10.14722/ndss.2023.24508"
    },
    "2301.02905": {
      "paper_id": "2301.02905",
      "title": "REaaS: Enabling Adversarially Robust Downstream Classifiers via Robust Encoder as a Service",
      "abstract": "Encoder as a service is an emerging cloud service. Specifically, a service provider first pre-trains an encoder (i.e., a general-purpose feature extractor) via either supervised learning or self-supervised learning and then deploys it as a cloud service API. A client queries the cloud service API to obtain feature vectors for its training/testing inputs when training/testing its classifier (called downstream classifier). A downstream classifier is vulnerable to adversarial examples, which are testing inputs with carefully crafted perturbation that the downstream classifier misclassifies. Therefore, in safety and security critical applications, a client aims to build a robust downstream classifier and certify its robustness guarantees against adversarial examples.   What APIs should the cloud service provide, such that a client can use any certification method to certify the robustness of its downstream classifier against adversarial examples while minimizing the number of queries to the APIs? How can a service provider pre-train an encoder such that clients can build more certifiably robust downstream classifiers? We aim to answer the two questions in this work. For the first question, we show that the cloud service only needs to provide two APIs, which we carefully design, to enable a client to certify the robustness of its downstream classifier with a minimal number of queries to the APIs. For the second question, we show that an encoder pre-trained using a spectral-norm regularization term enables clients to build more robust downstream classifiers.",
      "year": 2023,
      "venue": "NDSS",
      "authors": [
        "Wenjie Qu",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://arxiv.org/abs/2301.02905",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773812",
      "fetched_at": "2026-01-09T12:14:35.773813",
      "classified_at": "2026-01-09T13:23:02.862593",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2302.04332": {
      "paper_id": "2302.04332",
      "title": "Continuous Learning for Android Malware Detection",
      "abstract": "Machine learning methods can detect Android malware with very high accuracy. However, these classifiers have an Achilles heel, concept drift: they rapidly become out of date and ineffective, due to the evolution of malware apps and benign apps. Our research finds that, after training an Android malware classifier on one year's worth of data, the F1 score quickly dropped from 0.99 to 0.76 after 6 months of deployment on new test samples.   In this paper, we propose new methods to combat the concept drift problem of Android malware classifiers. Since machine learning technique needs to be continuously deployed, we use active learning: we select new samples for analysts to label, and then add the labeled samples to the training set to retrain the classifier. Our key idea is, similarity-based uncertainty is more robust against concept drift. Therefore, we combine contrastive learning with active learning. We propose a new hierarchical contrastive learning scheme, and a new sample selection technique to continuously train the Android malware classifier. Our evaluation shows that this leads to significant improvements, compared to previously published methods for active learning. Our approach reduces the false negative rate from 14% (for the best baseline) to 9%, while also reducing the false positive rate (from 0.86% to 0.48%). Also, our approach maintains more consistent performance across a seven-year time period than past methods.",
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [
        "Yizheng Chen",
        "Zhoujie Ding",
        "David Wagner"
      ],
      "url": "https://arxiv.org/abs/2302.04332",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773816",
      "fetched_at": "2026-01-09T12:14:35.773817",
      "classified_at": "2026-01-09T13:23:03.872084",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2202.01811": {
      "paper_id": "2202.01811",
      "title": "ObjectSeeker: Certifiably Robust Object Detection against Patch Hiding Attacks via Patch-agnostic Masking",
      "abstract": "Object detectors, which are widely deployed in security-critical systems such as autonomous vehicles, have been found vulnerable to patch hiding attacks. An attacker can use a single physically-realizable adversarial patch to make the object detector miss the detection of victim objects and undermine the functionality of object detection applications. In this paper, we propose ObjectSeeker for certifiably robust object detection against patch hiding attacks. The key insight in ObjectSeeker is patch-agnostic masking: we aim to mask out the entire adversarial patch without knowing the shape, size, and location of the patch. This masking operation neutralizes the adversarial effect and allows any vanilla object detector to safely detect objects on the masked images. Remarkably, we can evaluate ObjectSeeker's robustness in a certifiable manner: we develop a certification procedure to formally determine if ObjectSeeker can detect certain objects against any white-box adaptive attack within the threat model, achieving certifiable robustness. Our experiments demonstrate a significant (~10%-40% absolute and ~2-6x relative) improvement in certifiable robustness over the prior work, as well as high clean performance (~1% drop compared with undefended models).",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Chong Xiang",
        "Alexander Valtchanov",
        "Saeed Mahloujifar",
        "Prateek Mittal"
      ],
      "url": "https://arxiv.org/abs/2202.01811",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773820",
      "fetched_at": "2026-01-09T12:14:35.773821",
      "classified_at": "2026-01-09T13:23:04.552004",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2202.03277": {
      "paper_id": "2202.03277",
      "title": "On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks",
      "abstract": "While the literature on security attacks and defense of Machine Learning (ML) systems mostly focuses on unrealistic adversarial examples, recent research has raised concern about the under-explored field of realistic adversarial attacks and their implications on the robustness of real-world systems. Our paper paves the way for a better understanding of adversarial robustness against realistic attacks and makes two major contributions. First, we conduct a study on three real-world use cases (text classification, botnet detection, malware detection)) and five datasets in order to evaluate whether unrealistic adversarial examples can be used to protect models against realistic examples. Our results reveal discrepancies across the use cases, where unrealistic examples can either be as effective as the realistic ones or may offer only limited improvement. Second, to explain these results, we analyze the latent representation of the adversarial examples generated with realistic and unrealistic attacks. We shed light on the patterns that discriminate which unrealistic examples can be used for effective hardening. We release our code, datasets and models to support future research in exploring how to reduce the gap between unrealistic and realistic adversarial attacks.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Salijona Dyrmishi",
        "Salah Ghamizi",
        "Thibault Simonetto",
        "Yves Le Traon",
        "Maxime Cordy"
      ],
      "url": "https://arxiv.org/abs/2202.03277",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773823",
      "fetched_at": "2026-01-09T12:14:35.773824",
      "classified_at": "2026-01-09T13:23:05.210030",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2307.16630": {
      "paper_id": "2307.16630",
      "title": "Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks",
      "abstract": "The language models, especially the basic text classification models, have been shown to be susceptible to textual adversarial attacks such as synonym substitution and word insertion attacks. To defend against such attacks, a growing body of research has been devoted to improving the model robustness. However, providing provable robustness guarantees instead of empirical robustness is still widely unexplored. In this paper, we propose Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing. To our best knowledge, existing certified schemes for NLP can only certify the robustness against $\\ell_0$ perturbations in synonym substitution attacks. Representing each word-level adversarial operation (i.e., synonym substitution, word reordering, insertion, and deletion) as a combination of permutation and embedding transformation, we propose novel smoothing theorems to derive robustness bounds in both permutation and embedding space against such adversarial operations. To further improve certified accuracy and radius, we consider the numerical relationships between discrete words and select proper noise distributions for the randomized smoothing. Finally, we conduct substantial experiments on multiple language models and datasets. Text-CRS can address all four different word-level adversarial operations and achieve a significant accuracy improvement. We also provide the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Xinyu Zhang",
        "Hanbin Hong",
        "Yuan Hong",
        "Peng Huang",
        "Binghui Wang",
        "Zhongjie Ba",
        "Kui Ren"
      ],
      "url": "https://arxiv.org/abs/2307.16630",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773827",
      "fetched_at": "2026-01-09T12:14:35.773828",
      "classified_at": "2026-01-09T13:23:06.120759",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2309.11005": {
      "paper_id": "2309.11005",
      "title": "It's Simplex! Disaggregating Measures to Improve Certified Robustness",
      "abstract": "Certified robustness circumvents the fragility of defences against adversarial attacks, by endowing model predictions with guarantees of class invariance for attacks up to a calculated size. While there is value in these certifications, the techniques through which we assess their performance do not present a proper accounting of their strengths and weaknesses, as their analysis has eschewed consideration of performance over individual samples in favour of aggregated measures. By considering the potential output space of certified models, this work presents two distinct approaches to improve the analysis of certification mechanisms, that allow for both dataset-independent and dataset-dependent measures of certification performance. Embracing such a perspective uncovers new certification approaches, which have the potential to more than double the achievable radius of certification, relative to current state-of-the-art. Empirical evaluation verifies that our new approach can certify $9\\%$ more samples at noise scale $\u03c3= 1$, with greater relative improvements observed as the difficulty of the predictive task increases.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Andrew C. Cullen",
        "Paul Montague",
        "Shijie Liu",
        "Sarah M. Erfani",
        "Benjamin I. P. Rubinstein"
      ],
      "url": "https://arxiv.org/abs/2309.11005",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773831",
      "fetched_at": "2026-01-09T12:14:35.773832",
      "classified_at": "2026-01-09T13:23:06.725653",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_09f03fa8": {
      "paper_id": "seed_09f03fa8",
      "title": "SoK: Efficiency Robustness of Dynamic Deep Learning Systems",
      "abstract": "The past decade has witnessed the rapid evolution in blockchain technologies, which has attracted tremendous interests from both the research communities and industries. The blockchain network was originated from the Internet financial sector as a decentralized, immutable ledger system for transactional data ordering. Nowadays, it is envisioned as a powerful backbone/framework for decentralized data processing and data-driven self-organization in flat, open-access networks. In particular, the plausible characteristics of decentralization, immutability, and self-organization are primarily owing to the unique decentralized consensus mechanisms introduced by blockchain networks. This survey is motivated by the lack of a comprehensive literature review on the development of decentralized consensus mechanisms in blockchain networks. In this paper, we provide a systematic vision of the organization of blockchain networks. By emphasizing the unique characteristics of decentralized consensus in blockchain networks, our in-depth review of the state-of-the-art consensus protocols is focused on both the perspective of distributed consensus system design and the perspective of incentive mechanism design. From a game-theoretic point of view, we also provide a thorough review of the strategy adopted for self-organization by the individual nodes in the blockchain backbone networks. Consequently, we provide a comprehensive survey of the emerging applications of blockchain networks in a broad area of telecommunication. We highlight our special interest in how the consensus mechanisms impact these applications. Finally, we discuss several open issues in the protocol design for blockchain consensus and the related potential research directions.",
      "year": 2019,
      "venue": "IEEE Access",
      "authors": [
        "Wenbo Wang",
        "Dinh Thai Hoang",
        "Peizhao Hu",
        "Zehui Xiong",
        "Dusit Niyato",
        "Ping Wang",
        "Yonggang Wen",
        "Dong In Kim"
      ],
      "url": "https://openalex.org/W2905867785",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773834",
      "fetched_at": "2026-01-09T13:52:30.226746",
      "classified_at": "2026-01-09T13:56:46.841791",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2905867785",
      "doi": "https://doi.org/10.1109/access.2019.2896108"
    },
    "seed_8eb535d6": {
      "paper_id": "seed_8eb535d6",
      "title": "AGNNCert: Defending Graph Neural Networks against Arbitrary Perturbations with Deterministic Certification",
      "abstract": "Graph neural networks (GNNs) achieve the state-of-the-art on graph-relevant tasks such as node and graph classification. However, recent works show GNNs are vulnerable to adversarial perturbations include the perturbation on edges, nodes, and node features, the three components forming a graph. Empirical defenses against such attacks are soon broken by adaptive ones. While certified defenses offer robustness guarantees, they face several limitations: 1) almost all restrict the adversary's capability to only one type of perturbation, which is impractical; 2) all are designed for a particular GNN task, which limits their applicability; and 3) the robustness guarantees of all methods except one are not 100% accurate. We address all these limitations by developing AGNNCert, the first certified defense for GNNs against arbitrary (edge, node, and node feature) perturbations with deterministic robustness guarantees, and applicable to the two most common node and graph classification tasks. AGNNCert also encompass existing certified defenses as special cases. Extensive evaluations on multiple benchmark node/graph classification datasets and two real-world graph datasets, and multiple GNNs validate the effectiveness of AGNNCert to provably defend against arbitrary perturbations. AGNNCert also shows its superiority over the state-of-the-art certified defenses against the individual edge perturbation and node perturbation.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jiate Li",
        "Binghui Wang"
      ],
      "url": "https://openalex.org/W4407124074",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773837",
      "fetched_at": "2026-01-09T13:52:30.747630",
      "classified_at": "2026-01-09T13:56:48.640451",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4407124074",
      "doi": "https://doi.org/10.48550/arxiv.2502.00765"
    },
    "seed_54d774be": {
      "paper_id": "seed_54d774be",
      "title": "Robustifying ML-powered Network Classifiers with PANTS",
      "abstract": "Multiple network management tasks, from resource allocation to intrusion detection, rely on some form of ML-based network traffic classification (MNC). Despite their potential, MNCs are vulnerable to adversarial inputs, which can lead to outages, poor decision-making, and security violations, among other issues. The goal of this paper is to help network operators assess and enhance the robustness of their MNC against adversarial inputs. The most critical step for this is generating inputs that can fool the MNC while being realizable under various threat models. Compared to other ML models, finding adversarial inputs against MNCs is more challenging due to the existence of non-differentiable components e.g., traffic engineering and the need to constrain inputs to preserve semantics and ensure reliability. These factors prevent the direct use of well-established gradient-based methods developed in adversarial ML (AML). To address these challenges, we introduce PANTS, a practical white-box framework that uniquely integrates AML techniques with Satisfiability Modulo Theories (SMT) solvers to generate adversarial inputs for MNCs. We also embed PANTS into an iterative adversarial training process that enhances the robustness of MNCs against adversarial inputs. PANTS is 70% and 2x more likely in median to find adversarial inputs against target MNCs compared to state-of-the-art baselines, namely Amoeba and BAP. PANTS improves the robustness of the target MNCs by 52.7% (even against attackers outside of what is considered during robustification) without sacrificing their accuracy.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Minhao Jin",
        "Maria Apostolaki"
      ],
      "url": "https://openalex.org/W4403594037",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773841",
      "fetched_at": "2026-01-09T13:52:31.268382",
      "classified_at": "2026-01-09T13:56:51.227046",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4403594037",
      "doi": "https://doi.org/10.48550/arxiv.2409.04691"
    },
    "seed_3ec0a9e7": {
      "paper_id": "seed_3ec0a9e7",
      "title": "CertTA: Certified Robustness Made Practical for Learning-Based Traffic Analysis",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-yan-jinzhu.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773843",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_96a4775d": {
      "paper_id": "seed_96a4775d",
      "title": "Sylva: Tailoring Personalized Adversarial Defense in Pre-trained Models via Collaborative Fine-tuning",
      "abstract": null,
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [],
      "url": "https://arxiv.org/html/2506.05402v1",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773846",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_371d7f7f": {
      "paper_id": "seed_371d7f7f",
      "title": "Defeating DNN-Based Traffic Analysis Systems in Real-Time With Blind Adversarial Perturbations",
      "abstract": null,
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec21fall-nasr.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773849",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_2a3c0aa2": {
      "paper_id": "seed_2a3c0aa2",
      "title": "Pryde: A Modular Generalizable Workflow for Uncovering Evasion Attacks Against Stateful Firewall Deployments",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a144/1Ub242nYFoY",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773851",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_bba06dbc": {
      "paper_id": "seed_bba06dbc",
      "title": "Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a006/1RjE9LaYR0c",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773854",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_2500586a": {
      "paper_id": "seed_2500586a",
      "title": "Swallow: A Transfer-Robust Website Fingerprinting Attack via Consistent Feature Learning",
      "abstract": null,
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [],
      "url": "https://ccs25files.zoolab.org/main/ccsfa/TaZ6VzOa/3719027.3744795.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773856",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2102.00918": {
      "paper_id": "2102.00918",
      "title": "Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems",
      "abstract": "Deep Neural Networks (DNNs) have become prevalent in wireless communication systems due to their promising performance. However, similar to other DNN-based applications, they are vulnerable to adversarial examples. In this work, we propose an input-agnostic, undetectable, and robust adversarial attack against DNN-based wireless communication systems in both white-box and black-box scenarios. We design tailored Universal Adversarial Perturbations (UAPs) to perform the attack. We also use a Generative Adversarial Network (GAN) to enforce an undetectability constraint for our attack. Furthermore, we investigate the robustness of our attack against countermeasures. We show that in the presence of defense mechanisms deployed by the communicating parties, our attack performs significantly better compared to existing attacks against DNN-based wireless systems. In particular, the results demonstrate that even when employing well-considered defenses, DNN-based wireless communications are vulnerable to adversarial attacks.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Alireza Bahramali",
        "Milad Nasr",
        "Amir Houmansadr",
        "Dennis Goeckel",
        "Don Towsley"
      ],
      "url": "https://arxiv.org/abs/2102.00918",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773859",
      "fetched_at": "2026-01-09T12:14:35.773860",
      "classified_at": "2026-01-09T13:23:07.335621",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_df32b5c4": {
      "paper_id": "seed_df32b5c4",
      "title": "Adversarial Robustness for Tabular Data through Cost and Utility Awareness",
      "abstract": "value of a person's salary, another to their age, and another to a categorical value representing their marital status.The properties of the image domain have shaped the way adversarial examples and adversarial robustness are approached in the literature [11] and have greatly influenced adversarial robustness research in the text domain.In this paper, we argue that adversarial examples in tabular domains are of a different nature, and adversarial robustness has a different meaning.Thus, the definitions and techniques used to study these phenomena need to be revisited to reflect the tabular context.We argue that two high-level differences need to be addressed.First, imperceptibility, which is the main requirement considered for image and text adversarial examples, is ill-defined and can be irrelevant for tabular data.Second, existing methods assume that all adversarial inputs have the same value for the adversary, whereas in tabular domains different adversarial examples can bring drastically different gains.Imperceptibility and semantic similarity are not necessarily the primary constraints in tabular domains.The existing literature commonly formalizes the concept of \"an example deliberately crafted to cause a misclassification\" as a natural example, i.e., an example coming from the data distribution, that is imperceptibly modified by an adversary in a way that the classifier's decision changes.Typically, imperceptibility is formalized as closeness according to a mathematical distance such as L p [21,22].In tabular data, however, imperceptibility is not necessarily relevant.Let us consider the following toy example of financialfraud detection: Assume a fraud detector takes as input two features: (1) transaction amount, and (2) device from which the transaction was sent.The adversary aims to create a fraudulent financial transaction.The adversary starts with a natural example (amount=$200, device=Android phone) and changes the feature values until the detector no longer classifies the example as fraud.In this example, imperceptibility is not well-defined.Is a modification to the amount feature from $200 to $201 imperceptible?What increase or decrease would we consider perceptible?The issue is even more apparent with categorical data, for which standard distances such as L 2 , L \u221e cannot even capture imperceptibility: Is a change of the device feature from Android to an iPhone imperceptible?Even if imperceptibility was well-defined, imperceptibility might not be relevant.Should we only be concerned about adversaries making \"imperceptible\" changes, e.g., modifying amount from $200 to $201?What about attack vectors in Abstract-Many safety-critical applications of machine learning, such as fraud or abuse detection, use data in tabular domains.Adversarial examples can be particularly damaging for these applications.Yet, existing works on adversarial robustness primarily focus on machine-learning models in image and text domains.We argue that, due to the differences between tabular data and images or text, existing threat models are not suitable for tabular domains.These models do not capture that the costs of an attack could be more significant than imperceptibility, or that the adversary could assign different values to the utility obtained from deploying different adversarial examples.We demonstrate that, due to these differences, the attack and defense methods used for images and text cannot be directly applied to tabular settings.We address these issues by proposing new cost and utility-aware threat models that are tailored to the adversarial capabilities and constraints of attackers targeting tabular domains.We introduce a framework that enables us to design attack and defense mechanisms that result in models protected against cost or utility-aware adversaries, for example, adversaries constrained by a certain financial budget.We show that our approach is effective on three datasets corresponding to applications for which adversarial examples can have economic and social implications.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Klim Kireev",
        "Bogdan Kulynych",
        "Carmela Troncoso"
      ],
      "url": "https://openalex.org/W4324007133",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773864",
      "fetched_at": "2026-01-09T13:52:35.443166",
      "classified_at": "2026-01-09T13:56:53.047454",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4324007133",
      "doi": "https://doi.org/10.14722/ndss.2023.24924"
    },
    "seed_9a529de7": {
      "paper_id": "seed_9a529de7",
      "title": "Local Model Poisoning Attacks to Byzantine-Robust Federated Learning",
      "abstract": "In federated learning, multiple client devices jointly learn a machine learning model: each client device maintains a local model for its local training dataset, while a master device maintains a global model via aggregating the local models from the client devices. The machine learning community recently proposed several federated learning methods that were claimed to be robust against Byzantine failures (e.g., system failures, adversarial manipulations) of certain client devices. In this work, we perform the first systematic study on local model poisoning attacks to federated learning. We assume an attacker has compromised some client devices, and the attacker manipulates the local model parameters on the compromised client devices during the learning process such that the global model has a large testing error rate. We formulate our attacks as optimization problems and apply our attacks to four recent Byzantine-robust federated learning methods. Our empirical results on four real-world datasets show that our attacks can substantially increase the error rates of the models learnt by the federated learning methods that were claimed to be robust against Byzantine failures of some client devices. We generalize two defenses for data poisoning attacks to defend against our local model poisoning attacks. Our evaluation results show that one defense can effectively defend against our attacks in some cases, but the defenses are not effective enough in other cases, highlighting the need for new defenses against our local model poisoning attacks to federated learning.",
      "year": 2019,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Minghong Fang",
        "Xiaoyu Cao",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://openalex.org/W2990614164",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773866",
      "fetched_at": "2026-01-09T13:52:35.975862",
      "classified_at": "2026-01-09T13:56:54.949256",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2990614164",
      "doi": "https://doi.org/10.48550/arxiv.1911.11815"
    },
    "seed_7922130e": {
      "paper_id": "seed_7922130e",
      "title": "Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning",
      "abstract": "Federated learning (FL) enables many data owners (e.g., mobile devices) to train a joint ML model (e.g., a nextword prediction classifier) without the need of sharing their private training data.However, FL is known to be susceptible to poisoning attacks by malicious participants (e.g., adversaryowned mobile devices) who aim at hampering the accuracy of the jointly trained model through sending malicious inputs during the federated training process.In this paper, we present a generic framework for model poisoning attacks on FL.We show that our framework leads to poisoning attacks that substantially outperform state-of-the-art model poisoning attacks by large margins.For instance, our attacks result in 1.5\u00d7 to 60\u00d7 higher reductions in the accuracy of FL models compared to previously discovered poisoning attacks.Our work demonstrates that existing Byzantine-robust FL algorithms are significantly more susceptible to model poisoning than previously thought.Motivated by this, we design a defense against FL poisoning, called divide-and-conquer (DnC).We demonstrate that DnC outperforms all existing Byzantine-robust FL algorithms in defeating model poisoning attacks, specifically, it is 2.5\u00d7 to 12\u00d7 more resilient in our experiments with different datasets and models.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Virat Shejwalkar",
        "Amir Houmansadr"
      ],
      "url": "https://openalex.org/W3138153888",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773870",
      "fetched_at": "2026-01-09T13:52:36.493535",
      "classified_at": "2026-01-09T13:56:56.754131",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3138153888",
      "doi": "https://doi.org/10.14722/ndss.2021.24498"
    },
    "2201.00763": {
      "paper_id": "2201.00763",
      "title": "DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection",
      "abstract": "Federated Learning (FL) allows multiple clients to collaboratively train a Neural Network (NN) model on their private data without revealing the data. Recently, several targeted poisoning attacks against FL have been introduced. These attacks inject a backdoor into the resulting model that allows adversary-controlled inputs to be misclassified. Existing countermeasures against backdoor attacks are inefficient and often merely aim to exclude deviating models from the aggregation. However, this approach also removes benign models of clients with deviating data distributions, causing the aggregated model to perform poorly for such clients.   To address this problem, we propose DeepSight, a novel model filtering approach for mitigating backdoor attacks. It is based on three novel techniques that allow to characterize the distribution of data used to train model updates and seek to measure fine-grained differences in the internal structure and outputs of NNs. Using these techniques, DeepSight can identify suspicious model updates. We also develop a scheme that can accurately cluster model updates. Combining the results of both components, DeepSight is able to identify and eliminate model clusters containing poisoned models with high attack impact. We also show that the backdoor contributions of possibly undetected poisoned models can be effectively mitigated with existing weight clipping-based defenses. We evaluate the performance and effectiveness of DeepSight and show that it can mitigate state-of-the-art backdoor attacks with a negligible impact on the model's performance on benign data.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Phillip Rieger",
        "Thien Duc Nguyen",
        "Markus Miettinen",
        "Ahmad-Reza Sadeghi"
      ],
      "url": "https://arxiv.org/abs/2201.00763",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773873",
      "fetched_at": "2026-01-09T12:14:35.773874",
      "classified_at": "2026-01-09T13:23:08.096422",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_0f448dd2": {
      "paper_id": "seed_0f448dd2",
      "title": "FLAME: Taming Backdoors in Federated Learning",
      "abstract": "Federated Learning (FL) is a collaborative machine learning approach allowing participants to jointly train a model without having to share their private, potentially sensitive local datasets with others. Despite its benefits, FL is vulnerable to backdoor attacks, in which an adversary injects manipulated model updates into the model aggregation process so that the resulting model will provide targeted false predictions for specific adversary-chosen inputs. Proposed defenses against backdoor attacks based on detecting and filtering out malicious model updates consider only very specific and limited attacker models, whereas defenses based on differential privacy-inspired noise injection significantly deteriorate the benign performance of the aggregated model. To address these deficiencies, we introduce FLAME, a defense framework that estimates the sufficient amount of noise to be injected to ensure the elimination of backdoors while maintaining the model performance. To minimize the required amount of noise, FLAME uses a model clustering and weight clipping approach. Our evaluation of FLAME on several datasets stemming from application areas including image classification, word prediction, and IoT intrusion detection demonstrates that FLAME removes backdoors effectively with a negligible impact on the benign performance of the models. Furthermore, following the considerable attention that our research has received after its presentation at USENIX SEC 2022, FLAME has become the subject of numerous investigations proposing diverse attack methodologies in an attempt to circumvent it. As a response to these endeavors, we provide a comprehensive analysis of these attempts. Our findings show that these papers (e.g., 3DFed [36]) have not fully comprehended nor correctly employed the fundamental principles underlying FLAME, i.e., our defense mechanism effectively repels these attempted attacks.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Thien Duc Nguyen",
        "Phillip Rieger",
        "Huili Chen",
        "Hossein Yalame",
        "Helen M\u00f6llering",
        "Hossein Fereidooni",
        "Samuel Marchal",
        "Markus Miettinen",
        "Azalia Mirhoseini",
        "Shaza Zeitouni",
        "Farinaz Koushanfar",
        "Ahmad\u2010Reza Sadeghi",
        "Thomas Schneider"
      ],
      "url": "https://openalex.org/W4287393324",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773877",
      "fetched_at": "2026-01-09T13:52:37.246608",
      "classified_at": "2026-01-09T13:56:58.639885",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4287393324",
      "doi": "https://doi.org/10.48550/arxiv.2101.02281"
    },
    "2112.12727": {
      "paper_id": "2112.12727",
      "title": "EIFFeL: Ensuring Integrity for Federated Learning",
      "abstract": "Federated learning (FL) enables clients to collaborate with a server to train a machine learning model. To ensure privacy, the server performs secure aggregation of updates from the clients. Unfortunately, this prevents verification of the well-formedness (integrity) of the updates as the updates are masked. Consequently, malformed updates designed to poison the model can be injected without detection. In this paper, we formalize the problem of ensuring \\textit{both} update privacy and integrity in FL and present a new system, \\textsf{EIFFeL}, that enables secure aggregation of \\textit{verified} updates. \\textsf{EIFFeL} is a general framework that can enforce \\textit{arbitrary} integrity checks and remove malformed updates from the aggregate, without violating privacy. Our empirical evaluation demonstrates the practicality of \\textsf{EIFFeL}. For instance, with $100$ clients and $10\\%$ poisoning, \\textsf{EIFFeL} can train an MNIST classification model to the same accuracy as that of a non-poisoned federated learner in just $2.4s$ per iteration.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Amrita Roy Chowdhury",
        "Chuan Guo",
        "Somesh Jha",
        "Laurens van der Maaten"
      ],
      "url": "https://arxiv.org/abs/2112.12727",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773879",
      "fetched_at": "2026-01-09T12:14:35.773880",
      "classified_at": "2026-01-09T13:23:08.739750",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2111.07380": {
      "paper_id": "2111.07380",
      "title": "Eluding Secure Aggregation in Federated Learning via Model Inconsistency",
      "abstract": "Secure aggregation is a cryptographic protocol that securely computes the aggregation of its inputs. It is pivotal in keeping model updates private in federated learning. Indeed, the use of secure aggregation prevents the server from learning the value and the source of the individual model updates provided by the users, hampering inference and data attribution attacks. In this work, we show that a malicious server can easily elude secure aggregation as if the latter were not in place. We devise two different attacks capable of inferring information on individual private training datasets, independently of the number of users participating in the secure aggregation. This makes them concrete threats in large-scale, real-world federated learning applications. The attacks are generic and equally effective regardless of the secure aggregation protocol used. They exploit a vulnerability of the federated learning protocol caused by incorrect usage of secure aggregation and lack of parameter validation. Our work demonstrates that current implementations of federated learning with secure aggregation offer only a \"false sense of security\".",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Dario Pasquini",
        "Danilo Francati",
        "Giuseppe Ateniese"
      ],
      "url": "https://arxiv.org/abs/2111.07380",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773883",
      "fetched_at": "2026-01-09T12:14:35.773884",
      "classified_at": "2026-01-09T13:23:09.340906",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2210.10936": {
      "paper_id": "2210.10936",
      "title": "FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information",
      "abstract": "Federated learning is vulnerable to poisoning attacks in which malicious clients poison the global model via sending malicious model updates to the server. Existing defenses focus on preventing a small number of malicious clients from poisoning the global model via robust federated learning methods and detecting malicious clients when there are a large number of them. However, it is still an open challenge how to recover the global model from poisoning attacks after the malicious clients are detected. A naive solution is to remove the detected malicious clients and train a new global model from scratch, which incurs large cost that may be intolerable for resource-constrained clients such as smartphones and IoT devices.   In this work, we propose FedRecover, which can recover an accurate global model from poisoning attacks with small cost for the clients. Our key idea is that the server estimates the clients' model updates instead of asking the clients to compute and communicate them during the recovery process. In particular, the server stores the global models and clients' model updates in each round, when training the poisoned global model. During the recovery process, the server estimates a client's model update in each round using its stored historical information. Moreover, we further optimize FedRecover to recover a more accurate global model using warm-up, periodic correction, abnormality fixing, and final tuning strategies, in which the server asks the clients to compute and communicate their exact model updates. Theoretically, we show that the global model recovered by FedRecover is close to or the same as that recovered by train-from-scratch under some assumptions. Empirically, our evaluation on four datasets, three federated learning methods, as well as untargeted and targeted poisoning attacks (e.g., backdoor attacks) shows that FedRecover is both accurate and efficient.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Xiaoyu Cao",
        "Jinyuan Jia",
        "Zaixi Zhang",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://arxiv.org/abs/2210.10936",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773887",
      "fetched_at": "2026-01-09T12:14:35.773888",
      "classified_at": "2026-01-09T13:23:10.129858",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_ad933014": {
      "paper_id": "seed_ad933014",
      "title": "Every Vote Counts: Ranking-Based Training of Federated Learning to Resist Poisoning Attacks",
      "abstract": "Federated learning (FL) allows mutually untrusted clients to collaboratively train a common machine learning model without sharing their private/proprietary training data among each other. FL is unfortunately susceptible to poisoning by malicious clients who aim to hamper the accuracy of the commonly trained model through sending malicious model updates during FL's training process.   We argue that the key factor to the success of poisoning attacks against existing FL systems is the large space of model updates available to the clients, allowing malicious clients to search for the most poisonous model updates, e.g., by solving an optimization problem. To address this, we propose Federated Rank Learning (FRL). FRL reduces the space of client updates from model parameter updates (a continuous space of float numbers) in standard FL to the space of parameter rankings (a discrete space of integer values). To be able to train the global model using parameter ranks (instead of parameter weights), FRL leverage ideas from recent supermasks training mechanisms. Specifically, FRL clients rank the parameters of a randomly initialized neural network (provided by the server) based on their local training data. The FRL server uses a voting mechanism to aggregate the parameter rankings submitted by clients in each training epoch to generate the global ranking of the next training epoch.   Intuitively, our voting-based aggregation mechanism prevents poisoning clients from making significant adversarial modifications to the global model, as each client will have a single vote! We demonstrate the robustness of FRL to poisoning through analytical proofs and experimentation. We also show FRL's high communication efficiency. Our experiments demonstrate the superiority of FRL in real-world FL settings.",
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [
        "Hamid Mozaffari",
        "Virat Shejwalkar",
        "Amir Houmansadr"
      ],
      "url": "https://arxiv.org/abs/2110.04350",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773890",
      "fetched_at": "2026-01-09T13:52:37.770280",
      "classified_at": "2026-01-09T13:57:00.447288",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "2110.04350"
    },
    "seed_fdf2c3b3": {
      "paper_id": "seed_fdf2c3b3",
      "title": "Securing Federated Sensitive Topic Classification against Poisoning Attacks",
      "abstract": "We present a Federated Learning (FL) based solution for building a distributed classifier capable of detecting URLs containing sensitive content, i.e., content related to categories such as health, political beliefs, sexual orientation, etc. Although such a classifier addresses the limitations of previous offline/centralised classifiers, it is still vulnerable to poisoning attacks from malicious users that may attempt to reduce the accuracy for benign users by disseminating faulty model updates. To guard against this, we develop a robust aggregation scheme based on subjective logic and residual-based attack detection. Employing a combination of theoretical analysis, trace-driven simulation, as well as experimental validation with a prototype and real users, we show that our classifier can detect sensitive content with high accuracy, learn new labels fast, and remain robust in view of poisoning attacks from malicious users, as well as imperfect input from non-malicious ones.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Tianyue Chu",
        "\u00c1lvaro Garc\u00eda-Recuero",
        "Costas Iordanou",
        "Georgios Smaragdakis",
        "Nikolaos Laoutaris"
      ],
      "url": "https://openalex.org/W4307300892",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773893",
      "fetched_at": "2026-01-09T13:52:38.540311",
      "classified_at": "2026-01-09T13:57:02.248569",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4307300892",
      "doi": "https://doi.org/10.14722/ndss.2023.23112"
    },
    "2301.09508": {
      "paper_id": "2301.09508",
      "title": "BayBFed: Bayesian Backdoor Defense for Federated Learning",
      "abstract": "Federated learning (FL) allows participants to jointly train a machine learning model without sharing their private data with others. However, FL is vulnerable to poisoning attacks such as backdoor attacks. Consequently, a variety of defenses have recently been proposed, which have primarily utilized intermediary states of the global model (i.e., logits) or distance of the local models (i.e., L2-norm) from the global model to detect malicious backdoors. However, as these approaches directly operate on client updates, their effectiveness depends on factors such as clients' data distribution or the adversary's attack strategies. In this paper, we introduce a novel and more generic backdoor defense framework, called BayBFed, which proposes to utilize probability distributions over client updates to detect malicious updates in FL: it computes a probabilistic measure over the clients' updates to keep track of any adjustments made in the updates, and uses a novel detection algorithm that can leverage this probabilistic measure to efficiently detect and filter out malicious updates. Thus, it overcomes the shortcomings of previous approaches that arise due to the direct usage of client updates; as our probabilistic measure will include all aspects of the local client training strategies. BayBFed utilizes two Bayesian Non-Parametric extensions: (i) a Hierarchical Beta-Bernoulli process to draw a probabilistic measure given the clients' updates, and (ii) an adaptation of the Chinese Restaurant Process (CRP), referred by us as CRP-Jensen, which leverages this probabilistic measure to detect and filter out malicious updates. We extensively evaluate our defense approach on five benchmark datasets: CIFAR10, Reddit, IoT intrusion detection, MNIST, and FMNIST, and show that it can effectively detect and eliminate malicious updates in FL without deteriorating the benign performance of the global model.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Kavita Kumari",
        "Phillip Rieger",
        "Hossein Fereidooni",
        "Murtuza Jadliwala",
        "Ahmad-Reza Sadeghi"
      ],
      "url": "https://arxiv.org/abs/2301.09508",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773896",
      "fetched_at": "2026-01-09T12:14:35.773897",
      "classified_at": "2026-01-09T13:23:11.181886",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2201.02775": {
      "paper_id": "2201.02775",
      "title": "ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems",
      "abstract": "Vertical federated learning (VFL) system has recently become prominent as a concept to process data distributed across many individual sources without the need to centralize it. Multiple participants collaboratively train models based on their local data in a privacy-aware manner. To date, VFL has become a de facto solution to securely learn a model among organizations, allowing knowledge to be shared without compromising privacy of any individuals. Despite the prosperous development of VFL systems, we find that certain inputs of a participant, named adversarial dominating inputs (ADIs), can dominate the joint inference towards the direction of the adversary's will and force other (victim) participants to make negligible contributions, losing rewards that are usually offered regarding the importance of their contributions in federated learning scenarios. We conduct a systematic study on ADIs by first proving their existence in typical VFL systems. We then propose gradient-based methods to synthesize ADIs of various formats and exploit common VFL systems. We further launch greybox fuzz testing, guided by the saliency score of ``victim'' participants, to perturb adversary-controlled inputs and systematically explore the VFL attack surface in a privacy-preserving manner. We conduct an in-depth study on the influence of critical parameters and settings in synthesizing ADIs. Our study reveals new VFL attack opportunities, promoting the identification of unknown threats before breaches and building more secure VFL systems.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Qi Pang",
        "Yuanyuan Yuan",
        "Shuai Wang",
        "Wenting Zheng"
      ],
      "url": "https://arxiv.org/abs/2201.02775",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773900",
      "fetched_at": "2026-01-09T12:14:35.773901",
      "classified_at": "2026-01-09T13:23:11.789086",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_19e67c1a": {
      "paper_id": "seed_19e67c1a",
      "title": "3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning",
      "abstract": "Federated Learning (FL), the de-facto distributed machine learning paradigm that locally trains datasets at individual devices, is vulnerable to backdoor model poisoning attacks. By compromising or impersonating those devices, an attacker can upload crafted malicious model updates to manipulate the global model with backdoor behavior upon attacker-specified triggers. However, existing backdoor attacks require more information on the victim FL system beyond a practical black-box setting. Furthermore, they are often specialized to optimize for a single objective, which becomes ineffective as modern FL systems tend to adopt in-depth defense that detects backdoor models from different perspectives. Motivated by these concerns, in this paper, we propose 3DFed, an adaptive, extensible, and multi-layered framework to launch covert FL backdoor attacks in a black-box setting. 3DFed sports three evasion modules that camouflage backdoor models: backdoor training with constrained loss, noise mask, and decoy model. By implanting indicators into a backdoor model, 3DFed can obtain the attack feedback in the previous epoch from the global model and dynamically adjust the hyper-parameters of these backdoor evasion modules. Through extensive experimental results, we show that when all its components work together, 3DFed can evade the detection of all state-of-the-art FL backdoor defenses, including Deepsight, Foolsgold, FLAME, FL-Detector, and RFLBAT. New evasion modules can also be incorporated in 3DFed in the future as it is an extensible framework.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Haoyang Li",
        "Qingqing Ye",
        "Haibo Hu",
        "Jin Li",
        "Leixia Wang",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "url": "https://openalex.org/W4385187226",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773904",
      "fetched_at": "2026-01-09T13:52:39.308103",
      "classified_at": "2026-01-09T13:57:04.385898",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385187226",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179401"
    },
    "2308.05832": {
      "paper_id": "2308.05832",
      "title": "FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks",
      "abstract": "Federated learning (FL) is revolutionizing how we learn from data. With its growing popularity, it is now being used in many safety-critical domains such as autonomous vehicles and healthcare. Since thousands of participants can contribute in this collaborative setting, it is, however, challenging to ensure security and reliability of such systems. This highlights the need to design FL systems that are secure and robust against malicious participants' actions while also ensuring high utility, privacy of local data, and efficiency. In this paper, we propose a novel FL framework dubbed as FLShield that utilizes benign data from FL participants to validate the local models before taking them into account for generating the global model. This is in stark contrast with existing defenses relying on server's access to clean datasets -- an assumption often impractical in real-life scenarios and conflicting with the fundamentals of FL. We conduct extensive experiments to evaluate our FLShield framework in different settings and demonstrate its effectiveness in thwarting various types of poisoning and backdoor attacks including a defense-aware one. FLShield also preserves privacy of local data against gradient inversion attacks.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Ehsanul Kabir",
        "Zeyu Song",
        "Md Rafi Ur Rashid",
        "Shagufta Mehnaz"
      ],
      "url": "https://arxiv.org/abs/2308.05832",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773907",
      "fetched_at": "2026-01-09T12:14:35.773908",
      "classified_at": "2026-01-09T13:23:12.586285",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2304.08847": {
      "paper_id": "2304.08847",
      "title": "BadVFL: Backdoor Attacks in Vertical Federated Learning",
      "abstract": "Federated learning (FL) enables multiple parties to collaboratively train a machine learning model without sharing their data; rather, they train their own model locally and send updates to a central server for aggregation. Depending on how the data is distributed among the participants, FL can be classified into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the same set of training instances but only host a different and non-overlapping subset of the whole feature space. Whereas in HFL, each participant shares the same set of features while the training set is split into locally owned training data subsets.   VFL is increasingly used in applications like financial fraud detection; nonetheless, very little work has analyzed its security. In this paper, we focus on robustness in VFL, in particular, on backdoor attacks, whereby an adversary attempts to manipulate the aggregate model during the training process to trigger misclassifications. Performing backdoor attacks in VFL is more challenging than in HFL, as the adversary i) does not have access to the labels during training and ii) cannot change the labels as she only has access to the feature embeddings. We present a first-of-its-kind clean-label backdoor attack in VFL, which consists of two phases: a label inference and a backdoor phase. We demonstrate the effectiveness of the attack on three different datasets, investigate the factors involved in its success, and discuss countermeasures to mitigate its impact.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Mohammad Naseri",
        "Yufei Han",
        "Emiliano De Cristofaro"
      ],
      "url": "https://arxiv.org/abs/2304.08847",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773911",
      "fetched_at": "2026-01-09T12:14:35.773912",
      "classified_at": "2026-01-09T13:23:13.243981",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2210.07714": {
      "paper_id": "2210.07714",
      "title": "CrowdGuard: Federated Backdoor Detection in Federated Learning",
      "abstract": "Federated Learning (FL) is a promising approach enabling multiple clients to train Deep Neural Networks (DNNs) collaboratively without sharing their local training data. However, FL is susceptible to backdoor (or targeted poisoning) attacks. These attacks are initiated by malicious clients who seek to compromise the learning process by introducing specific behaviors into the learned model that can be triggered by carefully crafted inputs. Existing FL safeguards have various limitations: They are restricted to specific data distributions or reduce the global model accuracy due to excluding benign models or adding noise, are vulnerable to adaptive defense-aware adversaries, or require the server to access local models, allowing data inference attacks.   This paper presents a novel defense mechanism, CrowdGuard, that effectively mitigates backdoor attacks in FL and overcomes the deficiencies of existing techniques. It leverages clients' feedback on individual models, analyzes the behavior of neurons in hidden layers, and eliminates poisoned models through an iterative pruning scheme. CrowdGuard employs a server-located stacked clustering scheme to enhance its resilience to rogue client feedback. The evaluation results demonstrate that CrowdGuard achieves a 100% True-Positive-Rate and True-Negative-Rate across various scenarios, including IID and non-IID data distributions. Additionally, CrowdGuard withstands adaptive adversaries while preserving the original performance of protected models. To ensure confidentiality, CrowdGuard uses a secure and privacy-preserving architecture leveraging Trusted Execution Environments (TEEs) on both client and server sides.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Phillip Rieger",
        "Torsten Krau\u00df",
        "Markus Miettinen",
        "Alexandra Dmitrienko",
        "Ahmad-Reza Sadeghi"
      ],
      "url": "https://arxiv.org/abs/2210.07714",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773914",
      "fetched_at": "2026-01-09T12:14:35.773915",
      "classified_at": "2026-01-09T13:23:13.973863",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_c61a2c5c": {
      "paper_id": "seed_c61a2c5c",
      "title": "Automatic Adversarial Adaption for Stealthy Poisoning Attacks in Federated Learning",
      "abstract": "Federated Learning (FL) enables the training of machine learning models using distributed data.This approach offers benefits such as improved data privacy, reduced communication costs, and enhanced model performance through increased data diversity.However, FL systems are vulnerable to poisoning attacks, where adversaries introduce malicious updates to compromise the integrity of the aggregated model.Existing defense strategies against such attacks include filtering, influence reduction, and robust aggregation techniques.Filtering approaches have the advantage of not reducing classification accuracy, but face the challenge of adversaries adapting to the defense mechanisms.The lack of a universally accepted definition of \"adaptive adversaries\" in the literature complicates the assessment of detection capabilities and meaningful comparisons of FL defenses.In this paper, we address the limitations of the commonly used definition of \"adaptive attackers\" proposed by Bagdasaryan et al.We propose AutoAdapt, a novel adaptation method that leverages an Augmented Lagrangian optimization technique.AutoAdapt eliminates the manual search for optimal hyper-parameters by providing a more rational alternative.It generates more effective solutions by accommodating multiple inequality constraints, allowing adaptation to valid value ranges within the defensive metrics.Our proposed method significantly enhances adversaries' capabilities and accelerates research in developing attacks and defenses.By accommodating multiple valid range constraints and adapting to diverse defense metrics, AutoAdapt challenges defenses relying on multiple metrics and expands the range of potential adversarial behaviors.Through comprehensive studies, we demonstrate the effectiveness of AutoAdapt in simultaneously adapting to multiple constraints and showcasing its power by accelerating the performance of tests by a factor of 15.Furthermore, we establish the versatility of AutoAdapt across various application scenarios, encompassing datasets, model architectures, and hyper-parameters, emphasizing its practical utility in real-world contexts.Overall, our contributions advance the evaluation of FL defenses and drive progress in this field.Defenses against poisoning attacks employ three strategies: Influence Reduction (IR), Robust Aggregation (RA), and Detection and Filtering (DF).IR approaches [6], [8], [49], [71] perturb model parameters to cancel malicious behavior, RAbased defenses [81], [46] secure aggregation algorithms even in the presence of poisoned models, and DF-based solutions [13], [48], [67], [53], [31], [60], [84], [16] detect and filter out poisoned models before aggregation.Among the three categories, DF approaches appear to be more prominent solutions, as IR and RA-based systems unavoidably affect benign functionality.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Torsten Krau\u00df",
        "Jan K\u00f6nig",
        "Alexandra Dmitrienko",
        "Christian Kanzow"
      ],
      "url": "https://openalex.org/W4391725340",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773918",
      "fetched_at": "2026-01-09T13:52:40.105990",
      "classified_at": "2026-01-09T13:57:06.288176",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391725340",
      "doi": "https://doi.org/10.14722/ndss.2024.241366"
    },
    "2312.04432": {
      "paper_id": "2312.04432",
      "title": "FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning",
      "abstract": "Federated learning (FL) is a collaborative learning paradigm allowing multiple clients to jointly train a model without sharing their training data. However, FL is susceptible to poisoning attacks, in which the adversary injects manipulated model updates into the federated model aggregation process to corrupt or destroy predictions (untargeted poisoning) or implant hidden functionalities (targeted poisoning or backdoors). Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maintaining the utility of the aggregated model. To address the deficiencies of existing defenses, we take a generic and completely different approach to detect poisoning (targeted and untargeted) attacks. We present FreqFed, a novel aggregation mechanism that transforms the model updates (i.e., weights) into the frequency domain, where we can identify the core frequency components that inherit sufficient information about weights. This allows us to effectively filter out malicious updates during local training on the clients, regardless of attack types, strategies, and clients' data distributions. We extensively evaluate the efficiency and effectiveness of FreqFed in different application domains, including image classification, word prediction, IoT intrusion detection, and speech recognition. We demonstrate that FreqFed can mitigate poisoning attacks effectively with a negligible impact on the utility of the aggregated model.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Hossein Fereidooni",
        "Alessandro Pegoraro",
        "Phillip Rieger",
        "Alexandra Dmitrienko",
        "Ahmad-Reza Sadeghi"
      ],
      "url": "https://arxiv.org/abs/2312.04432",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773922",
      "fetched_at": "2026-01-09T12:14:35.773923",
      "classified_at": "2026-01-09T13:23:14.649713",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_f53dc38c": {
      "paper_id": "seed_f53dc38c",
      "title": "Dealing Doubt: Unveiling Threat Models in Gradient Inversion Attacks under Federated Learning \u2013 A Survey and Taxonomy",
      "abstract": "Federated learning (FL) is a collaborative learning paradigm allowing multiple clients to jointly train a model without sharing their training data. However, FL is susceptible to poisoning attacks, in which the adversary injects manipulated model updates into the federated model aggregation process to corrupt or destroy predictions (untargeted poisoning) or implant hidden functionalities (targeted poisoning or backdoors). Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maintaining the utility of the aggregated model. To address the deficiencies of existing defenses, we take a generic and completely different approach to detect poisoning (targeted and untargeted) attacks. We present FreqFed, a novel aggregation mechanism that transforms the model updates (i.e., weights) into the frequency domain, where we can identify the core frequency components that inherit sufficient information about weights. This allows us to effectively filter out malicious updates during local training on the clients, regardless of attack types, strategies, and clients' data distributions. We extensively evaluate the efficiency and effectiveness of FreqFed in different application domains, including image classification, word prediction, IoT intrusion detection, and speech recognition. We demonstrate that FreqFed can mitigate poisoning attacks effectively with a negligible impact on the utility of the aggregated model.",
      "year": 2023,
      "venue": "CCS",
      "authors": [
        "Hossein Fereidooni",
        "Alessandro Pegoraro",
        "Phillip Rieger",
        "Alexandra Dmitrienko",
        "Ahmad-Reza Sadeghi"
      ],
      "url": "https://arxiv.org/abs/2312.04432",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773927",
      "fetched_at": "2026-01-09T13:52:41.120087",
      "classified_at": "2026-01-09T13:57:08.218140",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "2312.04432"
    },
    "2406.10416": {
      "paper_id": "2406.10416",
      "title": "Byzantine-Robust Decentralized Federated Learning",
      "abstract": "Federated learning (FL) enables multiple clients to collaboratively train machine learning models without revealing their private training data. In conventional FL, the system follows the server-assisted architecture (server-assisted FL), where the training process is coordinated by a central server. However, the server-assisted FL framework suffers from poor scalability due to a communication bottleneck at the server, and trust dependency issues. To address challenges, decentralized federated learning (DFL) architecture has been proposed to allow clients to train models collaboratively in a serverless and peer-to-peer manner. However, due to its fully decentralized nature, DFL is highly vulnerable to poisoning attacks, where malicious clients could manipulate the system by sending carefully-crafted local models to their neighboring clients. To date, only a limited number of Byzantine-robust DFL methods have been proposed, most of which are either communication-inefficient or remain vulnerable to advanced poisoning attacks. In this paper, we propose a new algorithm called BALANCE (Byzantine-robust averaging through local similarity in decentralization) to defend against poisoning attacks in DFL. In BALANCE, each client leverages its own local model as a similarity reference to determine if the received model is malicious or benign. We establish the theoretical convergence guarantee for BALANCE under poisoning attacks in both strongly convex and non-convex settings. Furthermore, the convergence rate of BALANCE under poisoning attacks matches those of the state-of-the-art counterparts in Byzantine-free settings. Extensive experiments also demonstrate that BALANCE outperforms existing DFL methods and effectively defends against poisoning attacks.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Minghong Fang",
        "Zifan Zhang",
        " Hairi",
        "Prashant Khanduri",
        "Jia Liu",
        "Songtao Lu",
        "Yuchen Liu",
        "Neil Gong"
      ],
      "url": "https://arxiv.org/abs/2406.10416",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773930",
      "fetched_at": "2026-01-09T12:14:35.773931",
      "classified_at": "2026-01-09T13:23:15.259949",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_cd8b172b": {
      "paper_id": "seed_cd8b172b",
      "title": "PoiSAFL: Scalable Poisoning Attack Framework to Byzantine-resilient Semi-asynchronous Federated Learning",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-pang-xiaoyi.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773934",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_abf32fe4": {
      "paper_id": "seed_abf32fe4",
      "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
      "abstract": "Backdoor attacks are among the most effective, practical, and stealthy attacks in deep learning. In this paper, we consider a practical scenario where a developer obtains a deep model from a third party and uses it as part of a safety-critical system. The developer wants to inspect the model for potential backdoors prior to system deployment. We find that most existing detection techniques make assumptions that are not applicable to this scenario. In this paper, we present a novel framework for detecting backdoors under realistic restrictions. We generate candidate triggers by deductively searching over the space of possible triggers. We construct and optimize a smoothed version of Attack Success Rate as our search objective. Starting from a broad class of template attacks and just using the forward pass of a deep model, we reverse engineer the backdoor attack. We conduct extensive evaluation on a wide range of attacks, models, and datasets, with our technique performing almost perfectly across these settings.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "D Popovi\u0107",
        "Amin Sadeghi",
        "Ting Yu",
        "Sanjay Chawla",
        "Issa Khalil"
      ],
      "url": "https://openalex.org/W4415062401",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773936",
      "fetched_at": "2026-01-09T13:52:42.140531",
      "classified_at": "2026-01-09T13:57:10.102026",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4415062401",
      "doi": "https://doi.org/10.48550/arxiv.2503.21305"
    },
    "seed_82a4c513": {
      "paper_id": "seed_82a4c513",
      "title": "Justinian's GAAvernor: Robust Distributed Learning with Gradient Aggregation Agent",
      "abstract": null,
      "year": 2020,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec20-pan.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773940",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_a8f190bf": {
      "paper_id": "seed_a8f190bf",
      "title": "Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning",
      "abstract": "Word embeddings, i.e., low-dimensional vector representations such as GloVe and SGNS, encode word \"meaning\" in the sense that distances between words' vectors correspond to their semantic proximity. This enables transfer learning of semantics for a variety of natural language processing tasks. Word embeddings are typically trained on large public corpora such as Wikipedia or Twitter. We demonstrate that an attacker who can modify the corpus on which the embedding is trained can control the \"meaning\" of new and existing words by changing their locations in the embedding space. We develop an explicit expression over corpus features that serves as a proxy for distance between words and establish a causative relationship between its values and embedding distances. We then show how to use this relationship for two adversarial objectives: (1) make a word a top-ranked neighbor of another word, and (2) move a word from one semantic cluster to another. An attack on the embedding can affect diverse downstream tasks, demonstrating for the first time the power of data poisoning in transfer learning scenarios. We use this attack to manipulate query expansion in information retrieval systems such as resume search, make certain names more or less visible to named entity recognition models, and cause new words to be translated to a particular target word regardless of the language. Finally, we show how the attacker can generate linguistically likely corpus modifications, thus fooling defenses that attempt to filter implausible sentences from the corpus using a language model.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Roei Schuster",
        "Tal Schuster",
        "Yoav Meri",
        "Vitaly Shmatikov"
      ],
      "url": "https://openalex.org/W2999480335",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773942",
      "fetched_at": "2026-01-09T13:52:43.555221",
      "classified_at": "2026-01-09T13:57:11.954929",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2999480335",
      "doi": "https://doi.org/10.1109/sp40000.2020.00115"
    },
    "seed_7f4a9058": {
      "paper_id": "seed_7f4a9058",
      "title": "You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion",
      "abstract": "Code autocompletion is an integral feature of modern code editors and IDEs. The latest generation of autocompleters uses neural language models, trained on public open-source code repositories, to suggest likely (not just statically feasible) completions given the current context. \r\nWe demonstrate that neural code autocompleters are vulnerable to poisoning attacks. By adding a few specially-crafted files to the autocompleter's training corpus (data poisoning), or else by directly fine-tuning the autocompleter on these files (model poisoning), the attacker can influence its suggestions for attacker-chosen contexts. For example, the attacker can teach the autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3 for the SSL/TLS protocol version, or a low iteration count for password-based encryption. Moreover, we show that these attacks can be targeted: an autocompleter poisoned by a targeted attack is much more likely to suggest the insecure completion for files from a specific repo or specific developer. \r\nWe quantify the efficacy of targeted and untargeted data- and model-poisoning attacks against state-of-the-art autocompleters based on Pythia and GPT-2. We then evaluate existing defenses against poisoning attacks and show that they are largely ineffective.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Roei Schuster",
        "Congzheng Song",
        "Eran Tromer",
        "Vitaly Shmatikov"
      ],
      "url": "https://openalex.org/W3155981360",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773945",
      "fetched_at": "2026-01-09T13:52:44.315684",
      "classified_at": "2026-01-09T13:57:13.758128",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3155981360"
    },
    "2301.02344": {
      "paper_id": "2301.02344",
      "title": "TROJANPUZZLE: Covertly Poisoning Code-Suggestion Models",
      "abstract": "With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training by injecting malicious data. Poisoning attacks could be designed to influence the model's suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, COVERT and TROJANPUZZLE, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE, goes one step further in generating less suspicious poison data by never explicitly including certain (suspicious) parts of the payload in the poison data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TROJANPUZZLE robust against signature-based dataset-cleansing methods that can filter out suspicious sequences from the training data. Our evaluation against models of two sizes demonstrates that both COVERT and TROJANPUZZLE have significant implications for practitioners when selecting code used to train or tune code-suggestion models.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Hojjat Aghakhani",
        "Wei Dai",
        "Andre Manoel",
        "Xavier Fernandes",
        "Anant Kharkar",
        "Christopher Kruegel",
        "Giovanni Vigna",
        "David Evans",
        "Ben Zorn",
        "Robert Sim"
      ],
      "url": "https://arxiv.org/abs/2301.02344",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773948",
      "fetched_at": "2026-01-09T12:14:35.773949",
      "classified_at": "2026-01-09T13:23:16.019567",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_94d1790d": {
      "paper_id": "seed_94d1790d",
      "title": "Poisoning the Unlabeled Dataset of Semi-Supervised Learning",
      "abstract": "Semi-supervised machine learning models learn from a (small) set of labeled training examples, and a (large) set of unlabeled training examples. State-of-the-art models can reach within a few percentage points of fully-supervised training, while requiring 100x less labeled data. We study a new class of vulnerabilities: poisoning attacks that modify the unlabeled dataset. In order to be useful, unlabeled datasets are given strictly less review than labeled datasets, and adversaries can therefore poison them easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1% of the dataset size, we can manipulate a model trained on this poisoned dataset to misclassify arbitrary examples at test time (as any desired label). Our attacks are highly effective across datasets and semi-supervised learning methods. We find that more accurate methods (thus more likely to be used) are significantly more vulnerable to poisoning attacks, and as such better training methods are unlikely to prevent this attack. To counter this we explore the space of defenses, and propose two methods that mitigate our attack.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Nicholas Carlini"
      ],
      "url": "https://openalex.org/W3157597523",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773951",
      "fetched_at": "2026-01-09T13:52:45.077396",
      "classified_at": "2026-01-09T13:57:15.609772",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3157597523",
      "doi": "https://doi.org/10.48550/arxiv.2105.01622"
    },
    "2101.02644": {
      "paper_id": "2101.02644",
      "title": "Data Poisoning Attacks to Deep Learning Based Recommender Systems",
      "abstract": "Recommender systems play a crucial role in helping users to find their interested information in various web services such as Amazon, YouTube, and Google News. Various recommender systems, ranging from neighborhood-based, association-rule-based, matrix-factorization-based, to deep learning based, have been developed and deployed in industry. Among them, deep learning based recommender systems become increasingly popular due to their superior performance.   In this work, we conduct the first systematic study on data poisoning attacks to deep learning based recommender systems. An attacker's goal is to manipulate a recommender system such that the attacker-chosen target items are recommended to many users. To achieve this goal, our attack injects fake users with carefully crafted ratings to a recommender system. Specifically, we formulate our attack as an optimization problem, such that the injected ratings would maximize the number of normal users to whom the target items are recommended. However, it is challenging to solve the optimization problem because it is a non-convex integer programming problem. To address the challenge, we develop multiple techniques to approximately solve the optimization problem. Our experimental results on three real-world datasets, including small and large datasets, show that our attack is effective and outperforms existing attacks. Moreover, we attempt to detect fake users via statistical analysis of the rating patterns of normal and fake users. Our results show that our attack is still effective and outperforms existing attacks even if such a detector is deployed.",
      "year": 2021,
      "venue": "NDSS",
      "authors": [
        "Hai Huang",
        "Jiaming Mu",
        "Neil Zhenqiang Gong",
        "Qi Li",
        "Bin Liu",
        "Mingwei Xu"
      ],
      "url": "https://arxiv.org/abs/2101.02644",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773954",
      "fetched_at": "2026-01-09T12:14:35.773955",
      "classified_at": "2026-01-09T13:23:16.750526",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_2e985c83": {
      "paper_id": "seed_2e985c83",
      "title": "Reverse Attack: Black-box Attacks on Collaborative Recommendation",
      "abstract": "Collaborative filtering (CF) recommender systems have been extensively developed and widely deployed in various social websites, promoting products or services to the users of interest. Meanwhile, work has been attempted at poisoning attacks to CF recommender systems for distorting the recommend results to reap commercial or personal gains stealthily. While existing poisoning attacks have demonstrated their effectiveness with the offline social datasets, they are impractical when applied to the real setting on online social websites. This paper develops a novel and practical poisoning attack solution toward the CF recommender systems without knowing involved specific algorithms nor historical social data information a priori. Instead of directly attacking the unknown recommender systems, our solution performs certain operations on the social websites to collect a set of sampling data for use in constructing a surrogate model for deeply learning the inherent recommendation patterns. This surrogate model can estimate the item proximities, learned by the recommender systems. By attacking the surrogate model, the corresponding solutions (for availability and target attacks) can be directly migrated to attack the original recommender systems. Extensive experiments validate the generated surrogate model's reproductive capability and demonstrate the effectiveness of our attack upon various CF recommender algorithms.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Yihe Zhang",
        "Xu Yuan",
        "Jin Li",
        "Jiadong Lou",
        "Li Chen",
        "Nian-Feng Tzeng"
      ],
      "url": "https://openalex.org/W3214009246",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773958",
      "fetched_at": "2026-01-09T13:52:46.706531",
      "classified_at": "2026-01-09T13:57:18.211656",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3214009246",
      "doi": "https://doi.org/10.1145/3460120.3484805"
    },
    "2006.14026": {
      "paper_id": "2006.14026",
      "title": "Subpopulation Data Poisoning Attacks",
      "abstract": "Machine learning systems are deployed in critical settings, but they might fail in unexpected ways, impacting the accuracy of their predictions. Poisoning attacks against machine learning induce adversarial modification of data used by a machine learning algorithm to selectively change its output when it is deployed. In this work, we introduce a novel data poisoning attack called a \\emph{subpopulation attack}, which is particularly relevant when datasets are large and diverse. We design a modular framework for subpopulation attacks, instantiate it with different building blocks, and show that the attacks are effective for a variety of datasets and machine learning models. We further optimize the attacks in continuous domains using influence functions and gradient optimization methods. Compared to existing backdoor poisoning attacks, subpopulation attacks have the advantage of inducing misclassification in naturally distributed data points at inference time, making the attacks extremely stealthy. We also show that our attack strategy can be used to improve upon existing targeted attacks. We prove that, under some assumptions, subpopulation attacks are impossible to defend against, and empirically demonstrate the limitations of existing defenses against our attacks, highlighting the difficulty of protecting machine learning against this threat.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Matthew Jagielski",
        "Giorgio Severi",
        "Niklas Pousette Harger",
        "Alina Oprea"
      ],
      "url": "https://arxiv.org/abs/2006.14026",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773960",
      "fetched_at": "2026-01-09T12:14:35.773961",
      "classified_at": "2026-01-09T13:23:17.449731",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2111.04394": {
      "paper_id": "2111.04394",
      "title": "Get a Model! Model Hijacking Attack Against Machine Learning Models",
      "abstract": "Machine learning (ML) has established itself as a cornerstone for various critical applications ranging from autonomous driving to authentication systems. However, with this increasing adoption rate of machine learning models, multiple attacks have emerged. One class of such attacks is training time attack, whereby an adversary executes their attack before or during the machine learning model training. In this work, we propose a new training time attack against computer vision based machine learning models, namely model hijacking attack. The adversary aims to hijack a target model to execute a different task than its original one without the model owner noticing. Model hijacking can cause accountability and security risks since a hijacked model owner can be framed for having their model offering illegal or unethical services. Model hijacking attacks are launched in the same way as existing data poisoning attacks. However, one requirement of the model hijacking attack is to be stealthy, i.e., the data samples used to hijack the target model should look similar to the model's original training dataset. To this end, we propose two different model hijacking attacks, namely Chameleon and Adverse Chameleon, based on a novel encoder-decoder style ML model, namely the Camouflager. Our evaluation shows that both of our model hijacking attacks achieve a high attack success rate, with a negligible drop in model utility.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Ahmed Salem",
        "Michael Backes",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2111.04394",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773965",
      "fetched_at": "2026-01-09T12:14:35.773966",
      "classified_at": "2026-01-09T13:23:18.049786",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_0c08f38b": {
      "paper_id": "seed_0c08f38b",
      "title": "PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning",
      "abstract": "Contrastive learning pre-trains an image encoder using a large amount of unlabeled data such that the image encoder can be used as a general-purpose feature extractor for various downstream tasks. In this work, we propose PoisonedEncoder, a data poisoning attack to contrastive learning. In particular, an attacker injects carefully crafted poisoning inputs into the unlabeled pre-training data, such that the downstream classifiers built based on the poisoned encoder for multiple target downstream tasks simultaneously classify attacker-chosen, arbitrary clean inputs as attacker-chosen, arbitrary classes. We formulate our data poisoning attack as a bilevel optimization problem, whose solution is the set of poisoning inputs; and we propose a contrastive-learning-tailored method to approximately solve it. Our evaluation on multiple datasets shows that PoisonedEncoder achieves high attack success rates while maintaining the testing accuracy of the downstream classifiers built upon the poisoned encoder for non-attacker-chosen inputs. We also evaluate five defenses against PoisonedEncoder, including one pre-processing, three in-processing, and one post-processing defenses. Our results show that these defenses can decrease the attack success rate of PoisonedEncoder, but they also sacrifice the utility of the encoder or require a large clean pre-training dataset.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Hongbin Liu",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://openalex.org/W4280514286",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773969",
      "fetched_at": "2026-01-09T13:52:47.441382",
      "classified_at": "2026-01-09T13:57:20.177876",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4280514286",
      "doi": "https://doi.org/10.48550/arxiv.2205.06401"
    },
    "2402.01920": {
      "paper_id": "2402.01920",
      "title": "Preference Poisoning Attacks on Reward Model Learning",
      "abstract": "Learning reward models from pairwise comparisons is a fundamental component in a number of domains, including autonomous control, conversational agents, and recommendation systems, as part of a broad goal of aligning automated decisions with user preferences. These approaches entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability by considering an attacker who can flip a small subset of preference comparisons to either promote or demote a target outcome. We propose two classes of algorithmic approaches for these attacks: a gradient-based framework, and several variants of rank-by-distance methods. Next, we evaluate the efficacy of best attacks in both these classes in successfully achieving malicious goals on datasets from three domains: autonomous control, recommendation system, and textual prompt-response preference learning. We find that the best attacks are often highly successful, achieving in the most extreme case 100\\% success rate with only 0.3\\% of the data poisoned. However, \\emph{which} attack is best can vary significantly across domains. In addition, we observe that the simpler and more scalable rank-by-distance approaches are often competitive with, and on occasion significantly outperform, gradient-based methods. Finally, we show that state-of-the-art defenses against other classes of poisoning attacks exhibit limited efficacy in our setting.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Junlin Wu",
        "Jiongxiao Wang",
        "Chaowei Xiao",
        "Chenguang Wang",
        "Ning Zhang",
        "Yevgeniy Vorobeychik"
      ],
      "url": "https://arxiv.org/abs/2402.01920",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773973",
      "fetched_at": "2026-01-09T12:14:35.773974",
      "classified_at": "2026-01-09T13:23:18.689045",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2204.00032": {
      "paper_id": "2204.00032",
      "title": "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets",
      "abstract": "We introduce a new class of attacks on machine learning models. We show that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties. Our active inference attacks connect two independent lines of work targeting the integrity and privacy of machine learning training data.   Our attacks are effective across membership inference, attribute inference, and data extraction. For example, our targeted attacks can poison <0.1% of the training dataset to boost the performance of inference attacks by 1 to 2 orders of magnitude. Further, an adversary who controls a significant fraction of the training data (e.g., 50%) can launch untargeted attacks that enable 8x more precise inference on all other users' otherwise-private data points.   Our results cast doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Florian Tram\u00e8r",
        "Reza Shokri",
        "Ayrton San Joaquin",
        "Hoang Le",
        "Matthew Jagielski",
        "Sanghyun Hong",
        "Nicholas Carlini"
      ],
      "url": "https://arxiv.org/abs/2204.00032",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773976",
      "fetched_at": "2026-01-09T12:14:35.773977",
      "classified_at": "2026-01-09T13:23:19.490204",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2308.08505": {
      "paper_id": "2308.08505",
      "title": "Test-Time Poisoning Attacks Against Test-Time Adaptation Models",
      "abstract": "Deploying machine learning (ML) models in the wild is challenging as it suffers from distribution shifts, where the model trained on an original domain cannot generalize well to unforeseen diverse transfer domains. To address this challenge, several test-time adaptation (TTA) methods have been proposed to improve the generalization ability of the target pre-trained models under test data to cope with the shifted distribution. The success of TTA can be credited to the continuous fine-tuning of the target model according to the distributional hint from the test samples during test time. Despite being powerful, it also opens a new attack surface, i.e., test-time poisoning attacks, which are substantially different from previous poisoning attacks that occur during the training time of ML models (i.e., adversaries cannot intervene in the training process). In this paper, we perform the first test-time poisoning attack against four mainstream TTA methods, including TTT, DUA, TENT, and RPL. Concretely, we generate poisoned samples based on the surrogate models and feed them to the target TTA models. Experimental results show that the TTA methods are generally vulnerable to test-time poisoning attacks. For instance, the adversary can feed as few as 10 poisoned samples to degrade the performance of the target model from 76.20% to 41.83%. Our results demonstrate that TTA algorithms lacking a rigorous security assessment are unsuitable for deployment in real-life scenarios. As such, we advocate for the integration of defenses against test-time poisoning attacks into the design of TTA methods.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Tianshuo Cong",
        "Xinlei He",
        "Yun Shen",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2308.08505",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773980",
      "fetched_at": "2026-01-09T12:14:35.773981",
      "classified_at": "2026-01-09T13:57:22.049019",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_d2bc1e0d": {
      "paper_id": "seed_d2bc1e0d",
      "title": "Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks",
      "abstract": "In adversarial machine learning, new defenses against attacks on deep learning systems are routinely broken soon after their release by more powerful attacks. In this context, forensic tools can offer a valuable complement to existing defenses, by tracing back a successful attack to its root cause, and offering a path forward for mitigation to prevent similar attacks in the future. In this paper, we describe our efforts in developing a forensic traceback tool for poison attacks on deep neural networks. We propose a novel iterative clustering and pruning solution that trims \"innocent\" training samples, until all that remains is the set of poisoned data responsible for the attack. Our method clusters training samples based on their impact on model parameters, then uses an efficient data unlearning method to prune innocent clusters. We empirically demonstrate the efficacy of our system on three types of dirty-label (backdoor) poison attacks and three types of clean-label poison attacks, across domains of computer vision and malware classification. Our system achieves over 98.4% precision and 96.8% recall across all attacks. We also show that our system is robust against four anti-forensics measures specifically designed to attack it.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Shawn Shan",
        "Arjun Nitin Bhagoji",
        "Hai-Tao Zheng",
        "Ben Y. Zhao"
      ],
      "url": "https://openalex.org/W4286904975",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773984",
      "fetched_at": "2026-01-09T13:52:47.985140",
      "classified_at": "2026-01-09T13:57:23.993640",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4286904975",
      "doi": "https://doi.org/10.48550/arxiv.2110.06904"
    },
    "2409.12314": {
      "paper_id": "2409.12314",
      "title": "Understanding Implosion in Text-to-Image Generative Models",
      "abstract": "Recent works show that text-to-image generative models are surprisingly vulnerable to a variety of poisoning attacks. Empirical results find that these models can be corrupted by altering associations between individual text prompts and associated visual features. Furthermore, a number of concurrent poisoning attacks can induce \"model implosion,\" where the model becomes unable to produce meaningful images for unpoisoned prompts. These intriguing findings highlight the absence of an intuitive framework to understand poisoning attacks on these models. In this work, we establish the first analytical framework on robustness of image generative models to poisoning attacks, by modeling and analyzing the behavior of the cross-attention mechanism in latent diffusion models. We model cross-attention training as an abstract problem of \"supervised graph alignment\" and formally quantify the impact of training data by the hardness of alignment, measured by an Alignment Difficulty (AD) metric. The higher the AD, the harder the alignment. We prove that AD increases with the number of individual prompts (or concepts) poisoned. As AD grows, the alignment task becomes increasingly difficult, yielding highly distorted outcomes that frequently map meaningful text prompts to undefined or meaningless visual representations. As a result, the generative model implodes and outputs random, incoherent images at large. We validate our analytical framework through extensive experiments, and we confirm and explain the unexpected (and unexplained) effect of model implosion while producing new, unforeseen insights. Our work provides a useful tool for studying poisoning attacks against diffusion models and their defenses.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Wenxin Ding",
        "Cathy Y. Li",
        "Shawn Shan",
        "Ben Y. Zhao",
        "Haitao Zheng"
      ],
      "url": "https://arxiv.org/abs/2409.12314",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773987",
      "fetched_at": "2026-01-09T12:14:35.773988",
      "classified_at": "2026-01-09T13:24:20.459266",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_2b8ed5cf": {
      "paper_id": "seed_2b8ed5cf",
      "title": "Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection",
      "abstract": "A security threat to deep neural networks (DNN) is backdoor contamination, in which an adversary poisons the training data of a target model to inject a Trojan so that images carrying a specific trigger will always be classified into a specific label. Prior research on this problem assumes the dominance of the trigger in an image's representation, which causes any image with the trigger to be recognized as a member in the target class. Such a trigger also exhibits unique features in the representation space and can therefore be easily separated from legitimate images. Our research, however, shows that simple target contamination can cause the representation of an attack image to be less distinguishable from that of legitimate ones, thereby evading existing defenses against the backdoor infection. In our research, we show that such a contamination attack actually subtly changes the representation distribution for the target class, which can be captured by a statistic analysis. More specifically, we leverage an EM algorithm to decompose an image into its identity part (e.g., person, traffic sign) and variation part within a class (e.g., lighting, poses). Then we analyze the distribution in each class, identifying those more likely to be characterized by a mixture model resulted from adding attack samples to the legitimate image pool. Our research shows that this new technique effectively detects data contamination attacks, including the new one we propose, and is also robust against the evasion attempts made by a knowledgeable adversary.",
      "year": 2019,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Di Tang",
        "Xiaofeng Wang",
        "Haixu Tang",
        "Kehuan Zhang"
      ],
      "url": "https://openalex.org/W2965527544",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773990",
      "fetched_at": "2026-01-09T13:52:48.747128",
      "classified_at": "2026-01-09T13:57:25.841125",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2965527544",
      "doi": "https://doi.org/10.48550/arxiv.1908.00686"
    },
    "seed_1d4975b3": {
      "paper_id": "seed_1d4975b3",
      "title": "Double-Cross Attacks: Subverting Active Learning Systems",
      "abstract": null,
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec21-vicarte.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.773993",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "1910.03137": {
      "paper_id": "1910.03137",
      "title": "Detecting AI Trojans Using Meta Neural Analysis",
      "abstract": "In machine learning Trojan attacks, an adversary trains a corrupted model that obtains good performance on normal data but behaves maliciously on data samples with certain trigger patterns. Several approaches have been proposed to detect such attacks, but they make undesirable assumptions about the attack strategies or require direct access to the trained models, which restricts their utility in practice.   This paper addresses these challenges by introducing a Meta Neural Trojan Detection (MNTD) pipeline that does not make assumptions on the attack strategies and only needs black-box access to models. The strategy is to train a meta-classifier that predicts whether a given target model is Trojaned. To train the meta-model without knowledge of the attack strategy, we introduce a technique called jumbo learning that samples a set of Trojaned models following a general distribution. We then dynamically optimize a query set together with the meta-classifier to distinguish between Trojaned and benign models.   We evaluate MNTD with experiments on vision, speech, tabular data and natural language text datasets, and against different Trojan attacks such as data poisoning attack, model manipulation attack, and latent attack. We show that MNTD achieves 97% detection AUC score and significantly outperforms existing detection approaches. In addition, MNTD generalizes well and achieves high detection performance against unforeseen attacks. We also propose a robust MNTD pipeline which achieves 90% detection AUC even when the attacker aims to evade the detection with full knowledge of the system.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Xiaojun Xu",
        "Qi Wang",
        "Huichen Li",
        "Nikita Borisov",
        "Carl A. Gunter",
        "Bo Li"
      ],
      "url": "https://arxiv.org/abs/1910.03137",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773996",
      "fetched_at": "2026-01-09T12:14:35.773997",
      "classified_at": "2026-01-09T13:24:21.153692",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2108.00352": {
      "paper_id": "2108.00352",
      "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning",
      "abstract": "Self-supervised learning in computer vision aims to pre-train an image encoder using a large amount of unlabeled images or (image, text) pairs. The pre-trained image encoder can then be used as a feature extractor to build downstream classifiers for many downstream tasks with a small amount of or no labeled training data. In this work, we propose BadEncoder, the first backdoor attack to self-supervised learning. In particular, our BadEncoder injects backdoors into a pre-trained image encoder such that the downstream classifiers built based on the backdoored image encoder for different downstream tasks simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an optimization problem and we propose a gradient descent based method to solve it, which produces a backdoored image encoder from a clean one. Our extensive empirical evaluation results on multiple datasets show that our BadEncoder achieves high attack success rates while preserving the accuracy of the downstream classifiers. We also show the effectiveness of BadEncoder using two publicly available, real-world image encoders, i.e., Google's image encoder pre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training (CLIP) image encoder pre-trained on 400 million (image, text) pairs collected from the Internet. Moreover, we consider defenses including Neural Cleanse and MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our results show that these defenses are insufficient to defend against BadEncoder, highlighting the needs for new defenses against our BadEncoder. Our code is publicly available at: https://github.com/jjy1994/BadEncoder.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Jinyuan Jia",
        "Yupei Liu",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://arxiv.org/abs/2108.00352",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.773999",
      "fetched_at": "2026-01-09T12:14:35.774000",
      "classified_at": "2026-01-09T13:24:21.938385",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_bbc1dc00": {
      "paper_id": "seed_bbc1dc00",
      "title": "Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features",
      "abstract": "With the prevalent use of Deep Neural Networks (DNNs) in many applications, security of these networks is of importance. Pre-trained DNNs may contain backdoors that are injected through poisoned training. These trojaned models perform well when regular inputs are provided, but misclassify to a target output label when the input is stamped with a unique pattern called trojan trigger. Recently various backdoor detection and mitigation systems for DNN based AI applications have been proposed. However, many of them are limited to trojan attacks that require a specific patch trigger. In this paper, we introduce composite attack, a more flexible and stealthy trojan attack that eludes backdoor scanners using trojan triggers composed from existing benign features of multiple labels. We show that a neural network with a composed backdoor can achieve accuracy comparable to its original version on benign data and misclassifies when the composite trigger is present in the input. Our experiments on 7 different tasks show that this attack poses a severe threat. We evaluate our attack with two state-of-the-art backdoor scanners. The results show none of the injected backdoors can be detected by either scanner. We also study in details why the scanners are not effective. In the end, we discuss the essence of our attack and propose possible defense.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Junyu Lin",
        "Lei Xu",
        "Yingqi Liu",
        "Xiangyu Zhang"
      ],
      "url": "https://openalex.org/W3106646114",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774003",
      "fetched_at": "2026-01-09T13:52:49.813828",
      "classified_at": "2026-01-09T13:57:27.674348",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3106646114",
      "doi": "https://doi.org/10.1145/3372297.3423362"
    },
    "seed_fa84bab6": {
      "paper_id": "seed_fa84bab6",
      "title": "AI-Lancet: Locating Error-inducing Neurons to Optimize Neural Networks",
      "abstract": "Deep neural network (DNN) has been widely utilized in many areas due to its increasingly high accuracy. However, DNN models could also produce wrong outputs due to internal errors, which may lead to severe security issues. Unlike fixing bugs in traditional computer software, tracing the errors in DNN models and fixing them are much more difficult due to the uninterpretability of DNN. In this paper, we present a novel and systematic approach to trace and fix the errors in deep learning models. In particular, we locate the error-inducing neurons that play a leading role in the erroneous output. With the knowledge of error-inducing neurons, we propose two methods to fix the errors: the neuron-flip and the neuron-fine-tuning. We evaluate our approach using five different training datasets and seven different model architectures. The experimental results demonstrate its efficacy in different application scenarios, including backdoor removal and general defects fixing.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Yue Zhao",
        "Hong Zhu",
        "Kai Chen",
        "Shengzhi Zhang"
      ],
      "url": "https://openalex.org/W3213849916",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML10",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774007",
      "fetched_at": "2026-01-09T13:52:50.598831",
      "classified_at": "2026-01-09T13:57:29.507087",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3213849916",
      "doi": "https://doi.org/10.1145/3460120.3484818"
    },
    "seed_b13c9bc7": {
      "paper_id": "seed_b13c9bc7",
      "title": "LoneNeuron: a Highly-Effective Feature-Domain Neural Trojan Using Invisible and Polymorphic Watermarks",
      "abstract": "The wide adoption of deep neural networks (DNNs) in real-world applications raises increasing security concerns. Neural Trojans embedded in pre-trained neural networks are a harmful attack against the DNN model supply chain. They generate false outputs when certain stealthy triggers appear in the inputs. While data-poisoning attacks have been well studied in the literature, code-poisoning and model-poisoning backdoors only start to attract attention until recently. We present a novel model-poisoning neural Trojan, namely LoneNeuron, which responds to feature-domain patterns that transform into invisible, sample-specific, and polymorphic pixel-domain watermarks. With high attack specificity, LoneNeuron achieves a 100% attack success rate, while not affecting the main task performance. With LoneNeuron's unique watermark polymorphism property, the same feature-domain trigger is resolved to multiple watermarks in the pixel domain, which further improves watermark randomness, stealthiness, and resistance against Trojan detection. Extensive experiments show that LoneNeuron could escape state-of-the-art Trojan detectors. LoneNeuron~is also the first effective backdoor attack against vision transformers (ViTs).",
      "year": 2022,
      "venue": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
      "authors": [
        "Zeyan Liu",
        "Fengjun Li",
        "Zhu Li",
        "Bo Luo"
      ],
      "url": "https://openalex.org/W4308338624",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774010",
      "fetched_at": "2026-01-09T13:52:51.373584",
      "classified_at": "2026-01-09T13:57:32.000262",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4308338624",
      "doi": "https://doi.org/10.1145/3548606.3560678"
    },
    "seed_e67f1280": {
      "paper_id": "seed_e67f1280",
      "title": "ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks",
      "abstract": "Deep neural networks have achieved remarkable success on a variety of mission-critical tasks.However, recent studies show that deep neural networks are vulnerable to backdoor attacks, where the attacker releases backdoored models that behave normally on benign samples but misclassify any trigger-imposed samples to a target label.Unlike adversarial examples, backdoor attacks manipulate both the inputs and the model, perturbing samples with the trigger and injecting backdoors into the model.In this paper, we propose a novel attention-based evasive backdoor attack, dubbed ATTEQ-NN.Different from existing works that arbitrarily set the trigger mask, we carefully design an attentionbased trigger mask determination framework, which places the trigger at the crucial region with the most significant influence on the prediction results.To make the trigger-imposed samples appear more natural and imperceptible to human inspectors, we introduce a Quality-of-Experience (QoE) term into the loss function of trigger generation and carefully adjust the transparency of the trigger.During the process of iteratively optimizing the trigger generation and the backdoor injection components, we propose an alternating retraining strategy, which is shown to be effective in improving the clean data accuracy and evading some model-based defense approaches.We evaluate ATTEQ-NN with extensive experiments on VGG-Flower, CIFAR-10, GTSRB, CIFAR-100, and ImageNette datasets.The results show that ATTEQ-NN can increase the attack success rate by as much as 82% over baselines when the poison ratio is low while achieving a high QoE of the backdoored samples.We demonstrate that ATTEQ-NN reaches an attack success rate of more than 37.78% in the physical world under different lighting conditions and shooting angles.ATTEQ-NN preserves an attack success rate of more than 92.5% even if the original backdoored model is fine-tuned with clean data.It is shown that ATTEQ-NN is also effective in transfer learning scenarios.Our user studies show that the backdoored samples generated by ATTEQ-NN are indiscernible under visual inspections.ATTEQ-NN is shown to be evasive to state-of-the-art defense methods, including model pruning, NAD, STRIP, NC, and MNTD.We will open-source our codes upon publication.",
      "year": 2022,
      "venue": null,
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Jianshuo Dong",
        "Qian Wang"
      ],
      "url": "https://openalex.org/W4226550712",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774012",
      "fetched_at": "2026-01-09T13:52:52.142213",
      "classified_at": "2026-01-09T13:57:34.618130",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4226550712",
      "doi": "https://doi.org/10.14722/ndss.2022.24012"
    },
    "2003.08904": {
      "paper_id": "2003.08904",
      "title": "RAB: Provable Robustness Against Backdoor Attacks",
      "abstract": "Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible to train the robust smoothed models efficiently for simple models such as K-nearest neighbor classifiers, and we propose an exact smooth-training algorithm that eliminates the need to sample from a noise distribution for such models. Empirically, we conduct comprehensive experiments for different machine learning (ML) models such as DNNs, support vector machines, and K-NN models on MNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark for certified robustness against backdoor attacks. In addition, we evaluate K-NN models on a spambase tabular dataset to demonstrate the advantages of the proposed exact algorithm. Both the theoretic analysis and the comprehensive evaluation on diverse ML models and datasets shed light on further robust learning strategies against general training time attacks.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Maurice Weber",
        "Xiaojun Xu",
        "Bojan Karla\u0161",
        "Ce Zhang",
        "Bo Li"
      ],
      "url": "https://arxiv.org/abs/2003.08904",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774015",
      "fetched_at": "2026-01-09T12:14:35.774016",
      "classified_at": "2026-01-09T13:24:22.607933",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_68cb4028": {
      "paper_id": "seed_68cb4028",
      "title": "A Data-free Backdoor Injection Approach in Neural Networks",
      "abstract": "A convolutional neural network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention from both industry and academia in the past few years. The existing reviews mainly focus on CNN's applications in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide some novel ideas and prospects in this fast-growing field. Besides, not only 2-D convolution but also 1-D and multidimensional ones are involved. First, this review introduces the history of CNN. Second, we provide an overview of various convolutions. Third, some classic and advanced CNN models are introduced; especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for functions and hyperparameter selection. Fifth, the applications of 1-D, 2-D, and multidimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed as guidelines for future work.",
      "year": 2021,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Zewen Li",
        "Fan Liu",
        "Wenjie Yang",
        "Shouheng Peng",
        "Jun Zhou"
      ],
      "url": "https://openalex.org/W3168997536",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774018",
      "fetched_at": "2026-01-09T13:52:52.906272",
      "classified_at": "2026-01-09T13:57:36.698846",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3168997536",
      "doi": "https://doi.org/10.1109/tnnls.2021.3084827"
    },
    "2301.01197": {
      "paper_id": "2301.01197",
      "title": "Backdoor Attacks Against Dataset Distillation",
      "abstract": "Dataset distillation has emerged as a prominent technique to improve data efficiency when training machine learning models. It encapsulates the knowledge from a large dataset into a smaller synthetic dataset. A model trained on this smaller distilled dataset can attain comparable performance to a model trained on the original training dataset. However, the existing dataset distillation techniques mainly aim at achieving the best trade-off between resource usage efficiency and model utility. The security risks stemming from them have not been explored. This study performs the first backdoor attack against the models trained on the data distilled by dataset distillation models in the image domain. Concretely, we inject triggers into the synthetic data during the distillation procedure rather than during the model training stage, where all previous attacks are performed. We propose two types of backdoor attacks, namely NAIVEATTACK and DOORPING. NAIVEATTACK simply adds triggers to the raw data at the initial distillation phase, while DOORPING iteratively updates the triggers during the entire distillation procedure. We conduct extensive evaluations on multiple datasets, architectures, and dataset distillation techniques. Empirical evaluation shows that NAIVEATTACK achieves decent attack success rate (ASR) scores in some cases, while DOORPING reaches higher ASR scores (close to 1.0) in all cases. Furthermore, we conduct a comprehensive ablation study to analyze the factors that may affect the attack performance. Finally, we evaluate multiple defense mechanisms against our backdoor attacks and show that our attacks can practically circumvent these defense mechanisms.",
      "year": 2023,
      "venue": "NDSS",
      "authors": [
        "Yugeng Liu",
        "Zheng Li",
        "Michael Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2301.01197",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774021",
      "fetched_at": "2026-01-09T12:14:35.774022",
      "classified_at": "2026-01-09T13:24:23.314077",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2301.06241": {
      "paper_id": "2301.06241",
      "title": "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense",
      "abstract": "Deep Learning backdoor attacks have a threat model similar to traditional cyber attacks. Attack forensics, a critical counter-measure for traditional cyber attacks, is hence of importance for defending model backdoor attacks. In this paper, we propose a novel model backdoor forensics technique. Given a few attack samples such as inputs with backdoor triggers, which may represent different types of backdoors, our technique automatically decomposes them to clean inputs and the corresponding triggers. It then clusters the triggers based on their properties to allow automatic attack categorization and summarization. Backdoor scanners can then be automatically synthesized to find other instances of the same type of backdoor in other models. Our evaluation on 2,532 pre-trained models, 10 popular attacks, and comparison with 9 baselines show that our technique is highly effective. The decomposed clean inputs and triggers closely resemble the ground truth. The synthesized scanners substantially outperform the vanilla versions of existing scanners that can hardly generalize to different kinds of attacks.",
      "year": 2023,
      "venue": "NDSS",
      "authors": [
        "Siyuan Cheng",
        "Guanhong Tao",
        "Yingqi Liu",
        "Shengwei An",
        "Xiangzhe Xu",
        "Shiwei Feng",
        "Guangyu Shen",
        "Kaiyuan Zhang",
        "Qiuling Xu",
        "Shiqing Ma",
        "Xiangyu Zhang"
      ],
      "url": "https://arxiv.org/abs/2301.06241",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774025",
      "fetched_at": "2026-01-09T12:14:35.774026",
      "classified_at": "2026-01-09T13:24:24.138816",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_2d0aee9f": {
      "paper_id": "seed_2d0aee9f",
      "title": "Disguising Attacks with Explanation-Aware Backdoors",
      "abstract": "Explainable machine learning holds great potential for analyzing and understanding learning-based systems. These methods can, however, be manipulated to present unfaithful explanations, giving rise to powerful and stealthy adversaries. In this paper, we demonstrate how to fully disguise the adversarial operation of a machine learning model. Similar to neural backdoors, we change the model's prediction upon trigger presence but simultaneously fool an explanation method that is applied post-hoc for analysis. This enables an adversary to hide the presence of the trigger or point the explanation to entirely different portions of the input, throwing a red herring. We analyze different manifestations of these explanation-aware backdoors for gradient- and propagation-based explanation methods in the image domain, before we resume to conduct a red-herring attack against malware classification.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Maximilian Noppel",
        "Luk\u00e1\u0161 Peter",
        "Christian Wressnegger"
      ],
      "url": "https://openalex.org/W4385080308",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774028",
      "fetched_at": "2026-01-09T13:52:53.414812",
      "classified_at": "2026-01-09T13:57:38.798353",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385080308",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179308"
    },
    "2212.04687": {
      "paper_id": "2212.04687",
      "title": "Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models",
      "abstract": "In this paper, we present a simple yet surprisingly effective technique to induce \"selective amnesia\" on a backdoored model. Our approach, called SEAM, has been inspired by the problem of catastrophic forgetting (CF), a long standing issue in continual learning. Our idea is to retrain a given DNN model on randomly labeled clean data, to induce a CF on the model, leading to a sudden forget on both primary and backdoor tasks; then we recover the primary task by retraining the randomized model on correctly labeled clean data. We analyzed SEAM by modeling the unlearning process as continual learning and further approximating a DNN using Neural Tangent Kernel for measuring CF. Our analysis shows that our random-labeling approach actually maximizes the CF on an unknown backdoor in the absence of triggered inputs, and also preserves some feature extraction in the network to enable a fast revival of the primary task. We further evaluated SEAM on both image processing and Natural Language Processing tasks, under both data contamination and training manipulation attacks, over thousands of models either trained on popular image datasets or provided by the TrojAI competition. Our experiments show that SEAM vastly outperforms the state-of-the-art unlearning techniques, achieving a high Fidelity (measuring the gap between the accuracy of the primary task and that of the backdoor) within a few minutes (about 30 times faster than training a model from scratch using the MNIST dataset), with only a small amount of clean data (0.1% of training data for TrojAI models).",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Rui Zhu",
        "Di Tang",
        "Siyuan Tang",
        "XiaoFeng Wang",
        "Haixu Tang"
      ],
      "url": "https://arxiv.org/abs/2212.04687",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774031",
      "fetched_at": "2026-01-09T12:14:35.774032",
      "classified_at": "2026-01-09T13:24:24.772282",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_388cc2c1": {
      "paper_id": "seed_388cc2c1",
      "title": "AI-Guardian: Defeating Adversarial Attacks using Backdoors",
      "abstract": "Deep neural networks (DNNs) have been widely used in many fields due to their increasingly high accuracy. However, they are also vulnerable to adversarial attacks, posing a serious threat to security-critical applications such as autonomous driving, remote diagnosis, etc. Existing solutions are limited in detecting/preventing such attacks, and also impacting the performance on the original tasks. In this paper, we present AI-Guardian, a novel approach to defeating adversarial attacks that leverages intentionally embedded backdoors to fail the adversarial perturbations and maintain the performance of the original main task. We extensively evaluate AI-Guardian using five popular adversarial example generation approaches, and experimental results demonstrate its efficacy in defeating adversarial attacks. Specifically, AI-Guardian reduces the attack success rate from 97.3% to 3.2%, which outperforms the state-of-the-art works by 30.9%, with only a 0.9% decline on the clean data accuracy. Furthermore, AI-Guardian introduces only 0.36% overhead to the model prediction time, almost negligible in most cases.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Hong Zhu",
        "Shengzhi Zhang",
        "Kai Chen"
      ],
      "url": "https://openalex.org/W4384948583",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774036",
      "fetched_at": "2026-01-09T13:52:54.187462",
      "classified_at": "2026-01-09T13:57:40.631115",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4384948583",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179473"
    },
    "seed_4e4fdabc": {
      "paper_id": "seed_4e4fdabc",
      "title": "REDEEM MYSELF: Purifying Backdoors in Deep Learning Models using Self Attention Distillation",
      "abstract": "Recent works have revealed the vulnerability of deep neural networks to backdoor attacks, where a backdoored model orchestrates targeted or untargeted misclassification when activated by a trigger. A line of purification methods (e.g., fine-pruning, neural attention transfer, MCR [69]) have been proposed to remove the backdoor in a model. However, they either fail to reduce the attack success rate of more advanced backdoor attacks or largely degrade the prediction capacity of the model for clean samples. In this paper, we put forward a new purification defense framework, dubbed SAGE, which utilizes self-attention distillation to purge models of backdoors. Unlike traditional attention transfer mechanisms that require a teacher model to supervise the distillation process, SAGE can realize self-purification with a small number of clean samples. To enhance the defense performance, we further propose a dynamic learning rate adjustment strategy that carefully tracks the prediction accuracy of clean samples to guide the learning rate adjustment. We compare the defense performance of SAGE with 6 state-of-the-art defense approaches against 8 backdoor attacks on 4 datasets. It is shown that SAGE can reduce the attack success rate by as much as 90% with less than 3% decrease in prediction accuracy for clean samples. We will open-source our codes upon publication.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Yang Wang",
        "Qian Wang",
        "Yuzhe Gu",
        "Huayang Huang",
        "Chao Shen"
      ],
      "url": "https://openalex.org/W4385187298",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774040",
      "fetched_at": "2026-01-09T13:52:54.963372",
      "classified_at": "2026-01-09T13:57:42.723909",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385187298",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179375"
    },
    "2204.05255": {
      "paper_id": "2204.05255",
      "title": "NARCISSUS: A Practical Clean-Label Backdoor Attack with Limited Information",
      "abstract": "Backdoor attacks insert malicious data into a training set so that, during inference time, it misclassifies inputs that have been patched with a backdoor trigger as the malware specified label. For backdoor attacks to bypass human inspection, it is essential that the injected data appear to be correctly labeled. The attacks with such property are often referred to as \"clean-label attacks.\" Existing clean-label backdoor attacks require knowledge of the entire training set to be effective. Obtaining such knowledge is difficult or impossible because training data are often gathered from multiple sources (e.g., face images from different users). It remains a question whether backdoor attacks still present a real threat.   This paper provides an affirmative answer to this question by designing an algorithm to mount clean-label backdoor attacks based only on the knowledge of representative examples from the target class. With poisoning equal to or less than 0.5% of the target-class data and 0.05% of the training set, we can train a model to classify test examples from arbitrary classes into the target class when the examples are patched with a backdoor trigger. Our attack works well across datasets and models, even when the trigger presents in the physical world.   We explore the space of defenses and find that, surprisingly, our attack can evade the latest state-of-the-art defenses in their vanilla form, or after a simple twist, we can adapt to the downstream defenses. We study the cause of the intriguing effectiveness and find that because the trigger synthesized by our attack contains features as persistent as the original semantic features of the target class, any attempt to remove such triggers would inevitably hurt the model accuracy first.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Yi Zeng",
        "Minzhou Pan",
        "Hoang Anh Just",
        "Lingjuan Lyu",
        "Meikang Qiu",
        "Ruoxi Jia"
      ],
      "url": "https://arxiv.org/abs/2204.05255",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774043",
      "fetched_at": "2026-01-09T12:14:35.774044",
      "classified_at": "2026-01-09T13:24:25.445172",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_dddf388a": {
      "paper_id": "seed_dddf388a",
      "title": "ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms",
      "abstract": "Backdoor data detection is traditionally studied in an end-to-end supervised learning (SL) setting. However, recent years have seen the proliferating adoption of self-supervised learning (SSL) and transfer learning (TL), due to their lesser need for labeled data. Successful backdoor attacks have also been demonstrated in these new settings. However, we lack a thorough understanding of the applicability of existing detection methods across a variety of learning settings. By evaluating 56 attack settings, we show that the performance of most existing detection methods varies significantly across different attacks and poison ratios, and all fail on the state-of-the-art clean-label attack. In addition, they either become inapplicable or suffer large performance losses when applied to SSL and TL. We propose a new detection method called Active Separation via Offset (ASSET), which actively induces different model behaviors between the backdoor and clean samples to promote their separation. We also provide procedures to adaptively select the number of suspicious points to remove. In the end-to-end SL setting, ASSET is superior to existing methods in terms of consistency of defensive performance across different attacks and robustness to changes in poison ratios; in particular, it is the only method that can detect the state-of-the-art clean-label attack. Moreover, ASSET's average detection rates are higher than the best existing methods in SSL and TL, respectively, by 69.3% and 33.2%, thus providing the first practical backdoor defense for these new DL settings. We open-source the project to drive further development and encourage engagement: https://github.com/ruoxi-jia-group/ASSET.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Minzhou Pan",
        "Yi Zeng",
        "Lingjuan Lyu",
        "Xue Lin",
        "Ruoxi Jia"
      ],
      "url": "https://openalex.org/W4321649939",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774046",
      "fetched_at": "2026-01-09T13:52:55.473882",
      "classified_at": "2026-01-09T13:57:44.703798",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4321649939",
      "doi": "https://doi.org/10.48550/arxiv.2302.11408"
    },
    "seed_27e87237": {
      "paper_id": "seed_27e87237",
      "title": "ODSCAN: Backdoor Scanning for Object Detection Models",
      "abstract": "A backdoor attack in deep learning inserts a hidden backdoor in the model to trigger malicious behavior upon specific input patterns. Existing detection approaches assume a metric space (for either the original inputs or their latent representations) in which normal samples and malicious samples are separable. We show that this assumption has a severe limitation by introducing a novel SSDT (Source-Specific and Dynamic-Triggers) backdoor, which obscures the difference between normal samples and malicious samples.   To overcome this limitation, we move beyond looking for a perfect metric space that would work for different deep-learning models, and instead resort to more robust topological constructs. We propose TED (Topological Evolution Dynamics) as a model-agnostic basis for robust backdoor detection. The main idea of TED is to view a deep-learning model as a dynamical system that evolves inputs to outputs. In such a dynamical system, a benign input follows a natural evolution trajectory similar to other benign inputs. In contrast, a malicious sample displays a distinct trajectory, since it starts close to benign samples but eventually shifts towards the neighborhood of attacker-specified target samples to activate the backdoor.   Extensive evaluations are conducted on vision and natural language datasets across different network architectures. The results demonstrate that TED not only achieves a high detection rate, but also significantly outperforms existing state-of-the-art detection approaches, particularly in addressing the sophisticated SSDT attack. The code to reproduce the results is made public on GitHub.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Xiaoxing Mo",
        "Yechao Zhang",
        "Leo Yu Zhang",
        "Wei Luo",
        "Nan Sun",
        "Shengshan Hu",
        "Shang Gao",
        "Yang Xiang"
      ],
      "url": "https://arxiv.org/abs/2312.02673",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774049",
      "fetched_at": "2026-01-09T13:52:56.000856",
      "classified_at": "2026-01-09T13:57:46.685876",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "2312.02673"
    },
    "2205.06900": {
      "paper_id": "2205.06900",
      "title": "MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic",
      "abstract": "Backdoor attacks are an important type of adversarial threat against deep neural network classifiers, wherein test samples from one or more source classes will be (mis)classified to the attacker's target class when a backdoor pattern is embedded. In this paper, we focus on the post-training backdoor defense scenario commonly considered in the literature, where the defender aims to detect whether a trained classifier was backdoor-attacked without any access to the training set. Many post-training detectors are designed to detect attacks that use either one or a few specific backdoor embedding functions (e.g., patch-replacement or additive attacks). These detectors may fail when the backdoor embedding function used by the attacker (unknown to the defender) is different from the backdoor embedding function assumed by the defender. In contrast, we propose a post-training defense that detects backdoor attacks with arbitrary types of backdoor embeddings, without making any assumptions about the backdoor embedding type. Our detector leverages the influence of the backdoor attack, independent of the backdoor embedding mechanism, on the landscape of the classifier's outputs prior to the softmax layer. For each class, a maximum margin statistic is estimated. Detection inference is then performed by applying an unsupervised anomaly detector to these statistics. Thus, our detector does not need any legitimate clean samples, and can efficiently detect backdoor attacks with arbitrary numbers of source classes. These advantages over several state-of-the-art methods are demonstrated on four datasets, for three different types of backdoor patterns, and for a variety of attack configurations. Finally, we propose a novel, general approach for backdoor mitigation once a detection is made. The mitigation approach was the runner-up at the first IEEE Trojan Removal Competition. The code is online available.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Hang Wang",
        "Zhen Xiang",
        "David J. Miller",
        "George Kesidis"
      ],
      "url": "https://arxiv.org/abs/2205.06900",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774052",
      "fetched_at": "2026-01-09T12:14:35.774053",
      "classified_at": "2026-01-09T13:24:26.080576",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_580ab67c": {
      "paper_id": "seed_580ab67c",
      "title": "Distribution Preserving Backdoor Attack in Self-supervised Learning",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a029/1RjEa5rjsHK",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774055",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_8a38247d": {
      "paper_id": "seed_8a38247d",
      "title": "Backdooring Bias (B^2) into Stable Diffusion Models",
      "abstract": "Recent advances in large text-conditional diffusion models have revolutionized image generation by enabling users to create realistic, high-quality images from textual prompts, significantly enhancing artistic creation and visual communication. However, these advancements also introduce an underexplored attack opportunity: the possibility of inducing biases by an adversary into the generated images for malicious intentions, e.g., to influence public opinion and spread propaganda. In this paper, we study an attack vector that allows an adversary to inject arbitrary bias into a target model. The attack leverages low-cost backdooring techniques using a targeted set of natural textual triggers embedded within a small number of malicious data samples produced with public generative models. An adversary could pick common sequences of words that can then be inadvertently activated by benign users during inference. We investigate the feasibility and challenges of such attacks, demonstrating how modern generative models have made this adversarial process both easier and more adaptable. On the other hand, we explore various aspects of the detectability of such attacks and demonstrate that the model's utility remains intact in the absence of the triggers. Our extensive experiments using over 200,000 generated images and against hundreds of fine-tuned models demonstrate the feasibility of the presented backdoor attack. We illustrate how these biases maintain strong text-image alignment, highlighting the challenges in detecting biased images without knowing that bias in advance. Our cost analysis confirms the low financial barrier (\\$10-\\$15) to executing such attacks, underscoring the need for robust defensive strategies against such vulnerabilities in diffusion models.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Ali Naseh",
        "Jaechul Roh",
        "Eugene Bagdasaryan",
        "Amir Houmansadr"
      ],
      "url": "https://openalex.org/W4399991166",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774058",
      "fetched_at": "2026-01-09T13:52:57.578095",
      "classified_at": "2026-01-09T13:57:48.545627",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4399991166",
      "doi": "https://doi.org/10.48550/arxiv.2406.15213"
    },
    "seed_4cf4a072": {
      "paper_id": "seed_4cf4a072",
      "title": "Watch the Watchers! On the Security Risks of Robustness-Enhancing Diffusion Models",
      "abstract": "ABSTRACT We estimate private benefits of control in 39 countries using 393 controlling blocks sales. On average the value of control is 14 percent, but in some countries can be as low as \u22124 percent, in others as high a +65 percent. As predicted by theory, higher private benefits of control are associated with less developed capital markets, more concentrated ownership, and more privately negotiated privatizations. We also analyze what institutions are most important in curbing private benefits. We find evidence for both legal and extra\u2010legal mechanisms. In a multivariate analysis, however, media pressure and tax enforcement seem to be the dominating factors.",
      "year": 2004,
      "venue": "The Journal of Finance",
      "authors": [
        "Alexander Dyck",
        "Luigi Zingales"
      ],
      "url": "https://openalex.org/W3123748565",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774060",
      "fetched_at": "2026-01-09T13:52:58.385218",
      "classified_at": "2026-01-09T13:57:50.470069",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3123748565",
      "doi": "https://doi.org/10.1111/j.1540-6261.2004.00642.x"
    },
    "seed_03288c70": {
      "paper_id": "seed_03288c70",
      "title": "Pretender: Universal Active Defense against Diffusion Finetuning Attacks",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-sun-zekun.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774063",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_ae5a938e": {
      "paper_id": "seed_ae5a938e",
      "title": "Rowhammer-Based Trojan Injection: One Bit Flip Is Sufficient for Backdooring DNNs",
      "abstract": "State-of-the-art deep neural networks (DNNs) have been proven to be vulnerable to adversarial manipulation and backdoor attacks. Backdoored models deviate from expected behavior on inputs with predefined triggers while retaining performance on clean data. Recent works focus on software simulation of backdoor injection during the inference phase by modifying network weights, which we find often unrealistic in practice due to restrictions in hardware. In contrast, in this work for the first time, we present an end-to-end backdoor injection attack realized on actual hardware on a classifier model using Rowhammer as the fault injection method. To this end, we first investigate the viability of backdoor injection attacks in real-life deployments of DNNs on hardware and address such practical issues in hardware implementation from a novel optimization perspective. We are motivated by the fact that vulnerable memory locations are very rare, device-specific, and sparsely distributed. Consequently, we propose a novel network training algorithm based on constrained optimization to achieve a realistic backdoor injection attack in hardware. By modifying parameters uniformly across the convolutional and fully-connected layers as well as optimizing the trigger pattern together, we achieve state-of-the-art attack performance with fewer bit flips. For instance, our method on a hardware-deployed ResNet-20 model trained on CIFAR-10 achieves over 89% test accuracy and 92% attack success rate by flipping only 10 out of 2.2 million bits.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "M. Caner Tol",
        "Saad Islam",
        "Andrew Adiletta",
        "Berk Sunar",
        "Ziming Zhang"
      ],
      "url": "https://openalex.org/W4286904258",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774066",
      "fetched_at": "2026-01-09T13:52:59.677284",
      "classified_at": "2026-01-09T13:57:52.374680",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4286904258",
      "doi": "https://doi.org/10.48550/arxiv.2110.07683"
    },
    "seed_49c6c5ce": {
      "paper_id": "seed_49c6c5ce",
      "title": "From Purity to Peril: Backdooring Merged Models From \"Harmless\" Benign Components",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-wang-lijin.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774070",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_c385cdc7": {
      "paper_id": "seed_c385cdc7",
      "title": "Revisiting Training-Inference Trigger Intensity in Backdoor Attacks",
      "abstract": "Contains fulltext : 310103.pdf (Publisher\u2019s version ) (Open Access)",
      "year": 2024,
      "venue": null,
      "authors": [
        "Gorka Abad",
        "O\u011fuzhan Ersoy",
        "Stjepan Picek",
        "Aitor Urbieta"
      ],
      "url": "https://openalex.org/W4391725253",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774076",
      "fetched_at": "2026-01-09T13:53:01.015980",
      "classified_at": "2026-01-09T13:57:54.285911",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391725253",
      "doi": "https://doi.org/10.14722/ndss.2024.24334"
    },
    "seed_48a59125": {
      "paper_id": "seed_48a59125",
      "title": "Persistent Backdoor Attacks in Continual Learning",
      "abstract": "Backdoor attacks pose a significant threat to neural networks, enabling adversaries to manipulate model outputs on specific inputs, often with devastating consequences, especially in critical applications. While backdoor attacks have been studied in various contexts, little attention has been given to their practicality and persistence in continual learning, particularly in understanding how the continual updates to model parameters, as new data distributions are learned and integrated, impact the effectiveness of these attacks over time. To address this gap, we introduce two persistent backdoor attacks-Blind Task Backdoor and Latent Task Backdoor-each leveraging minimal adversarial influence. Our blind task backdoor subtly alters the loss computation without direct control over the training process, while the latent task backdoor influences only a single task's training, with all other tasks trained benignly. We evaluate these attacks under various configurations, demonstrating their efficacy with static, dynamic, physical, and semantic triggers. Our results show that both attacks consistently achieve high success rates across different continual learning algorithms, while effectively evading state-of-the-art defenses, such as SentiNet and I-BAU.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Guo Zhen",
        "Abhinav Kumar",
        "Reza Tourani"
      ],
      "url": "https://openalex.org/W4403752757",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774078",
      "fetched_at": "2026-01-09T13:53:01.528274",
      "classified_at": "2026-01-09T13:57:56.148602",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4403752757",
      "doi": "https://doi.org/10.48550/arxiv.2409.13864"
    },
    "seed_048d0c77": {
      "paper_id": "seed_048d0c77",
      "title": "T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification",
      "abstract": "Deep Neural Network (DNN) classifiers are known to be vulnerable to Trojan or backdoor attacks, where the classifier is manipulated such that it misclassifies any input containing an attacker-determined Trojan trigger. Backdoors compromise a model's integrity, thereby posing a severe threat to the landscape of DNN-based classification. While multiple defenses against such attacks exist for classifiers in the image domain, there have been limited efforts to protect classifiers in the text domain. We present Trojan-Miner (T-Miner) -- a defense framework for Trojan attacks on DNN-based text classifiers. T-Miner employs a sequence-to-sequence (seq-2-seq) generative model that probes the suspicious classifier and learns to produce text sequences that are likely to contain the Trojan trigger. T-Miner then analyzes the text produced by the generative model to determine if they contain trigger phrases, and correspondingly, whether the tested classifier has a backdoor. T-Miner requires no access to the training dataset or clean inputs of the suspicious classifier, and instead uses synthetically crafted \"nonsensical\" text inputs to train the generative model. We extensively evaluate T-Miner on 1100 model instances spanning 3 ubiquitous DNN model architectures, 5 different classification tasks, and a variety of trigger phrases. We show that T-Miner detects Trojan and clean models with a 98.75% overall accuracy, while achieving low false positives on clean models. We also show that T-Miner is robust against a variety of targeted, advanced attacks from an adaptive attacker.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Ahmadreza Azizi",
        "Ibrahim Asadullah Tahmid",
        "Asim Waheed",
        "Neal Mangaokar",
        "Jiameng Pu",
        "Mobin Javed",
        "Chandan K. Reddy",
        "Bimal Viswanath"
      ],
      "url": "https://openalex.org/W3135366566",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774081",
      "fetched_at": "2026-01-09T13:53:02.081314",
      "classified_at": "2026-01-09T13:57:57.973072",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3135366566",
      "doi": "https://doi.org/10.48550/arxiv.2103.04264"
    },
    "2105.00164": {
      "paper_id": "2105.00164",
      "title": "Hidden Backdoors in Human-Centric Language Models",
      "abstract": "Natural language processing (NLP) systems have been proven to be vulnerable to backdoor attacks, whereby hidden features (backdoors) are trained into a language model and may only be activated by specific inputs (called triggers), to trick the model into producing unexpected behaviors. In this paper, we create covert and natural triggers for textual backdoor attacks, \\textit{hidden backdoors}, where triggers can fool both modern language models and human inspection. We deploy our hidden backdoors through two state-of-the-art trigger embedding methods. The first approach via homograph replacement, embeds the trigger into deep neural networks through the visual spoofing of lookalike character replacement. The second approach uses subtle differences between text generated by language models and real natural text to produce trigger sentences with correct grammar and high fluency. We demonstrate that the proposed hidden backdoors can be effective across three downstream security-critical NLP tasks, representative of modern human-centric NLP systems, including toxic comment detection, neural machine translation (NMT), and question answering (QA). Our two hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at least $97\\%$ with an injection rate of only $3\\%$ in toxic comment detection, $95.1\\%$ ASR in NMT with less than $0.5\\%$ injected data, and finally $91.12\\%$ ASR against QA updated with only 27 poisoning data samples on a model previously trained with 92,024 samples (0.029\\%). We are able to demonstrate the adversary's high success rate of attacks, while maintaining functionality for regular users, with triggers inconspicuous by the human administrators.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Shaofeng Li",
        "Hui Liu",
        "Tian Dong",
        "Benjamin Zi Hao Zhao",
        "Minhui Xue",
        "Haojin Zhu",
        "Jialiang Lu"
      ],
      "url": "https://arxiv.org/abs/2105.00164",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774084",
      "fetched_at": "2026-01-09T12:14:35.774085",
      "classified_at": "2026-01-09T13:24:26.772405",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2111.00197": {
      "paper_id": "2111.00197",
      "title": "Backdoor Pre-trained Models Can Transfer to All",
      "abstract": "Pre-trained general-purpose language models have been a dominating component in enabling real-world natural language processing (NLP) applications. However, a pre-trained model with backdoor can be a severe threat to the applications. Most existing backdoor attacks in NLP are conducted in the fine-tuning phase by introducing malicious triggers in the targeted class, thus relying greatly on the prior knowledge of the fine-tuning task. In this paper, we propose a new approach to map the inputs containing triggers directly to a predefined output representation of the pre-trained NLP models, e.g., a predefined output representation for the classification token in BERT, instead of a target label. It can thus introduce backdoor to a wide range of downstream tasks without any prior knowledge. Additionally, in light of the unique properties of triggers in NLP, we propose two new metrics to measure the performance of backdoor attacks in terms of both effectiveness and stealthiness. Our experiments with various types of triggers show that our method is widely applicable to different fine-tuning tasks (classification and named entity recognition) and to different models (such as BERT, XLNet, BART), which poses a severe threat. Furthermore, by collaborating with the popular online model repository Hugging Face, the threat brought by our method has been confirmed. Finally, we analyze the factors that may affect the attack performance and share insights on the causes of the success of our backdoor attack.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Lujia Shen",
        "Shouling Ji",
        "Xuhong Zhang",
        "Jinfeng Li",
        "Jing Chen",
        "Jie Shi",
        "Chengfang Fang",
        "Jianwei Yin",
        "Ting Wang"
      ],
      "url": "https://arxiv.org/abs/2111.00197",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774088",
      "fetched_at": "2026-01-09T12:14:35.774089",
      "classified_at": "2026-01-09T13:24:27.616325",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_afd91528": {
      "paper_id": "seed_afd91528",
      "title": "Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation",
      "abstract": "Metaverse is expected to emerge as a new paradigm for the next-generation Internet, providing fully immersive and personalized experiences to socialize, work, and play in self-sustaining and hyper-spatio-temporal virtual world(s). The advancements in different technologies such as augmented reality, virtual reality, extended reality (XR), artificial intelligence (AI), and 5G/6G communication will be the key enablers behind the realization of AI-XR metaverse applications. While AI itself has many potential applications in the aforementioned technologies (e.g., avatar generation, network optimization), ensuring the security of AI in critical applications like AI-XR metaverse applications is profoundly crucial to avoid undesirable actions that could undermine users\u2019 privacy and safety, consequently putting their lives in danger. To this end, we attempt to analyze the security, privacy, and trustworthiness aspects associated with the use of various AI techniques in AI-XR metaverse applications. Specifically, we discuss numerous such challenges and present a taxonomy of potential solutions that could be leveraged to develop secure, private, robust, and trustworthy AI-XR applications. To highlight the real implications of AI-associated adversarial threats, we designed a metaverse-specific case study and analyzed it through the adversarial lens. Finally, we elaborate upon various open issues that require further research interest from the community.",
      "year": 2023,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Adnan Qayyum",
        "Muhammad Atif Butt",
        "Hassan Ali",
        "Muhammad Usman",
        "Osama Halabi",
        "Ala Al\u2010Fuqaha",
        "Qammer H. Abbasi",
        "Muhammad Ali Imran",
        "Junaid Qadir"
      ],
      "url": "https://openalex.org/W4385724403",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774091",
      "fetched_at": "2026-01-09T13:53:02.885770",
      "classified_at": "2026-01-09T13:57:59.780146",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385724403",
      "doi": "https://doi.org/10.1145/3614426"
    },
    "2311.11225": {
      "paper_id": "2311.11225",
      "title": "TextGuard: Provable Defense against Backdoor Attacks on Text Classification",
      "abstract": "Backdoor attacks have become a major security threat for deploying machine learning models in security-critical applications. Existing research endeavors have proposed many defenses against backdoor attacks. Despite demonstrating certain empirical defense efficacy, none of these techniques could provide a formal and provable security guarantee against arbitrary attacks. As a result, they can be easily broken by strong adaptive attacks, as shown in our evaluation. In this work, we propose TextGuard, the first provable defense against backdoor attacks on text classification. In particular, TextGuard first divides the (backdoored) training data into sub-training sets, achieved by splitting each training sentence into sub-sentences. This partitioning ensures that a majority of the sub-training sets do not contain the backdoor trigger. Subsequently, a base classifier is trained from each sub-training set, and their ensemble provides the final prediction. We theoretically prove that when the length of the backdoor trigger falls within a certain threshold, TextGuard guarantees that its prediction will remain unaffected by the presence of the triggers in training and testing inputs. In our evaluation, we demonstrate the effectiveness of TextGuard on three benchmark text classification tasks, surpassing the certification accuracy of existing certified defenses against backdoor attacks. Furthermore, we propose additional strategies to enhance the empirical performance of TextGuard. Comparisons with state-of-the-art empirical defenses validate the superiority of TextGuard in countering multiple backdoor attacks. Our code and data are available at https://github.com/AI-secure/TextGuard.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Hengzhi Pei",
        "Jinyuan Jia",
        "Wenbo Guo",
        "Bo Li",
        "Dawn Song"
      ],
      "url": "https://arxiv.org/abs/2311.11225",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774094",
      "fetched_at": "2026-01-09T12:14:35.774095",
      "classified_at": "2026-01-09T13:24:28.417756",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2006.11890": {
      "paper_id": "2006.11890",
      "title": "Graph Backdoor",
      "abstract": "One intriguing property of deep neural networks (DNNs) is their inherent vulnerability to backdoor attacks -- a trojan model responds to trigger-embedded inputs in a highly predictable manner while functioning normally otherwise. Despite the plethora of prior work on DNNs for continuous data (e.g., images), the vulnerability of graph neural networks (GNNs) for discrete-structured data (e.g., graphs) is largely unexplored, which is highly concerning given their increasing use in security-sensitive domains. To bridge this gap, we present GTA, the first backdoor attack on GNNs. Compared with prior work, GTA departs in significant ways: graph-oriented -- it defines triggers as specific subgraphs, including both topological structures and descriptive features, entailing a large design spectrum for the adversary; input-tailored -- it dynamically adapts triggers to individual graphs, thereby optimizing both attack effectiveness and evasiveness; downstream model-agnostic -- it can be readily launched without knowledge regarding downstream models or fine-tuning strategies; and attack-extensible -- it can be instantiated for both transductive (e.g., node classification) and inductive (e.g., graph classification) tasks, constituting severe threats for a range of security-critical applications. Through extensive evaluation using benchmark datasets and state-of-the-art models, we demonstrate the effectiveness of GTA. We further provide analytical justification for its effectiveness and discuss potential countermeasures, pointing to several promising research directions.",
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [
        "Zhaohan Xi",
        "Ren Pang",
        "Shouling Ji",
        "Ting Wang"
      ],
      "url": "https://arxiv.org/abs/2006.11890",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774098",
      "fetched_at": "2026-01-09T12:14:35.774099",
      "classified_at": "2026-01-09T13:24:29.162184",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2407.08935": {
      "paper_id": "2407.08935",
      "title": "Distributed Backdoor Attacks on Federated Graph Learning and Certified Defenses",
      "abstract": "Federated graph learning (FedGL) is an emerging federated learning (FL) framework that extends FL to learn graph data from diverse sources. FL for non-graph data has shown to be vulnerable to backdoor attacks, which inject a shared backdoor trigger into the training data such that the trained backdoored FL model can predict the testing data containing the trigger as the attacker desires. However, FedGL against backdoor attacks is largely unexplored, and no effective defense exists.   In this paper, we aim to address such significant deficiency. First, we propose an effective, stealthy, and persistent backdoor attack on FedGL. Our attack uses a subgraph as the trigger and designs an adaptive trigger generator that can derive the effective trigger location and shape for each graph. Our attack shows that empirical defenses are hard to detect/remove our generated triggers. To mitigate it, we further develop a certified defense for any backdoored FedGL model against the trigger with any shape at any location. Our defense involves carefully dividing a testing graph into multiple subgraphs and designing a majority vote-based ensemble classifier on these subgraphs. We then derive the deterministic certified robustness based on the ensemble classifier and prove its tightness. We extensively evaluate our attack and defense on six graph datasets. Our attack results show our attack can obtain > 90% backdoor accuracy in almost all datasets. Our defense results show, in certain cases, the certified accuracy for clean testing graphs against an arbitrary trigger with size 20 can be close to the normal accuracy under no attack, while there is a moderate gap in other cases. Moreover, the certified backdoor accuracy is always 0 for backdoored testing graphs generated by our attack, implying our defense can fully mitigate the attack. Source code is available at: https://github.com/Yuxin104/Opt-GDBA.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Yuxin Yang",
        "Qiang Li",
        "Jinyuan Jia",
        "Yuan Hong",
        "Binghui Wang"
      ],
      "url": "https://arxiv.org/abs/2407.08935",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774102",
      "fetched_at": "2026-01-09T12:14:35.774102",
      "classified_at": "2026-01-09T13:24:29.978871",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_cc1fa34c": {
      "paper_id": "seed_cc1fa34c",
      "title": "Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers",
      "abstract": "Training pipelines for machine learning (ML) based malware classification often rely on crowdsourced threat feeds, exposing a natural attack injection point. In this paper, we study the susceptibility of feature-based ML malware classifiers to backdoor poisoning attacks, specifically focusing on challenging \"clean label\" attacks where attackers do not control the sample labeling process. We propose the use of techniques from explainable machine learning to guide the selection of relevant features and values to create effective backdoor triggers in a model-agnostic fashion. Using multiple reference datasets for malware classification, including Windows PE files, PDFs, and Android applications, we demonstrate effective attacks against a diverse set of machine learning models and evaluate the effect of various constraints imposed on the attacker. To demonstrate the feasibility of our backdoor attacks in practice, we create a watermarking utility for Windows PE files that preserves the binary's functionality, and we leverage similar behavior-preserving alteration methodologies for Android and PDF files. Finally, we experiment with potential defensive strategies and show the difficulties of completely defending against these attacks, especially when the attacks blend in with the legitimate sample distribution.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Giorgio Severi",
        "Jim Meyer",
        "Scott E. Coull",
        "Alina Oprea"
      ],
      "url": "https://openalex.org/W3120073944",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774106",
      "fetched_at": "2026-01-09T13:53:03.653138",
      "classified_at": "2026-01-09T13:58:01.647072",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3120073944",
      "doi": "https://doi.org/10.48550/arxiv.2003.01031"
    },
    "seed_69e81ba3": {
      "paper_id": "seed_69e81ba3",
      "title": "TrojanModel: A Practical Trojan Attack against Automatic Speech Recognition Systems",
      "abstract": "While deep learning techniques have achieved great success in modern digital products, researchers have shown that deep learning models are susceptible to Trojan attacks. In a Trojan attack, an adversary stealthily modifies a deep learning model such that the model will output a predefined label whenever a trigger is present in the input. In this paper, we present TrojanModel, a practical Trojan attack against Automatic Speech Recognition (ASR) systems. ASR systems aim to transcribe voice input into text, which is easier for subsequent downstream applications to process. We consider a practical attack scenario in which an adversary inserts a Trojan into the acoustic model of a target ASR system. Unlike existing work that uses noise-like triggers that will easily arouse user suspicion, the work in this paper focuses on the use of unsuspicious sounds as a trigger, e.g., a piece of music playing in the background. In addition, TrojanModel does not require the retraining of a target model. Experimental results show that TrojanModel can achieve high attack success rates with negligible effect on the target model's performance. We also demonstrate that the attack is effective in an over-the-air attack scenario, where audio is played over a physical speaker and received by a microphone.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Wei Zong",
        "Yang-Wai Chow",
        "Willy Susilo",
        "Kien Do",
        "Svetha Venkatesh"
      ],
      "url": "https://openalex.org/W4385080316",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774109",
      "fetched_at": "2026-01-09T13:53:04.465172",
      "classified_at": "2026-01-09T13:58:03.547167",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385080316",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179331"
    },
    "seed_92c470d3": {
      "paper_id": "seed_92c470d3",
      "title": "MagBackdoor: Beware of Your Loudspeaker as Backdoor of Magnetic Attack for Malicious Command Injection",
      "abstract": "Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named &#x201C;DirtyFlipping&#x201D;, which uses dirty label techniques, &#x2018;label-on-label&#x2018;, to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Orson Mengara"
      ],
      "url": "https://openalex.org/W4393285751",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774111",
      "fetched_at": "2026-01-09T13:53:05.237115",
      "classified_at": "2026-01-09T13:58:05.576437",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4393285751",
      "doi": "https://doi.org/10.1109/access.2024.3382839"
    },
    "seed_c9cfa9dc": {
      "paper_id": "seed_c9cfa9dc",
      "title": "Backdooring Multimodal Learning",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a031/1RjEa7rmaxW",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774114",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2302.06279": {
      "paper_id": "2302.06279",
      "title": "Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data",
      "abstract": "Deep neural networks (DNNs) have demonstrated remarkable performance across various tasks, including image and speech recognition. However, maximizing the effectiveness of DNNs requires meticulous optimization of numerous hyperparameters and network parameters through training. Moreover, high-performance DNNs entail many parameters, which consume significant energy during training. In order to overcome these challenges, researchers have turned to spiking neural networks (SNNs), which offer enhanced energy efficiency and biologically plausible data processing capabilities, rendering them highly suitable for sensory data tasks, particularly in neuromorphic data. Despite their advantages, SNNs, like DNNs, are susceptible to various threats, including adversarial examples and backdoor attacks. Yet, the field of SNNs still needs to be explored in terms of understanding and countering these attacks.   This paper delves into backdoor attacks in SNNs using neuromorphic datasets and diverse triggers. Specifically, we explore backdoor triggers within neuromorphic data that can manipulate their position and color, providing a broader scope of possibilities than conventional triggers in domains like images. We present various attack strategies, achieving an attack success rate of up to 100% while maintaining a negligible impact on clean accuracy. Furthermore, we assess these attacks' stealthiness, revealing that our most potent attacks possess significant stealth capabilities. Lastly, we adapt several state-of-the-art defenses from the image domain, evaluating their efficacy on neuromorphic data and uncovering instances where they fall short, leading to compromised performance.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Gorka Abad",
        "Oguzhan Ersoy",
        "Stjepan Picek",
        "Aitor Urbieta"
      ],
      "url": "https://arxiv.org/abs/2302.06279",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774117",
      "fetched_at": "2026-01-09T12:14:35.774118",
      "classified_at": "2026-01-09T13:24:30.993310",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_60201f2f": {
      "paper_id": "seed_60201f2f",
      "title": "Blind Backdoors in Deep Learning Models",
      "abstract": "We investigate a new method for injecting backdoors into machine learning models, based compromising the loss-value computation in the model-training code. We use it to demonstrate new classes of backdoors strictly more powerful than those in the prior literature: single-pixel and physical backdoors in ImageNet models, backdoors that switch the model to a covert, privacy-violating task, and backdoors that do not require inference-time input modifications. \r\nOur attack is blind: the attacker cannot modify the training data, nor observe the execution of his code, nor access the resulting model. The attack code creates poisoned training inputs on the fly, as the model is training, and uses multi-objective optimization to achieve high accuracy both the main and backdoor tasks. We show how a blind attack can evade any known defense and propose new ones.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Eugene Bagdasaryan",
        "Vitaly Shmatikov"
      ],
      "url": "https://openalex.org/W3195462295",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774120",
      "fetched_at": "2026-01-09T13:53:06.491887",
      "classified_at": "2026-01-09T13:58:07.391773",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3195462295"
    },
    "seed_e57ae380": {
      "paper_id": "seed_e57ae380",
      "title": "IvySyn: Automated Vulnerability Discovery in Deep Learning Frameworks",
      "abstract": "We present IvySyn, the first fully-automated framework for discovering memory error vulnerabilities in Deep Learning (DL) frameworks. IvySyn leverages the statically-typed nature of native APIs in order to automatically perform type-aware mutation-based fuzzing on low-level kernel code. Given a set of offending inputs that trigger memory safety (and runtime) errors in low-level, native DL (C/C++) code, IvySyn automatically synthesizes code snippets in high-level languages (e.g., in Python), which propagate error-triggering input via high(er)-level APIs. Such code snippets essentially act as \"Proof of Vulnerability\", as they demonstrate the existence of bugs in native code that an attacker can target through various high-level APIs. Our evaluation shows that IvySyn significantly outperforms past approaches, both in terms of efficiency and effectiveness, in finding vulnerabilities in popular DL frameworks. Specifically, we used IvySyn to test TensorFlow and PyTorch. Although still an early prototype, IvySyn has already helped the TensorFlow and PyTorch framework developers to identify and fix 61 previously-unknown security vulnerabilities, and assign 39 unique CVEs.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Neophytos Christou",
        "Di Jin",
        "Vaggelis Atlidakis",
        "Baishakhi Ray",
        "Vasileios P. Kemerlis"
      ],
      "url": "https://openalex.org/W4298186935",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML08",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774123",
      "fetched_at": "2026-01-09T13:53:07.259795",
      "classified_at": "2026-01-09T13:58:09.470027",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4298186935",
      "doi": "https://doi.org/10.48550/arxiv.2209.14921"
    },
    "seed_483d31e5": {
      "paper_id": "seed_483d31e5",
      "title": "Towards Understanding and Detecting Cyberbullying in Real-world Images",
      "abstract": "Cyberbullying has become widely recognized as a critical social problem plaguing today's Internet users.This problem involves perpetrators using Internet-based technologies to bully their victims by sharing cyberbullying-related content.To combat this problem, researchers have studied the factors associated with such content and proposed automatic detection techniques based on those factors.However, most of these studies have mainly focused on understanding the factors of textual content, such as comments and text messages, while largely overlooking the misuse of visual content in perpetrating cyberbullying.Recent technological advancements in the way users access the Internet have led to a new cyberbullying paradigm.Perpetrators can use visual media to bully their victims through sending and distributing images with cyberbullying content.As a first step to understand the threat of cyberbullying in images, we report in this paper a comprehensive study on the nature of images used in cyberbullying.We first collect a real-world cyberbullying images dataset with 19,300 valid images.We then analyze the images in our dataset and identify the factors related to cyberbullying images that can be used to build systems to detect cyberbullying in images.Our analysis of factors in cyberbullying images reveals that unlike traditional offensive image content (e.g., violence and nudity), the factors in cyberbullying images tend to be highly contextual.We further demonstrate the effectiveness of the factors by measuring several classifier models based on the identified factors.With respect to the cyberbullying factors identified in our work, the best classifier model based on multimodal classification achieves a mean detection accuracy of 93.36% on our cyberbullying images dataset.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Nishant Vishwamitra",
        "Hongxin Hu",
        "Feng Luo",
        "Cheng Long"
      ],
      "url": "https://openalex.org/W3136177841",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774125",
      "fetched_at": "2026-01-09T13:53:08.068851",
      "classified_at": "2026-01-09T13:58:11.419638",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3136177841",
      "doi": "https://doi.org/10.14722/ndss.2021.24260"
    },
    "2308.05596": {
      "paper_id": "2308.05596",
      "title": "You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content",
      "abstract": "The spread of toxic content online is an important problem that has adverse effects on user experience online and in our society at large. Motivated by the importance and impact of the problem, research focuses on developing solutions to detect toxic content, usually leveraging machine learning (ML) models trained on human-annotated datasets. While these efforts are important, these models usually do not generalize well and they can not cope with new trends (e.g., the emergence of new toxic terms). Currently, we are witnessing a shift in the approach to tackling societal issues online, particularly leveraging large language models (LLMs) like GPT-3 or T5 that are trained on vast corpora and have strong generalizability. In this work, we investigate how we can use LLMs and prompt learning to tackle the problem of toxic content, particularly focusing on three tasks; 1) Toxicity Classification, 2) Toxic Span Detection, and 3) Detoxification. We perform an extensive evaluation over five model architectures and eight datasets demonstrating that LLMs with prompt learning can achieve similar or even better performance compared to models trained on these specific tasks. We find that prompt learning achieves around 10\\% improvement in the toxicity classification task compared to the baselines, while for the toxic span detection task we find better performance to the best baseline (0.643 vs. 0.640 in terms of $F_1$-score). Finally, for the detoxification task, we find that prompt learning can successfully reduce the average toxicity score (from 0.775 to 0.213) while preserving semantic meaning.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Xinlei He",
        "Savvas Zannettou",
        "Yun Shen",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2308.05596",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774128",
      "fetched_at": "2026-01-09T12:14:35.774129",
      "classified_at": "2026-01-09T13:24:31.833310",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_9e901d35": {
      "paper_id": "seed_9e901d35",
      "title": "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns",
      "abstract": "Large Language Models (LLMs) have raised increasing concerns about their misuse in generating hate speech. Among all the efforts to address this issue, hate speech detectors play a crucial role. However, the effectiveness of different detectors against LLM-generated hate speech remains largely unknown. In this paper, we propose HateBench, a framework for benchmarking hate speech detectors on LLM-generated hate speech. We first construct a hate speech dataset of 7,838 samples generated by six widely-used LLMs covering 34 identity groups, with meticulous annotations by three labelers. We then assess the effectiveness of eight representative hate speech detectors on the LLM-generated dataset. Our results show that while detectors are generally effective in identifying LLM-generated hate speech, their performance degrades with newer versions of LLMs. We also reveal the potential of LLM-driven hate campaigns, a new threat that LLMs bring to the field of hate speech detection. By leveraging advanced techniques like adversarial attacks and model stealing attacks, the adversary can intentionally evade the detector and automate hate campaigns online. The most potent adversarial attack achieves an attack success rate of 0.966, and its attack efficiency can be further improved by $13-21\\times$ through model stealing attacks with acceptable attack performance. We hope our study can serve as a call to action for the research community and platform moderators to fortify defenses against these emerging threats.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Xinyue Shen",
        "Yixin Wu",
        "Yiting Qu",
        "Michael Backes",
        "Savvas Zannettou",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W4406959651",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774132",
      "fetched_at": "2026-01-09T13:53:08.847182",
      "classified_at": "2026-01-09T13:58:13.539168",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4406959651",
      "doi": "https://doi.org/10.48550/arxiv.2501.16750"
    },
    "seed_9c0b2b83": {
      "paper_id": "seed_9c0b2b83",
      "title": "FARE: Enabling Fine-grained Attack Categorization under Low-quality Labeled Data",
      "abstract": "Supervised machine learning classifiers have been widely used for attack detection, but their training requires abundant high-quality labels.Unfortunately, high-quality labels are difficult to obtain in practice due to the high cost of data labeling and the constant evolution of attackers.Without such labels, it is challenging to train and deploy targeted countermeasures.In this paper, we propose FARE, a clustering method to enable fine-grained attack categorization under low-quality labels.We focus on two common issues in data labels: 1) missing labels for certain attack classes or families; and 2) only having coarsegrained labels available for different attack types.The core idea of FARE is to take full advantage of the limited labels while using the underlying data distribution to consolidate the lowquality labels.We design an ensemble model to fuse the results of multiple unsupervised learning algorithms with the given labels to mitigate the negative impact of missing classes and coarsegrained labels.We then train an input transformation network to map the input data into a low-dimensional latent space for fine-grained clustering.Using two security datasets (Android malware and network intrusion traces), we show that FARE significantly outperforms the state-of-the-art (semi-)supervised learning methods in clustering quality/correctness.Further, we perform an initial deployment of FARE by working with a large e-commerce service to detect fraudulent accounts.With realworld A/B tests and manual investigation, we demonstrate the effectiveness of FARE to catch previously-unseen frauds.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Junjie Liang",
        "Wenbo Guo",
        "Tongbo Luo",
        "Vasant Honavar",
        "Gang Wang",
        "Xinyu Xing"
      ],
      "url": "https://openalex.org/W3137832402",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774135",
      "fetched_at": "2026-01-09T13:53:09.395893",
      "classified_at": "2026-01-09T13:58:15.522364",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3137832402",
      "doi": "https://doi.org/10.14722/ndss.2021.24403"
    },
    "seed_6b713476": {
      "paper_id": "seed_6b713476",
      "title": "From Grim Reality to Practical Solution: Malware Classification in Real-World Noise",
      "abstract": "Malware datasets inevitably contain incorrect labels due to the shortage of expertise and experience needed for sample labeling. Previous research demonstrated that a training dataset with incorrectly labeled samples would result in inaccurate model learning. To address this problem, researchers have proposed various noise learning methods to offset the impact of incorrectly labeled samples, and in image recognition and text mining applications, these methods demonstrated great success. In this work, we apply both representative and state-of-the-art noise learning methods to real-world malware classification tasks. We surprisingly observe that none of the existing methods could minimize incorrect labels' impact. Through a carefully designed experiment, we discover that the inefficacy mainly results from extreme data imbalance and the high percentage of incorrectly labeled data samples. As such, we further propose a new noise learning method and name it after MORSE. Unlike existing methods, MORSE customizes and extends a state-of-the-art semi-supervised learning technique. It takes possibly incorrectly labeled data as unlabeled data and thus avoids their potential negative impact on model learning. In MORSE, we also integrate a sample re-weighting method that balances the training data usage in the model learning and thus handles the data imbalance challenge. We evaluate MORSE on both our synthesized and real-world datasets. We show that MORSE could significantly outperform existing noise learning methods and minimize the impact of incorrectly labeled data.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Xian Wu",
        "Wenbo Guo",
        "Jia Yan",
        "Bar\u0131\u015f Co\u015fkun",
        "Xinyu Xing"
      ],
      "url": "https://openalex.org/W4384948606",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774138",
      "fetched_at": "2026-01-09T13:53:10.202805",
      "classified_at": "2026-01-09T13:58:17.417590",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4384948606",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179453"
    },
    "seed_5d2be2f8": {
      "paper_id": "seed_5d2be2f8",
      "title": "Decoding the Secrets of Machine Learning in Windows Malware Classification: A Deep Dive into Datasets, Features, and Model Performance",
      "abstract": "Many studies have proposed machine-learning (ML) models for malware detection and classification, reporting an almost-perfect performance. However, they assemble ground-truth in different ways, use diverse static- and dynamic-analysis techniques for feature extraction, and even differ on what they consider a malware family. As a consequence, our community still lacks an understanding of malware classification results: whether they are tied to the nature and distribution of the collected dataset, to what extent the number of families and samples in the training dataset influence performance, and how well static and dynamic features complement each other.   This work sheds light on those open questions. by investigating the key factors influencing ML-based malware detection and classification. For this, we collect the largest balanced malware dataset so far with 67K samples from 670 families (100 samples each), and train state-of-the-art models for malware detection and family classification using our dataset. Our results reveal that static features perform better than dynamic features, and that combining both only provides marginal improvement over static features. We discover no correlation between packing and classification accuracy, and that missing behaviors in dynamically-extracted features highly penalize their performance. We also demonstrate how a larger number of families to classify make the classification harder, while a higher number of samples per family increases accuracy. Finally, we find that models trained on a uniform distribution of samples per family better generalize on unseen data.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Savino Dambra",
        "Yufei Han",
        "Simone Aonzo",
        "Platon Kotzias",
        "Antonino Vitale",
        "Juan Caballero",
        "Davide Balzarotti",
        "Leyla Bilge"
      ],
      "url": "https://arxiv.org/abs/2307.14657",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774141",
      "fetched_at": "2026-01-09T13:53:10.829278",
      "classified_at": "2026-01-09T13:58:19.231170",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "2307.14657"
    },
    "2308.05034": {
      "paper_id": "2308.05034",
      "title": "KAIROS: Practical Intrusion Detection and Investigation using Whole-system Provenance",
      "abstract": "Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approaches sacrifice at least one and struggle to achieve comparable detection performance.   Kairos leverages a novel graph neural network-based encoder-decoder architecture that learns the temporal evolution of a provenance graph's structural changes to quantify the degree of anomalousness for each system event. Then, based on this fine-grained information, Kairos reconstructs attack footprints, generating compact summary graphs that accurately describe malicious activity over a stream of system audit logs. Using state-of-the-art benchmark datasets, we demonstrate that Kairos outperforms previous approaches.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Zijun Cheng",
        "Qiujian Lv",
        "Jinyuan Liang",
        "Yan Wang",
        "Degang Sun",
        "Thomas Pasquier",
        "Xueyuan Han"
      ],
      "url": "https://arxiv.org/abs/2308.05034",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774143",
      "fetched_at": "2026-01-09T12:14:35.774144",
      "classified_at": "2026-01-09T13:24:32.473155",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_dd838dcb": {
      "paper_id": "seed_dd838dcb",
      "title": "FLASH: A Comprehensive Approach to Intrusion Detection via Provenance Graph Representation Learning",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a139/1Ub23WQw20U",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774147",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_37a647e9": {
      "paper_id": "seed_37a647e9",
      "title": "Understanding and Bridging the Gap Between Unsupervised Network Representation Learning and Security Analytics",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a012/1RjE9Q5gQrm",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774151",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2311.16940": {
      "paper_id": "2311.16940",
      "title": "FP-Fed: Privacy-Preserving Federated Detection of Browser Fingerprinting",
      "abstract": "Browser fingerprinting often provides an attractive alternative to third-party cookies for tracking users across the web. In fact, the increasing restrictions on third-party cookies placed by common web browsers and recent regulations like the GDPR may accelerate the transition. To counter browser fingerprinting, previous work proposed several techniques to detect its prevalence and severity. However, these rely on 1) centralized web crawls and/or 2) computationally intensive operations to extract and process signals (e.g., information-flow and static analysis). To address these limitations, we present FP-Fed, the first distributed system for browser fingerprinting detection. Using FP-Fed, users can collaboratively train on-device models based on their real browsing patterns, without sharing their training data with a central entity, by relying on Differentially Private Federated Learning (DP-FL). To demonstrate its feasibility and effectiveness, we evaluate FP-Fed's performance on a set of 18.3k popular websites with different privacy levels, numbers of participants, and features extracted from the scripts. Our experiments show that FP-Fed achieves reasonably high detection performance and can perform both training and inference efficiently, on-device, by only relying on runtime signals extracted from the execution trace, without requiring any resource-intensive operation.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Meenatchi Sundaram Muthu Selva Annamalai",
        "Igor Bilogrevic",
        "Emiliano De Cristofaro"
      ],
      "url": "https://arxiv.org/abs/2311.16940",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774153",
      "fetched_at": "2026-01-09T12:14:35.774154",
      "classified_at": "2026-01-09T13:24:33.120202",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_429c61e9": {
      "paper_id": "seed_429c61e9",
      "title": "GNNIC: Finding Long-Lost Sibling Functions with Abstract Similarity",
      "abstract": "Generating accurate call graphs for large programs, particularly at the operating system (OS) level, poses a well-known challenge.This difficulty stems from the widespread use of indirect calls within large programs, wherein the computation of call targets is deferred until runtime to achieve program polymorphism.Consequently, compilers are unable to statically determine indirect call edges.Recent advancements have attempted to use type analysis to globally match indirect call targets in programs.However, these approaches still suffer from low precision when handling large target programs or generic types.This paper presents GNNIC, a Graph Neural Network (GNN) based Indirect Call analyzer.GNNIC employs a technique called abstract-similarity search to accurately identify indirect call targets in large programs.The approach is based on the observation that although indirect call targets exhibit intricate polymorphic behaviors, they share common abstract characteristics, such as function descriptions, data types, and invoked function calls.We consolidate such information into a representative abstraction graph (RAG) and employ GNNs to learn function embeddings.Abstract-similarity search relies on at least one anchor target to bootstrap.Therefore, we also propose a new program analysis technique to locally identify valid targets of each indirect call.Starting from anchor targets, GNNIC can expand the search scope to find more targets of indirect calls in the whole program.The implementation of GNNIC utilizes LLVM and GNN, and we evaluated it on multiple OS kernels.The results demonstrate that GNNIC outperforms state-of-the-art type-based techniques by reducing 86% to 93% of false target functions.Moreover, the abstract similarity and precise call graphs generated by GNNIC can enhance security applications by discovering new bugs, alleviating path-explosion issues, and improving the efficiency of static program analysis.The combination of static analysis and GNNIC resulted in finding 97 new bugs in Linux and FreeBSD kernels.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Qiushi Wu",
        "Zhongshu Gu",
        "Hani Jamjoom",
        "Kangjie Lu"
      ],
      "url": "https://openalex.org/W4391724812",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774157",
      "fetched_at": "2026-01-09T13:53:12.974027",
      "classified_at": "2026-01-09T13:58:21.078434",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391724812",
      "doi": "https://doi.org/10.14722/ndss.2024.23492"
    },
    "seed_b8ef1770": {
      "paper_id": "seed_b8ef1770",
      "title": "Experimental Analyses of the Physical Surveillance Risks in Client-Side Content Scanning",
      "abstract": "Content scanning systems employ perceptual hashing algorithms to scan user content for illicit material, such as child pornography or terrorist recruitment flyers.Perceptual hashing algorithms help determine whether two images are visually similar while preserving the privacy of the input images.Several efforts from industry and academia propose scanning on client devices such as smartphones due to the impending rollout of end-to-end encryption that will make server-side scanning difficult.These proposals have met with strong criticism because of the potential for the technology to be misused for censorship.However, the risks of this technology in the context of surveillance are not well understood.Our work informs this conversation by experimentally characterizing the potential for one type of misuse -attackers manipulating the content scanning system to perform physical surveillance on target locations.Our contributions are threefold: (1) we offer a definition of physical surveillance in the context of client-side image scanning systems; (2) we experimentally characterize this risk and create a surveillance algorithm that achieves physical surveillance rates more than 30% by poisoning 0.2% of the perceptual hash database; (3) we experimentally study the trade-off between the robustness of client-side image scanning systems and surveillance, showing that more robust detection of illicit material leads to an increased potential for physical surveillance in most settings.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Ashish Hooda",
        "Andrey Labunets",
        "Tadayoshi Kohno",
        "Earlence Fernandes"
      ],
      "url": "https://openalex.org/W4391725331",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774160",
      "fetched_at": "2026-01-09T13:53:13.517172",
      "classified_at": "2026-01-09T13:58:22.919199",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391725331",
      "doi": "https://doi.org/10.14722/ndss.2024.241401"
    },
    "seed_f24b7a6e": {
      "paper_id": "seed_f24b7a6e",
      "title": "Attributions for ML-based ICS Anomaly Detection: From Theory to Practice",
      "abstract": "Industrial Control Systems (ICS) govern critical infrastructure like power plants and water treatment plants.ICS can be attacked through manipulations of its sensor or actuator values, causing physical harm.A promising technique for detecting such attacks is machine-learning-based anomaly detection, but it does not identify which sensor or actuator was manipulated and makes it difficult for ICS operators to diagnose the anomaly's root cause.Prior work has proposed using attribution methods to identify what features caused an ICS anomaly-detection model to raise an alarm, but it is unclear how well these attribution methods work in practice.In this paper, we compare state-of-the-art attribution methods for the ICS domain with real attacks from multiple datasets.We find that attribution methods for ICS anomaly detection do not perform as well as suggested in prior work and identify two main reasons.First, anomaly detectors often detect attacks either immediately or significantly after the attack start; we find that attributions computed at these detection points are inaccurate.Second, attribution accuracy varies greatly across attack properties, and attribution methods struggle with attacks on categorical-valued actuators.Despite these challenges, we find that ensembles of attributions can compensate for weaknesses in individual attribution methods.Towards practical use of attributions for ICS anomaly detection, we provide recommendations for researchers and practitioners, such as the need to evaluate attributions with diverse datasets and the potential for attributions in non-real-time workflows.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Clement Fung",
        "Eric Zeng",
        "Lujo Bauer"
      ],
      "url": "https://openalex.org/W4391725261",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774163",
      "fetched_at": "2026-01-09T13:53:14.329258",
      "classified_at": "2026-01-09T13:58:24.737807",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391725261",
      "doi": "https://doi.org/10.14722/ndss.2024.23216"
    },
    "2301.13577": {
      "paper_id": "2301.13577",
      "title": "DRAINCLoG: Detecting Rogue Accounts with Illegally-obtained NFTs using Classifiers Learned on Graphs",
      "abstract": "As Non-Fungible Tokens (NFTs) continue to grow in popularity, NFT users have become targets of phishing attacks by cybercriminals, called \\textit{NFT drainers}. Over the last year, \\$100 million worth of NFTs were stolen by drainers, and their presence remains a serious threat to the NFT trading space. However, no work has yet comprehensively investigated the behaviors of drainers in the NFT ecosystem.   In this paper, we present the first study on the trading behavior of NFT drainers and introduce the first dedicated NFT drainer detection system. We collect 127M NFT transaction data from the Ethereum blockchain and 1,135 drainer accounts from five sources for the year 2022. We find that drainers exhibit significantly different transactional and social contexts from those of regular users. With these insights, we design \\textit{DRAINCLoG}, an automatic drainer detection system utilizing Graph Neural Networks. This system effectively captures the multifaceted web of interactions within the NFT space through two distinct graphs: the NFT-User graph for transaction contexts and the User graph for social contexts. Evaluations using real-world NFT transaction data underscore the robustness and precision of our model. Additionally, we analyze the security of \\textit{DRAINCLoG} under a wide variety of evasion attacks.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Hanna Kim",
        "Jian Cui",
        "Eugene Jang",
        "Chanhee Lee",
        "Yongjae Lee",
        "Jin-Woo Chung",
        "Seungwon Shin"
      ],
      "url": "https://arxiv.org/abs/2301.13577",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774167",
      "fetched_at": "2026-01-09T12:14:35.774168",
      "classified_at": "2026-01-09T13:24:33.754505",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2309.04798": {
      "paper_id": "2309.04798",
      "title": "Low-Quality Training Data Only? A Robust Framework for Detecting Encrypted Malicious Network Traffic",
      "abstract": "Machine learning (ML) is promising in accurately detecting malicious flows in encrypted network traffic; however, it is challenging to collect a training dataset that contains a sufficient amount of encrypted malicious data with correct labels. When ML models are trained with low-quality training data, they suffer degraded performance. In this paper, we aim at addressing a real-world low-quality training dataset problem, namely, detecting encrypted malicious traffic generated by continuously evolving malware. We develop RAPIER that fully utilizes different distributions of normal and malicious traffic data in the feature space, where normal data is tightly distributed in a certain area and the malicious data is scattered over the entire feature space to augment training data for model training. RAPIER includes two pre-processing modules to convert traffic into feature vectors and correct label noises. We evaluate our system on two public datasets and one combined dataset. With 1000 samples and 45% noises from each dataset, our system achieves the F1 scores of 0.770, 0.776, and 0.855, respectively, achieving average improvements of 352.6%, 284.3%, and 214.9% over the existing methods, respectively. Furthermore, We evaluate RAPIER with a real-world dataset obtained from a security enterprise. RAPIER effectively achieves encrypted malicious traffic detection with the best F1 score of 0.773 and improves the F1 score of existing methods by an average of 272.5%.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Yuqi Qing",
        "Qilei Yin",
        "Xinhao Deng",
        "Yihao Chen",
        "Zhuotao Liu",
        "Kun Sun",
        "Ke Xu",
        "Jia Zhang",
        "Qi Li"
      ],
      "url": "https://arxiv.org/abs/2309.04798",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774171",
      "fetched_at": "2026-01-09T12:14:35.774172",
      "classified_at": "2026-01-09T13:24:34.399608",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2409.09272": {
      "paper_id": "2409.09272",
      "title": "SafeEar: Content Privacy-Preserving Audio Deepfake Detection",
      "abstract": "Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited remarkable performance in generating realistic and natural audio. However, their dark side, audio deepfake poses a significant threat to both society and individuals. Existing countermeasures largely focus on determining the genuineness of speech based on complete original audio recordings, which however often contain private content. This oversight may refrain deepfake detection from many applications, particularly in scenarios involving sensitive information like business secrets. In this paper, we propose SafeEar, a novel framework that aims to detect deepfake audios without relying on accessing the speech content within. Our key idea is to devise a neural audio codec into a novel decoupling model that well separates the semantic and acoustic information from audio samples, and only use the acoustic information (e.g., prosody and timbre) for deepfake detection. In this way, no semantic content will be exposed to the detector. To overcome the challenge of identifying diverse deepfake audio without semantic clues, we enhance our deepfake detector with real-world codec augmentation. Extensive experiments conducted on four benchmark datasets demonstrate SafeEar's effectiveness in detecting various deepfake techniques with an equal error rate (EER) down to 2.02%. Simultaneously, it shields five-language speech content from being deciphered by both machine and human auditory analysis, demonstrated by word error rates (WERs) all above 93.93% and our user study. Furthermore, our benchmark constructed for anti-deepfake and anti-content recovery evaluation helps provide a basis for future research in the realms of audio privacy preservation and deepfake detection.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Xinfeng Li",
        "Kai Li",
        "Yifan Zheng",
        "Chen Yan",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "url": "https://arxiv.org/abs/2409.09272",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774174",
      "fetched_at": "2026-01-09T12:14:35.774175",
      "classified_at": "2026-01-09T13:24:35.047200",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_4517d045": {
      "paper_id": "seed_4517d045",
      "title": "USD: NSFW Content Detection for Text-to-Image Models via Scene Graph",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-zhang-yuyang.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774178",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_e8a2ea2f": {
      "paper_id": "seed_e8a2ea2f",
      "title": "On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts",
      "abstract": "Malicious or manipulated prompts are known to exploit text-to-image models to generate unsafe images. Existing studies, however, focus on the passive exploitation of such harmful capabilities. In this paper, we investigate the proactive generation of unsafe images from benign prompts (e.g., a photo of a cat) through maliciously modified text-to-image models. Our preliminary investigation demonstrates that poisoning attacks are a viable method to achieve this goal but uncovers significant side effects, where unintended spread to non-targeted prompts compromises attack stealthiness. Root cause analysis identifies conceptual similarity as an important contributing factor to these side effects. To address this, we propose a stealthy poisoning attack method that balances covertness and performance. Our findings highlight the potential risks of adopting text-to-image models in real-world scenarios, thereby calling for future research and safety measures in this space.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yixin Wu",
        "Ning Yu",
        "Michael Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W4387964003",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774181",
      "fetched_at": "2026-01-09T13:53:15.618109",
      "classified_at": "2026-01-09T13:58:28.319369",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4387964003",
      "doi": "https://doi.org/10.48550/arxiv.2310.16613"
    },
    "seed_6976bfa7": {
      "paper_id": "seed_6976bfa7",
      "title": "VoiceWukong: Benchmarking Deepfake Voice Detection",
      "abstract": "With the rapid advancement of technologies like text-to-speech (TTS) and voice conversion (VC), detecting deepfake voices has become increasingly crucial. However, both academia and industry lack a comprehensive and intuitive benchmark for evaluating detectors. Existing datasets are limited in language diversity and lack many manipulations encountered in real-world production environments. To fill this gap, we propose VoiceWukong, a benchmark designed to evaluate the performance of deepfake voice detectors. To build the dataset, we first collected deepfake voices generated by 19 advanced and widely recognized commercial tools and 15 open-source tools. We then created 38 data variants covering six types of manipulations, constructing the evaluation dataset for deepfake voice detection. VoiceWukong thus includes 265,200 English and 148,200 Chinese deepfake voice samples. Using VoiceWukong, we evaluated 12 state-of-the-art detectors. AASIST2 achieved the best equal error rate (EER) of 13.50%, while all others exceeded 20%. Our findings reveal that these detectors face significant challenges in real-world applications, with dramatically declining performance. In addition, we conducted a user study with more than 300 participants. The results are compared with the performance of the 12 detectors and a multimodel large language model (MLLM), i.e., Qwen2-Audio, where different detectors and humans exhibit varying identification capabilities for deepfake voices at different deception levels, while the LALM demonstrates no detection ability at all. Furthermore, we provide a leaderboard for deepfake voice detection, publicly available at {https://voicewukong.github.io}.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Zhenyu Yuan",
        "Y. Zhao",
        "H. Wang"
      ],
      "url": "https://openalex.org/W4403623444",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774183",
      "fetched_at": "2026-01-09T13:53:16.102720",
      "classified_at": "2026-01-09T13:58:30.242677",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4403623444",
      "doi": "https://doi.org/10.48550/arxiv.2409.06348"
    },
    "seed_b13259c3": {
      "paper_id": "seed_b13259c3",
      "title": "SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
      "abstract": "Speech synthesis technology has brought great convenience, while the widespread usage of realistic deepfake audio has triggered hazards. Malicious adversaries may unauthorizedly collect victims' speeches and clone a similar voice for illegal exploitation (\\textit{e.g.}, telecom fraud). However, the existing defense methods cannot effectively prevent deepfake exploitation and are vulnerable to robust training techniques. Therefore, a more effective and robust data protection method is urgently needed. In response, we propose a defensive framework, \\textit{\\textbf{SafeSpeech}}, which protects the users' audio before uploading by embedding imperceptible perturbations on original speeches to prevent high-quality synthetic speech. In SafeSpeech, we devise a robust and universal proactive protection technique, \\textbf{S}peech \\textbf{PE}rturbative \\textbf{C}oncealment (\\textbf{SPEC}), that leverages a surrogate model to generate universally applicable perturbation for generative synthetic models. Moreover, we optimize the human perception of embedded perturbation in terms of time and frequency domains. To evaluate our method comprehensively, we conduct extensive experiments across advanced models and datasets, both subjectively and objectively. Our experimental results demonstrate that SafeSpeech achieves state-of-the-art (SOTA) voice protection effectiveness and transferability and is highly robust against advanced adaptive adversaries. Moreover, SafeSpeech has real-time capability in real-world tests. The source code is available at \\href{https://github.com/wxzyd123/SafeSpeech}{https://github.com/wxzyd123/SafeSpeech}.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Zhisheng Zhang",
        "Derui Wang",
        "Qianyi Yang",
        "Pengyang Huang",
        "Jialun Pu",
        "Yuxin Cao",
        "Kai Ye",
        "Jie Hao",
        "Yixian Yang"
      ],
      "url": "https://openalex.org/W4415158376",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774186",
      "fetched_at": "2026-01-09T13:53:16.604768",
      "classified_at": "2026-01-09T13:58:32.070788",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4415158376",
      "doi": "https://doi.org/10.48550/arxiv.2504.09839"
    },
    "2410.17910": {
      "paper_id": "2410.17910",
      "title": "Slot: Provenance-Driven APT Detection through Graph Reinforcement Learning",
      "abstract": "Advanced Persistent Threats (APTs) represent sophisticated cyberattacks characterized by their ability to remain undetected within the victim system for extended periods, aiming to exfiltrate sensitive data or disrupt operations. Existing detection approaches often struggle to effectively identify these complex threats, construct the attack chain for defense facilitation, or resist adversarial attacks. To overcome these challenges, we propose Slot, an advanced APT detection approach based on provenance graphs and graph reinforcement learning. Slot excels in uncovering multi-level hidden relationships, such as causal, contextual, and indirect connections, among system behaviors through provenance graph mining. By pioneering the integration of graph reinforcement learning, Slot dynamically adapts to new user activities and evolving attack strategies, enhancing its resilience against adversarial attacks. Additionally, Slot automatically constructs the attack chain according to detected attacks with clustering algorithms, providing precise identification of attack paths and facilitating the development of defense strategies. Evaluations with real-world datasets demonstrate Slot's outstanding accuracy, efficiency, adaptability, and robustness in APT detection, with most metrics surpassing state-of-the-art methods. Additionally, case studies conducted to assess Slot's effectiveness in supporting APT defense further establish it as a practical and reliable tool for cybersecurity protection.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Wei Qiao",
        "Yebo Feng",
        "Teng Li",
        "Zhuo Ma",
        "Yulong Shen",
        "JianFeng Ma",
        "Yang Liu"
      ],
      "url": "https://arxiv.org/abs/2410.17910",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774188",
      "fetched_at": "2026-01-09T12:14:35.774189",
      "classified_at": "2026-01-09T13:24:35.702353",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2405.04095": {
      "paper_id": "2405.04095",
      "title": "Combating Concept Drift with Explanatory Detection and Adaptation for Android Malware Classification",
      "abstract": "Machine learning-based Android malware classifiers achieve high accuracy in stationary environments but struggle with concept drift. The rapid evolution of malware, especially with new families, can depress classification accuracy to near-random levels. Previous research has largely centered on detecting drift samples, with expert-led label revisions on these samples to guide model retraining. However, these methods often lack a comprehensive understanding of malware concepts and provide limited guidance for effective drift adaptation, leading to unstable detection performance and high human labeling costs.   To combat concept drift, we propose DREAM, a novel system that improves drift detection and establishes an explanatory adaptation process. Our core idea is to integrate classifier and expert knowledge within a unified model. To achieve this, we embed malware explanations (or concepts) within the latent space of a contrastive autoencoder, while constraining sample reconstruction based on classifier predictions. This approach enhances classifier retraining in two key ways: 1) capturing the target classifier's characteristics to select more effective samples in drift detection and 2) enabling concept revisions that extend the classifier's semantics to provide stronger guidance for adaptation. Additionally, DREAM eliminates reliance on training data during real-time drift detection and provides a behavior-based drift explainer to support concept revision. Our evaluation shows that DREAM effectively improves the drift detection accuracy and reduces the expert analysis effort in adaptation across different malware datasets and classifiers. Notably, when updating a widely-used Drebin classifier, DREAM achieves the same accuracy with 76.6% fewer newly labeled samples compared to the best existing methods.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Yiling He",
        "Junchi Lei",
        "Zhan Qin",
        "Kui Ren",
        "Chun Chen"
      ],
      "url": "https://arxiv.org/abs/2405.04095",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774192",
      "fetched_at": "2026-01-09T12:14:35.774193",
      "classified_at": "2026-01-09T13:24:36.305326",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_6e8ef1de": {
      "paper_id": "seed_6e8ef1de",
      "title": "MM4flow: A Pre-trained Multi-modal Model for Versatile Network Traffic Analysis",
      "abstract": null,
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [],
      "url": "https://ccs25files.zoolab.org/main/ccsfa/j43ciqBt/3719027.3744804.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774196",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2506.17162": {
      "paper_id": "2506.17162",
      "title": "Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model",
      "abstract": "Malicious PDF files have emerged as a persistent threat and become a popular attack vector in web-based attacks. While machine learning-based PDF malware classifiers have shown promise, these classifiers are often susceptible to adversarial attacks, undermining their reliability. To address this issue, recent studies have aimed to enhance the robustness of PDF classifiers. Despite these efforts, the feature engineering underlying these studies remains outdated. Consequently, even with the application of cutting-edge machine learning techniques, these approaches fail to fundamentally resolve the issue of feature instability.   To tackle this, we propose a novel approach for PDF feature extraction and PDF malware detection. We introduce the PDFObj IR (PDF Object Intermediate Representation), an assembly-like language framework for PDF objects, from which we extract semantic features using a pretrained language model. Additionally, we construct an Object Reference Graph to capture structural features, drawing inspiration from program analysis. This dual approach enables us to analyze and detect PDF malware based on both semantic and structural features. Experimental results demonstrate that our proposed classifier achieves strong adversarial robustness while maintaining an exceptionally low false positive rate of only 0.07% on baseline dataset compared to state-of-the-art PDF malware classifiers.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Side Liu",
        "Jiang Ming",
        "Guodong Zhou",
        "Xinyi Liu",
        "Jianming Fu",
        "Guojun Peng"
      ],
      "url": "https://arxiv.org/abs/2506.17162",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774200",
      "fetched_at": "2026-01-09T12:14:35.774201",
      "classified_at": "2026-01-09T13:24:36.917864",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_eb0167d6": {
      "paper_id": "seed_eb0167d6",
      "title": "WtaGraph: Web Tracking and Advertising Detection using Graph Neural Networks",
      "abstract": "Web tracking and advertising (WTA) nowadays are ubiquitously performed on the web, continuously compromising users' privacy. Existing defense solutions, such as widely deployed blocking tools based on filter lists and alternative machine learning based solutions proposed in prior research, have limitations in terms of accuracy and effectiveness. In this work, we propose WTAGRAPH, a web tracking and advertising detection framework based on Graph Neural Networks (GNNs). We first construct an attributed homogenous multi-graph (AHMG) that represents HTTP network traffic, and formulate web tracking and advertising detection as a task of GNN-based edge representation learning and classification in AHMG. We then design four components in WTAGRAPH so that it can (1) collect HTTP network traffic, DOM, and JavaScript data, (2) construct AHMG and extract corresponding edge and node features, (3) build a GNN model for edge representation learning and WTA detection in the transductive learning setting, and (4) use a pre-trained GNN model for WTA detection in the inductive learning setting. We evaluate WTAGRAPH on a dataset collected from Alexa Top 10K websites, and show that WTAGRAPH can effectively detect WTA requests in both transductive and inductive learning settings. Manual verification results indicate that WTAGRAPH can detect new WTA requests that are missed by filter lists and recognize non-WTA requests that are mistakenly labeled by filter lists. Our ablation analysis, evasion evaluation, and real-time evaluation show that WTAGRAPH can have a competitive performance with flexible deployment options in practice.",
      "year": 2022,
      "venue": "2022 IEEE Symposium on Security and Privacy (SP)",
      "authors": [
        "Zhiju Yang",
        "Weiping Pei",
        "Monchu Chen",
        "Chuan Yue"
      ],
      "url": "https://openalex.org/W4288057716",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774203",
      "fetched_at": "2026-01-09T13:53:18.129220",
      "classified_at": "2026-01-09T13:58:34.003721",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4288057716",
      "doi": "https://doi.org/10.1109/sp46214.2022.9833670"
    },
    "seed_7dc13de8": {
      "paper_id": "seed_7dc13de8",
      "title": "Text Captcha Is Dead? A Large Scale Deployment and Empirical Studys",
      "abstract": "The development of deep learning techniques has significantly increased the ability of computers to recognize CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart), thus breaking or mitigating the security of existing captcha schemes. To protect against these attacks, recent works have been proposed to leverage adversarial machine learning to perturb captcha pictures. However, they either require the prior knowledge of captcha solving models or lack adaptivity to the evolving behaviors of attackers. Most importantly, none of them has been deployed in practical applications, and their practical applicability and effectiveness are unknown.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Chenghui Shi",
        "Shouling Ji",
        "Qianjun Liu",
        "Changchang Liu",
        "Yuefeng Chen",
        "Yuan He",
        "Zhe Liu",
        "Raheem Beyah",
        "Ting Wang"
      ],
      "url": "https://openalex.org/W3094898851",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774206",
      "fetched_at": "2026-01-09T13:53:18.922437",
      "classified_at": "2026-01-09T13:58:36.001564",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3094898851",
      "doi": "https://doi.org/10.1145/3372297.3417258"
    },
    "seed_465ab6b4": {
      "paper_id": "seed_465ab6b4",
      "title": "Attacks as Defenses: Designing Robust Audio CAPTCHAs Using Attacks on Automatic Speech Recognition Systems",
      "abstract": "Audio CAPTCHAs are supposed to provide a strong defense for online resources; however, advances in speech-totext mechanisms have rendered these defenses ineffective.Audio CAPTCHAs cannot simply be abandoned, as they are specifically named by the W3C as important enablers of accessibility.Accordingly, demonstrably more robust audio CAPTCHAs are important to the future of a secure and accessible Web.We look to recent literature on attacks on speech-to-text systems for inspiration for the construction of robust, principle-driven audio defenses.We begin by comparing 20 recent attack papers, classifying and measuring their suitability to serve as the basis of new \"robust to transcription\" but \"easy for humans to understand\" CAPTCHAs.After showing that none of these attacks alone are sufficient, we propose a new mechanism that is both comparatively intelligible (evaluated through a user study) and hard to automatically transcribe (i.e., P (transcription) = 4 \u00d7 10 -5 ).We also demonstrate that our audio samples have a high probability of being detected as CAPTCHAs when given to speech-to-text systems (P (evasion) = 1.77 \u00d7 10 -4 ).Finally, we show that our method can break WaveGuard, a mechanism designed to defend adversarial audio, with a 99% success rate.In so doing, we not only demonstrate a CAPTCHA that is approximately four orders of magnitude more difficult to crack, but that such systems can be designed based on the insights gained from attack papers using the differences between the ways that humans and computers process audio.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Hadi Abdullah",
        "Aditya Karlekar",
        "Saurabh Prasad",
        "Muhammad Sajidur Rahman",
        "Logan Blue",
        "Luke A. Bauer",
        "Vincent Bindschaedler",
        "Patrick Traynor"
      ],
      "url": "https://openalex.org/W4324007038",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774208",
      "fetched_at": "2026-01-09T13:53:20.392830",
      "classified_at": "2026-01-09T13:58:37.934981",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4324007038",
      "doi": "https://doi.org/10.14722/ndss.2023.24243"
    },
    "seed_74f050f6": {
      "paper_id": "seed_74f050f6",
      "title": "A Generic, Efficient, and Effortless Solver with Self-Supervised Learning for Breaking Text Captchas",
      "abstract": "Although text-based captcha, which is used to differentiate between human users and bots, has faced many attack methods, it remains a widely used security mechanism and is employed by some websites. Some deep learning-based text captcha solvers have shown excellent results, but the labor-intensive and time-consuming labeling process severely limits their viability. Previous works attempted to create easy-to-use solvers using a limited collection of labeled data. However, they are hampered by inefficient preprocessing procedures and inability to recognize the captchas with complicated security features.In this paper, we propose GeeSolver, a generic, efficient, and effortless solver for breaking text-based captchas based on self-supervised learning. Our insight is that numerous difficult-to-attack captcha schemes that \"damage\" the standard font of characters are similar to image masks. And we could leverage masked autoencoders (MAE) to improve the captcha solver to learn the latent representation from the \"unmasked\" part of the captcha images. Specifically, our model consists of a ViT encoder as latent representation extractor and a well-designed decoder for captcha recognition. We apply MAE paradigm to train our encoder, which enables the encoder to extract latent representation from local information (i.e., without masking part) that can infer the corresponding character. Further, we freeze the parameters of the encoder and leverage a few labeled captchas and many unlabeled captchas to train our captcha decoder with semi-supervised learning.Our experiments with real-world captcha schemes demonstrate that GeeSolver outperforms the state-of-the-art methods by a large margin using a few labeled captchas. We also show that GeeSolver is highly efficient as it can solve a captcha within 25 ms using a desktop CPU and 9 ms using a desktop GPU. Besides, thanks to latent representation extraction, we successfully break the hard-to-attack captcha schemes, proving the generality of our solver. We hope that our work will help security experts to revisit the design and availability of text-based captchas. The code is available at https://github.com/NSSL-SJTU/GeeSolver.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Ruijie Zhao",
        "Xianwen Deng",
        "Yanhao Wang",
        "Zhicong Yan",
        "Zhengguang Han",
        "Libo Chen",
        "Zhi Xue",
        "Yijun Wang"
      ],
      "url": "https://openalex.org/W4385679715",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774211",
      "fetched_at": "2026-01-09T13:53:21.164731",
      "classified_at": "2026-01-09T13:58:39.848283",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385679715",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179379"
    },
    "2103.03809": {
      "paper_id": "2103.03809",
      "title": "PalmTree: Learning an Assembly Language Model for Instruction Embedding",
      "abstract": "Deep learning has demonstrated its strengths in numerous binary analysis tasks, including function boundary detection, binary code search, function prototype inference, value set analysis, etc. When applying deep learning to binary analysis tasks, we need to decide what input should be fed into the neural network model. More specifically, we need to answer how to represent an instruction in a fixed-length vector. The idea of automatically learning instruction representations is intriguing, however the existing schemes fail to capture the unique characteristics of disassembly. These schemes ignore the complex intra-instruction structures and mainly rely on control flow in which the contextual information is noisy and can be influenced by compiler optimizations.   In this paper, we propose to pre-train an assembly language model called PalmTree for generating general-purpose instruction embeddings by conducting self-supervised training on large-scale unlabeled binary corpora. PalmTree utilizes three pre-training tasks to capture various characteristics of assembly language. These training tasks overcome the problems in existing schemes, thus can help to generate high-quality representations. We conduct both intrinsic and extrinsic evaluations, and compare PalmTree with other instruction embedding schemes. PalmTree has the best performance for intrinsic metrics, and outperforms the other instruction embedding schemes for all downstream tasks.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Xuezixiang Li",
        "Qu Yu",
        "Heng Yin"
      ],
      "url": "https://arxiv.org/abs/2103.03809",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774213",
      "fetched_at": "2026-01-09T12:14:35.774214",
      "classified_at": "2026-01-09T13:24:37.548165",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2111.01415": {
      "paper_id": "2111.01415",
      "title": "CALLEE: Recovering Call Graphs for Binaries with Transfer and Contrastive Learning",
      "abstract": "Recovering binary programs' call graphs is crucial for inter-procedural analysis tasks and applications based on them.transfer One of the core challenges is recognizing targets of indirect calls (i.e., indirect callees). Existing solutions all have high false positives and negatives, making call graphs inaccurate. In this paper, we propose a new solution Callee combining transfer learning and contrastive learning. The key insight is that, deep neural networks (DNNs) can automatically identify patterns concerning indirect calls, which can be more efficient than designing approximation algorithms or heuristic rules to handle various cases. Inspired by the advances in question-answering applications, we utilize contrastive learning to answer the callsite-callee question. However, one of the toughest challenges is that DNNs need large datasets to achieve high performance, while collecting large-scale indirect-call ground-truths can be computational-expensive. Since direct calls and indirect calls share similar calling conventions, it is possible to transfer knowledge learned from direct calls to indirect ones. Therefore, we leverage transfer learning to pre-train DNNs with easy-to-collect direct calls and further fine-tune the indirect-call DNNs. We evaluate Callee on several groups of targets, and results show that our solution could match callsites to callees with an F1-Measure of 94.6%, much better than state-of-the-art solutions. Further, we apply Callee to binary code similarity detection and hybrid fuzzing, and found it could greatly improve their performance.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Wenyu Zhu",
        "Zhiyao Feng",
        "Zihan Zhang",
        "Jianjun Chen",
        "Zhijian Ou",
        "Min Yang",
        "Chao Zhang"
      ],
      "url": "https://arxiv.org/abs/2111.01415",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774217",
      "fetched_at": "2026-01-09T12:14:35.774218",
      "classified_at": "2026-01-09T13:24:38.394761",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2112.02125": {
      "paper_id": "2112.02125",
      "title": "Examining Zero-Shot Vulnerability Repair with Large Language Models",
      "abstract": "Human developers can produce code with cybersecurity bugs. Can emerging 'smart' code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI's Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information - both semantically and syntactically - with natural languages. We perform a large scale study of five commercially available, black-box, \"off-the-shelf\" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model's performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Hammond Pearce",
        "Benjamin Tan",
        "Baleegh Ahmad",
        "Ramesh Karri",
        "Brendan Dolan-Gavitt"
      ],
      "url": "https://arxiv.org/abs/2112.02125",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774221",
      "fetched_at": "2026-01-09T12:14:35.774222",
      "classified_at": "2026-01-09T13:24:39.265576",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2409.02074": {
      "paper_id": "2409.02074",
      "title": "Raconteur: A Knowledgeable, Insightful, and Portable LLM-Powered Shell Command Explainer",
      "abstract": "Malicious shell commands are linchpins to many cyber-attacks, but may not be easy to understand by security analysts due to complicated and often disguised code structures. Advances in large language models (LLMs) have unlocked the possibility of generating understandable explanations for shell commands. However, existing general-purpose LLMs suffer from a lack of expert knowledge and a tendency to hallucinate in the task of shell command explanation. In this paper, we present Raconteur, a knowledgeable, expressive and portable shell command explainer powered by LLM. Raconteur is infused with professional knowledge to provide comprehensive explanations on shell commands, including not only what the command does (i.e., behavior) but also why the command does it (i.e., purpose). To shed light on the high-level intent of the command, we also translate the natural-language-based explanation into standard technique & tactic defined by MITRE ATT&CK, the worldwide knowledge base of cybersecurity. To enable Raconteur to explain unseen private commands, we further develop a documentation retriever to obtain relevant information from complementary documentations to assist the explanation process. We have created a large-scale dataset for training and conducted extensive experiments to evaluate the capability of Raconteur in shell command explanation. The experiments verify that Raconteur is able to provide high-quality explanations and in-depth insight of the intent of the command.",
      "year": 2025,
      "venue": "NDSS",
      "authors": [
        "Jiangyi Deng",
        "Xinfeng Li",
        "Yanjiao Chen",
        "Yijie Bai",
        "Haiqin Weng",
        "Yan Liu",
        "Tao Wei",
        "Wenyuan Xu"
      ],
      "url": "https://arxiv.org/abs/2409.02074",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774225",
      "fetched_at": "2026-01-09T12:14:35.774226",
      "classified_at": "2026-01-09T13:24:40.123605",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2209.03463": {
      "paper_id": "2209.03463",
      "title": "Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots",
      "abstract": "Chatbots are used in many applications, e.g., automated agents, smart home assistants, interactive characters in online games, etc. Therefore, it is crucial to ensure they do not behave in undesired manners, providing offensive or toxic responses to users. This is not a trivial task as state-of-the-art chatbot models are trained on large, public datasets openly collected from the Internet. This paper presents a first-of-its-kind, large-scale measurement of toxicity in chatbots. We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too. We then set out to design and experiment with an attack, ToxicBuddy, which relies on fine-tuning GPT-2 to generate non-toxic queries that make chatbots respond in a toxic manner. Our extensive experimental evaluation demonstrates that our attack is effective against public chatbot models and outperforms manually-crafted malicious queries proposed by previous work. We also evaluate three defense mechanisms against ToxicBuddy, showing that they either reduce the attack performance at the cost of affecting the chatbot's utility or are only effective at mitigating a portion of the attack. This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users. Overall, we are confident that ToxicBuddy can be used as an auditing tool and that our work will pave the way toward designing more effective defenses for chatbot safety.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Wai Man Si",
        "Michael Backes",
        "Jeremy Blackburn",
        "Emiliano De Cristofaro",
        "Gianluca Stringhini",
        "Savvas Zannettou",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2209.03463",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774228",
      "fetched_at": "2026-01-09T12:14:35.774229",
      "classified_at": "2026-01-09T13:24:40.899766",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_9fbb1fc2": {
      "paper_id": "seed_9fbb1fc2",
      "title": "Towards a General Video-based Keystroke Inference Attack",
      "abstract": null,
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec23summer_338-yang_zhuolin-prepub.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774233",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2306.11924": {
      "paper_id": "2306.11924",
      "title": "Deep perceptual hashing algorithms with hidden dual purpose: when client-side scanning does facial recognition",
      "abstract": "End-to-end encryption (E2EE) provides strong technical protections to individuals from interferences. Governments and law enforcement agencies around the world have however raised concerns that E2EE also allows illegal content to be shared undetected. Client-side scanning (CSS), using perceptual hashing (PH) to detect known illegal content before it is shared, is seen as a promising solution to prevent the diffusion of illegal content while preserving encryption. While these proposals raise strong privacy concerns, proponents of the solutions have argued that the risk is limited as the technology has a limited scope: detecting known illegal content. In this paper, we show that modern perceptual hashing algorithms are actually fairly flexible pieces of technology and that this flexibility could be used by an adversary to add a secondary hidden feature to a client-side scanning system. More specifically, we show that an adversary providing the PH algorithm can ``hide\" a secondary purpose of face recognition of a target individual alongside its primary purpose of image copy detection. We first propose a procedure to train a dual-purpose deep perceptual hashing model by jointly optimizing for both the image copy detection and the targeted facial recognition task. Second, we extensively evaluate our dual-purpose model and show it to be able to reliably identify a target individual 67% of the time while not impacting its performance at detecting illegal content. We also show that our model is neither a general face detection nor a facial recognition model, allowing its secondary purpose to be hidden. Finally, we show that the secondary purpose can be enabled by adding a single illegal looking image to the database. Taken together, our results raise concerns that a deep perceptual hashing-based CSS system could turn billions of user devices into tools to locate targeted individuals.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Shubham Jain",
        "Ana-Maria Cretu",
        "Antoine Cully",
        "Yves-Alexandre de Montjoye"
      ],
      "url": "https://arxiv.org/abs/2306.11924",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774236",
      "fetched_at": "2026-01-09T12:14:35.774237",
      "classified_at": "2026-01-09T13:24:41.541764",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_b5287fd2": {
      "paper_id": "seed_b5287fd2",
      "title": "Dos and Don'ts of Machine Learning in Computer Security",
      "abstract": "With the growing processing power of computing systems and the increasing availability of massive datasets, machine learning algorithms have led to major breakthroughs in many different areas. This development has influenced computer security, spawning a series of work on learning-based security systems, such as for malware detection, vulnerability discovery, and binary code analysis. Despite great potential, machine learning in security is prone to subtle pitfalls that undermine its performance and render learning-based systems potentially unsuitable for security tasks and practical deployment. In this paper, we look at this problem with critical eyes. First, we identify common pitfalls in the design, implementation, and evaluation of learning-based security systems. We conduct a study of 30 papers from top-tier security conferences within the past 10 years, confirming that these pitfalls are widespread in the current security literature. In an empirical analysis, we further demonstrate how individual pitfalls can lead to unrealistic performance and interpretations, obstructing the understanding of the security problem at hand. As a remedy, we propose actionable recommendations to support researchers in avoiding or mitigating the pitfalls where possible. Furthermore, we identify open problems when applying machine learning in security and provide directions for further research.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Daniel J. Arp",
        "Erwin Quiring",
        "Feargus Pendlebury",
        "Alexander Warnecke",
        "Fabio Pierazzi",
        "Christian Wressnegger",
        "Lorenzo Cavallaro",
        "Konrad Rieck"
      ],
      "url": "https://openalex.org/W3092978542",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774240",
      "fetched_at": "2026-01-09T13:53:23.440850",
      "classified_at": "2026-01-09T13:58:41.887781",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3092978542",
      "doi": "https://doi.org/10.48550/arxiv.2010.09470"
    },
    "seed_4cdacd1c": {
      "paper_id": "seed_4cdacd1c",
      "title": "\u201cSecurity is not my field, I\u2019m a stats guy\u201d: A Qualitative Root Cause Analysis of Barriers to Adversarial Machine Learning Defenses in Industry",
      "abstract": null,
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec23fall-prepub-324-mink.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774244",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_262179b7": {
      "paper_id": "seed_262179b7",
      "title": "Everybody\u2019s Got ML, Tell Me What Else You Have: Practitioners\u2019 Perception of ML-Based Security Tools and Explanations",
      "abstract": "Significant efforts have been investigated to develop machine learning (ML) based tools to support security operations. However, they still face key challenges in practice. A generally perceived weakness of machine learning is the lack of explanation, which motivates researchers to develop machine learning explanation techniques. However, it is not yet well understood how security practitioners perceive the benefits and pain points of machine learning and corresponding explanation methods in the context of security operations. To fill this gap and understand \"what is needed\", we conducted semi-structured interviews with 18 security practitioners with diverse roles, duties, and expertise. We find practitioners generally believe that ML tools should be used in conjunction with (instead of replacing) traditional rule-based methods. While ML's output is perceived as difficult to reason, surprisingly, rule-based methods are not strictly easier to interpret. We also find that only few practitioners considered security (robustness to adversarial attacks) as a key factor for the choice of tools. Regarding ML explanations, while recognizing their values in model verification and understanding security events, practitioners also identify gaps between existing explanation methods and the needs of their downstream tasks. We collect and synthesize the suggestions from practitioners regarding explanation scheme designs, and discuss how future work can help to address these needs.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Jaron Mink",
        "Hadjer Benkraouda",
        "Limin Yang",
        "Arridhana Ciptadi",
        "Ali Ahmadzadeh",
        "Daniel Votipka",
        "Gang Wang"
      ],
      "url": "https://openalex.org/W4385080331",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774247",
      "fetched_at": "2026-01-09T13:53:24.803882",
      "classified_at": "2026-01-09T13:58:43.738098",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385080331",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179321"
    },
    "2209.03050": {
      "paper_id": "2209.03050",
      "title": "CERBERUS: Exploring Federated Prediction of Security Events",
      "abstract": "Modern defenses against cyberattacks increasingly rely on proactive approaches, e.g., to predict the adversary's next actions based on past events. Building accurate prediction models requires knowledge from many organizations; alas, this entails disclosing sensitive information, such as network structures, security postures, and policies, which might often be undesirable or outright impossible. In this paper, we explore the feasibility of using Federated Learning (FL) to predict future security events. To this end, we introduce Cerberus, a system enabling collaborative training of Recurrent Neural Network (RNN) models for participating organizations. The intuition is that FL could potentially offer a middle-ground between the non-private approach where the training data is pooled at a central server and the low-utility alternative of only training local models. We instantiate Cerberus on a dataset obtained from a major security company's intrusion prevention product and evaluate it vis-a-vis utility, robustness, and privacy, as well as how participants contribute to and benefit from the system. Overall, our work sheds light on both the positive aspects and the challenges of using FL for this task and paves the way for deploying federated approaches to predictive security.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Mohammad Naseri",
        "Yufei Han",
        "Enrico Mariconti",
        "Yun Shen",
        "Gianluca Stringhini",
        "Emiliano De Cristofaro"
      ],
      "url": "https://arxiv.org/abs/2209.03050",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774251",
      "fetched_at": "2026-01-09T12:14:35.774252",
      "classified_at": "2026-01-09T13:24:42.145867",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_1a4b676e": {
      "paper_id": "seed_1a4b676e",
      "title": "VulChecker: Graph-based Vulnerability Localization in Source Code",
      "abstract": "We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 18,945 vulnerable functions spanning 150 CWEs and 330,492 non-vulnerable functions extracted from 7,514 commits. Our dataset covers 295 more projects than all previous datasets combined.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Yizheng Chen",
        "Zhoujie Ding",
        "Lamya Alowain",
        "Xinyun Chen",
        "David Wagner"
      ],
      "url": "https://openalex.org/W4387298393",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774255",
      "fetched_at": "2026-01-09T13:53:25.313844",
      "classified_at": "2026-01-09T13:58:46.423249",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4387298393",
      "doi": "https://doi.org/10.1145/3607199.3607242"
    },
    "seed_26ed0105": {
      "paper_id": "seed_26ed0105",
      "title": "On the Security Risks of AutoML",
      "abstract": "Neural Architecture Search (NAS) represents an emerging machine learning (ML) paradigm that automatically searches for models tailored to given tasks, which greatly simplifies the development of ML systems and propels the trend of ML democratization. Yet, little is known about the potential security risks incurred by NAS, which is concerning given the increasing use of NAS-generated models in critical domains. This work represents a solid initial step towards bridging the gap. Through an extensive empirical study of 10 popular NAS methods, we show that compared with their manually designed counterparts, NAS-generated models tend to suffer greater vulnerability to various malicious attacks (e.g., adversarial evasion, model poisoning, and functionality stealing). Further, with both empirical and analytical evidence, we provide possible explanations for such phenomena: given the prohibitive search space and training cost, most NAS methods favor models that converge fast at early training stages; this preference results in architectural properties associated with attack vulnerability (e.g., high loss smoothness and low gradient variance). Our findings not only reveal the relationships between model characteristics and attack vulnerability but also suggest the inherent connections underlying different attacks. Finally, we discuss potential remedies to mitigate such drawbacks, including increasing cell depth and suppressing skip connects, which lead to several promising research directions.",
      "year": 2021,
      "venue": "PolyU Institutional Research Archive (Hong Kong Polytechnic University)",
      "authors": [
        "Ren Pang",
        "Zhaohan Xi",
        "Shouling Ji",
        "Xiapu Luo",
        "Ting Wang"
      ],
      "url": "https://openalex.org/W3206584998",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774258",
      "fetched_at": "2026-01-09T13:53:26.070252",
      "classified_at": "2026-01-09T13:58:48.229780",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3206584998",
      "doi": "https://doi.org/10.48550/arxiv.2110.06018"
    },
    "2009.09663": {
      "paper_id": "2009.09663",
      "title": "DeepDyve: Dynamic Verification for Deep Neural Networks",
      "abstract": "Deep neural networks (DNNs) have become one of the enabling technologies in many safety-critical applications, e.g., autonomous driving and medical image analysis. DNN systems, however, suffer from various kinds of threats, such as adversarial example attacks and fault injection attacks. While there are many defense methods proposed against maliciously crafted inputs, solutions against faults presented in the DNN system itself (e.g., parameters and calculations) are far less explored. In this paper, we develop a novel lightweight fault-tolerant solution for DNN-based systems, namely DeepDyve, which employs pre-trained neural networks that are far simpler and smaller than the original DNN for dynamic verification. The key to enabling such lightweight checking is that the smaller neural network only needs to produce approximate results for the initial task without sacrificing fault coverage much. We develop efficient and effective architecture and task exploration techniques to achieve optimized risk/overhead trade-off in DeepDyve. Experimental results show that DeepDyve can reduce 90% of the risks at around 10% overhead.",
      "year": 2020,
      "venue": "ACM CCS",
      "authors": [
        "Yu Li",
        "Min Li",
        "Bo Luo",
        "Ye Tian",
        "Qiang Xu"
      ],
      "url": "https://arxiv.org/abs/2009.09663",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML08",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774260",
      "fetched_at": "2026-01-09T12:14:35.774261",
      "classified_at": "2026-01-09T13:24:42.818096",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_aed90e6e": {
      "paper_id": "seed_aed90e6e",
      "title": "NeuroPots: Realtime Proactive Defense against Bit-Flip Attacks in Neural Networks",
      "abstract": null,
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec23summer_334-liu_qi-prepub.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774264",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_41596acc": {
      "paper_id": "seed_41596acc",
      "title": "Aegis: Mitigating Targeted Bit-flip Attacks against Deep Neural Networks",
      "abstract": "Bit-flip attacks (BFAs) have attracted substantial attention recently, in which an adversary could tamper with a small number of model parameter bits to break the integrity of DNNs. To mitigate such threats, a batch of defense methods are proposed, focusing on the untargeted scenarios. Unfortunately, they either require extra trustworthy applications or make models more vulnerable to targeted BFAs. Countermeasures against targeted BFAs, stealthier and more purposeful by nature, are far from well established. In this work, we propose Aegis, a novel defense method to mitigate targeted BFAs. The core observation is that existing targeted attacks focus on flipping critical bits in certain important layers. Thus, we design a dynamic-exit mechanism to attach extra internal classifiers (ICs) to hidden layers. This mechanism enables input samples to early-exit from different layers, which effectively upsets the adversary's attack plans. Moreover, the dynamic-exit mechanism randomly selects ICs for predictions during each inference to significantly increase the attack cost for the adaptive attacks where all defense mechanisms are transparent to the adversary. We further propose a robustness training strategy to adapt ICs to the attack scenarios by simulating BFAs during the IC training phase, to increase model robustness. Extensive evaluations over four well-known datasets and two popular DNN structures reveal that Aegis could effectively mitigate different state-of-the-art targeted attacks, reducing attack success rate by 5-10$\\times$, significantly outperforming existing defense methods.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jialai Wang",
        "Ziyuan Zhang",
        "Meiqi Wang",
        "Qiu Han",
        "Tianwei Zhang",
        "Qi Li",
        "Zongpeng Li",
        "Tao Wei",
        "C. Zhang"
      ],
      "url": "https://openalex.org/W4322717137",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML10",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774268",
      "fetched_at": "2026-01-09T13:53:27.356066",
      "classified_at": "2026-01-09T13:58:50.147743",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4322717137",
      "doi": "https://doi.org/10.48550/arxiv.2302.13520"
    },
    "2109.11495": {
      "paper_id": "2109.11495",
      "title": "DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection in Security Applications",
      "abstract": "Unsupervised Deep Learning (DL) techniques have been widely used in various security-related anomaly detection applications, owing to the great promise of being able to detect unforeseen threats and superior performance provided by Deep Neural Networks (DNN). However, the lack of interpretability creates key barriers to the adoption of DL models in practice. Unfortunately, existing interpretation approaches are proposed for supervised learning models and/or non-security domains, which are unadaptable for unsupervised DL models and fail to satisfy special requirements in security domains.   In this paper, we propose DeepAID, a general framework aiming to (1) interpret DL-based anomaly detection systems in security domains, and (2) improve the practicality of these systems based on the interpretations. We first propose a novel interpretation method for unsupervised DNNs by formulating and solving well-designed optimization problems with special constraints for security domains. Then, we provide several applications based on our Interpreter as well as a model-based extension Distiller to improve security systems by solving domain-specific problems. We apply DeepAID over three types of security-related anomaly detection systems and extensively evaluate our Interpreter with representative prior works. Experimental results show that DeepAID can provide high-quality interpretations for unsupervised DL models while meeting the special requirements of security domains. We also provide several use cases to show that DeepAID can help security operators to understand model decisions, diagnose system mistakes, give feedback to models, and reduce false positives.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Dongqi Han",
        "Zhiliang Wang",
        "Wenqi Chen",
        "Ying Zhong",
        "Su Wang",
        "Han Zhang",
        "Jiahai Yang",
        "Xingang Shi",
        "Xia Yin"
      ],
      "url": "https://arxiv.org/abs/2109.11495",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774271",
      "fetched_at": "2026-01-09T12:14:35.774272",
      "classified_at": "2026-01-09T13:24:43.433888",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2309.05679": {
      "paper_id": "2309.05679",
      "title": "Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing",
      "abstract": "While enjoying the great achievements brought by deep learning (DL), people are also worried about the decision made by DL models, since the high degree of non-linearity of DL models makes the decision extremely difficult to understand. Consequently, attacks such as adversarial attacks are easy to carry out, but difficult to detect and explain, which has led to a boom in the research on local explanation methods for explaining model decisions. In this paper, we evaluate the faithfulness of explanation methods and find that traditional tests on faithfulness encounter the random dominance problem, \\ie, the random selection performs the best, especially for complex data. To further solve this problem, we propose three trend-based faithfulness tests and empirically demonstrate that the new trend tests can better assess faithfulness than traditional tests on image, natural language and security tasks. We implement the assessment system and evaluate ten popular explanation methods. Benefiting from the trend tests, we successfully assess the explanation methods on complex data for the first time, bringing unprecedented discoveries and inspiring future research. Downstream tasks also greatly benefit from the tests. For example, model debugging equipped with faithful explanation methods performs much better for detecting and correcting accuracy and security problems.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Jinwen He",
        "Kai Chen",
        "Guozhu Meng",
        "Jiangshan Zhang",
        "Congyi Li"
      ],
      "url": "https://arxiv.org/abs/2309.05679",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774275",
      "fetched_at": "2026-01-09T12:14:35.774276",
      "classified_at": "2026-01-09T13:24:44.083237",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2308.05362": {
      "paper_id": "2308.05362",
      "title": "FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis",
      "abstract": "Deep learning classifiers achieve state-of-the-art performance in various risk detection applications. They explore rich semantic representations and are supposed to automatically discover risk behaviors. However, due to the lack of transparency, the behavioral semantics cannot be conveyed to downstream security experts to reduce their heavy workload in security analysis. Although feature attribution (FA) methods can be used to explain deep learning, the underlying classifier is still blind to what behavior is suspicious, and the generated explanation cannot adapt to downstream tasks, incurring poor explanation fidelity and intelligibility. In this paper, we propose FINER, the first framework for risk detection classifiers to generate high-fidelity and high-intelligibility explanations. The high-level idea is to gather explanation efforts from model developer, FA designer, and security experts. To improve fidelity, we fine-tune the classifier with an explanation-guided multi-task learning strategy. To improve intelligibility, we engage task knowledge to adjust and ensemble FA methods. Extensive evaluations show that FINER improves explanation quality for risk detection. Moreover, we demonstrate that FINER outperforms a state-of-the-art tool in facilitating malware analysis.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Yiling He",
        "Jian Lou",
        "Zhan Qin",
        "Kui Ren"
      ],
      "url": "https://arxiv.org/abs/2308.05362",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774278",
      "fetched_at": "2026-01-09T12:14:35.774280",
      "classified_at": "2026-01-09T13:24:44.830399",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_2e078e21": {
      "paper_id": "seed_2e078e21",
      "title": "Who Are You (I Really Wanna Know)? Detecting Audio DeepFakes Through Vocal Tract Reconstruction",
      "abstract": "The rapid development of deep neural networks and generative AI has catalyzed growth in realistic speech synthesis. While this technology has great potential to improve lives, it also leads to the emergence of ''DeepFake'' where synthesized speech can be misused to deceive humans and machines for nefarious purposes. In response to this evolving threat, there has been a significant amount of interest in mitigating this threat by DeepFake detection.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Zhiyuan Yu",
        "Shixuan Zhai",
        "Ning Zhang"
      ],
      "url": "https://openalex.org/W4388856757",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774283",
      "fetched_at": "2026-01-09T13:53:28.150731",
      "classified_at": "2026-01-09T13:58:51.949547",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4388856757",
      "doi": "https://doi.org/10.1145/3576915.3623209"
    },
    "seed_5fd92817": {
      "paper_id": "seed_5fd92817",
      "title": "ImU: Physical Impersonating Attack for Face Recognition System with Natural Style Changes",
      "abstract": "This paper presents a novel physical impersonating attack against face recognition systems. It aims at generating consistent style changes across multiple pictures of the attacker under different conditions and poses. Additionally, the style changes are required to be physically realizable by make-up and can induce the intended misclassification. To achieve the goal, we develop novel techniques to embed multiple pictures of the same physical person to vectors in the StyleGAN's latent space, such that the embedded latent vectors have some implicit correlations to make the search for consistent style changes feasible. Our digital and physical evaluation results show our approach can allow an outsider attacker to successfully impersonate the insiders with consistent and natural changes.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Shengwei An",
        "Yuan Yao",
        "Qiuling Xu",
        "Shiqing Ma",
        "Guanhong Tao",
        "Siyuan Cheng",
        "Kaiyuan Zhang",
        "Yingqi Liu",
        "Guangyu Shen",
        "Ian Kelk",
        "Xiangyu Zhang"
      ],
      "url": "https://openalex.org/W4384948696",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774286",
      "fetched_at": "2026-01-09T13:53:28.973738",
      "classified_at": "2026-01-09T13:58:53.812936",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4384948696",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179360"
    },
    "seed_00817b50": {
      "paper_id": "seed_00817b50",
      "title": "DepthFake: Spoofing 3D Face Authentication with a 2D Photo",
      "abstract": "Face authentication has been widely used in access control, and the latest 3D face authentication systems employ 3D liveness detection techniques to cope with the photo replay attacks, whereby an attacker uses a 2D photo to bypass the authentication. In this paper, we analyze the security of 3D liveness detection systems that utilize structured light depth cameras and discover a new attack surface against 3D face authentication systems. We propose DepthFake attacks that can spoof a 3D face authentication using only one single 2D photo. To achieve this goal, DepthFake first estimates the 3D depth information of a target victim's face from his 2D photo. Then, DepthFake projects the carefully-crafted scatter patterns embedded with the face depth information, in order to empower the 2D photo with 3D authentication properties. We overcome a collection of practical challenges, e.g., depth estimation errors from 2D photos, depth images forgery based on structured light, the alignment of the RGB image and depth images for a face, and implemented DepthFake in laboratory setups. We validated DepthFake on 3 commercial face authentication systems (i.e., Tencent Cloud, Baidu Cloud, and 3DiVi) and one commercial access control device. The results over 50 users demonstrate that DepthFake achieves an overall Depth attack success rate of 79.4% and RGB-D attack success rate of 59.4% in the real world.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Zhihao Wu",
        "Yushi Cheng",
        "Jiahui Yang",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "url": "https://openalex.org/W4384948616",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774288",
      "fetched_at": "2026-01-09T13:53:29.763212",
      "classified_at": "2026-01-09T13:58:55.703662",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4384948616",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179429"
    },
    "seed_07a7bc21": {
      "paper_id": "seed_07a7bc21",
      "title": "Understanding the (In)Security of Cross-side Face Verification Systems in Mobile Apps: A System Perspective",
      "abstract": "Face Verification Systems (FVSes) are more and more deployed by real-world mobile applications (apps) to verify a human's claimed identity. One popular type of FVSes is called cross-side FVS (XFVS), which splits the FVS functionality into two sides: one at a mobile phone to take pictures or videos and the other at a trusted server for verification. Prior works have studied the security of XFVSes from the machine learning perspective, i.e., whether the learning models used by XFVSes are robust to adversarial attacks. However, the security of other parts of XFVSes, especially the design and implementation of the verification procedure used by XFVSes, is not well understood.In this paper, we conduct the first measurement study on the security of real-world XFVSes used by popular mobile apps from a system perspective. More specifically, we design and implement a semi-automated system, called XFVSChecker, to detect XFVSes in mobile apps and then inspect their compliance with four security properties. Our evaluation reveals that most of existing XFVS apps, including those with billions of downloads, are vulnerable to at least one of four types of attacks. These attacks require only easily available attack prerequisites, such as one photo of the victim, to pose significant security risks, including complete account takeover, identity fraud and financial loss. Our findings result in 14 Chinese National Vulnerability Database (CNVD) IDs and one of them, particularly CNVD-2021-86899, is awarded the most valuable vulnerability in 2021 among all the reported vulnerabilities to CNVD.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Xiaohan Zhang",
        "H. Ye",
        "Ziqi Huang",
        "Ye Xiao",
        "Yinzhi Cao",
        "Yuan Zhang",
        "Min Yang"
      ],
      "url": "https://openalex.org/W4385187425",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774291",
      "fetched_at": "2026-01-09T13:53:30.591914",
      "classified_at": "2026-01-09T13:58:57.504690",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385187425",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179474"
    },
    "2210.09421": {
      "paper_id": "2210.09421",
      "title": "Deepfake Text Detection: Limitations and Opportunities",
      "abstract": "Recent advances in generative models for language have enabled the creation of convincing synthetic text or deepfake text. Prior work has demonstrated the potential for misuse of deepfake text to mislead content consumers. Therefore, deepfake text detection, the task of discriminating between human and machine-generated text, is becoming increasingly critical. Several defenses have been proposed for deepfake text detection. However, we lack a thorough understanding of their real-world applicability. In this paper, we collect deepfake text from 4 online services powered by Transformer-based tools to evaluate the generalization ability of the defenses on content in the wild. We develop several low-cost adversarial attacks, and investigate the robustness of existing defenses against an adaptive attacker. We find that many defenses show significant degradation in performance under our evaluation scenarios compared to their original claimed performance. Our evaluation shows that tapping into the semantic information in the text content is a promising approach for improving the robustness and generalization performance of deepfake text detection schemes.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Jiameng Pu",
        "Zain Sarwar",
        "Sifat Muhammad Abdullah",
        "Abdullah Rehman",
        "Yoonjin Kim",
        "Parantapa Bhattacharya",
        "Mobin Javed",
        "Bimal Viswanath"
      ],
      "url": "https://arxiv.org/abs/2210.09421",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774294",
      "fetched_at": "2026-01-09T12:14:35.774295",
      "classified_at": "2026-01-09T13:24:45.431566",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2303.14822": {
      "paper_id": "2303.14822",
      "title": "MGTBench: Benchmarking Machine-Generated Text Detection",
      "abstract": "Nowadays, powerful large language models (LLMs) such as ChatGPT have demonstrated revolutionary power in a variety of tasks. Consequently, the detection of machine-generated texts (MGTs) is becoming increasingly crucial as LLMs become more advanced and prevalent. These models have the ability to generate human-like language, making it challenging to discern whether a text is authored by a human or a machine. This raises concerns regarding authenticity, accountability, and potential bias. However, existing methods for detecting MGTs are evaluated using different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework that encompasses various methodologies. Furthermore, it remains unclear how existing detection methods would perform against powerful LLMs. In this paper, we fill this gap by proposing the first benchmark framework for MGT detection against powerful LLMs, named MGTBench. Extensive evaluations on public datasets with curated texts generated by various powerful LLMs such as ChatGPT-turbo and Claude demonstrate the effectiveness of different detection methods. Our ablation study shows that a larger number of words in general leads to better performance and most detection methods can achieve similar performance with much fewer training samples. Moreover, we delve into a more challenging task: text attribution. Our findings indicate that the model-based detection methods still perform well in the text attribution task. To investigate the robustness of different detection methods, we consider three adversarial attacks, namely paraphrasing, random spacing, and adversarial perturbations. We discover that these attacks can significantly diminish detection effectiveness, underscoring the critical need for the development of more robust detection methods.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Xinlei He",
        "Xinyue Shen",
        "Zeyuan Chen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2303.14822",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774297",
      "fetched_at": "2026-01-09T12:14:35.774298",
      "classified_at": "2026-01-09T13:24:46.075134",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2510.05173": {
      "paper_id": "2510.05173",
      "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models",
      "abstract": "Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions. However, these models are highly vulnerable to adversarial prompts, which can bypass safety measures and produce harmful content. Despite various defensive strategies, achieving robustness against attacks while maintaining practical utility in real-world applications remains a significant challenge. To address this issue, we first conduct an empirical study of the text encoder in the Stable Diffusion (SD) model, which is a widely used and representative text-to-image model. Our findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting distinct distributional patterns between benign and adversarial prompts in its embedding space. Building on this insight, we introduce SafeGuider, a two-step framework designed for robust safety control without compromising generation quality. SafeGuider combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm. This integration enables the framework to maintain high-quality image generation for benign prompts while ensuring robust defense against both in-domain and out-of-domain attacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack success rates, achieving a maximum rate of only 5.48\\% across various attack scenarios. Moreover, instead of refusing to generate or producing black images for unsafe prompts, SafeGuider generates safe and meaningful images, enhancing its practical utility. In addition, SafeGuider is not limited to the SD model and can be effectively applied to other text-to-image models, such as the Flux model, demonstrating its versatility and adaptability across different architectures. We hope that SafeGuider can shed some light on the practical deployment of secure text-to-image systems.",
      "year": 2025,
      "venue": "CCS",
      "authors": [
        "Peigui Qi",
        "Kunsheng Tang",
        "Wenbo Zhou",
        "Weiming Zhang",
        "Nenghai Yu",
        "Tianwei Zhang",
        "Qing Guo",
        "Jie Zhang"
      ],
      "url": "https://arxiv.org/abs/2510.05173",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774302",
      "fetched_at": "2026-01-09T12:14:35.774303",
      "classified_at": "2026-01-09T13:24:46.711426",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_847fa1e6": {
      "paper_id": "seed_847fa1e6",
      "title": "SoK: The Good, The Bad, and The Unbalanced: Measuring Structural Limitations of Deepfake Media Datasets",
      "abstract": null,
      "year": 2024,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity24-layton.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774306",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_50efc607": {
      "paper_id": "seed_50efc607",
      "title": "\"Better Be Computer or I\u2019m Dumb\": A Large-Scale Evaluation of Humans as Audio Deepfake Detectors",
      "abstract": null,
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [],
      "url": "https://cise.ufl.edu/~butler/pubs/ccs24-warren-deepfake.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774311",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2302.05319": {
      "paper_id": "2302.05319",
      "title": "Large Language Models for Code: Security Hardening and Adversarial Testing",
      "abstract": "Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN closely matches the original LMs in functional correctness.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Jingxuan He",
        "Martin Vechev"
      ],
      "url": "https://arxiv.org/abs/2302.05319",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774313",
      "fetched_at": "2026-01-09T12:14:35.774314",
      "classified_at": "2026-01-09T13:24:47.487741",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_ae484b94": {
      "paper_id": "seed_ae484b94",
      "title": "DeGPT: Optimizing Decompiler Output with LLM",
      "abstract": "Reverse engineering is essential in malware analysis, vulnerability discovery, etc. Decompilers assist the reverse engineers by lifting the assembly to the high-level programming language, which highly boosts binary comprehension.However, decompilers suffer from problems such as meaningless variable names, redundant variables, and lacking comments describing the purpose of the code.Previous studies have shown promising performance in refining the decompiler output by training the models with huge datasets containing various decompiler outputs.However, even datasets that take much time to construct cover limited binaries in the real world.The performance degrades severely facing the binary migration.In this paper, we present DeGPT, an end-to-end framework aiming to optimize the decompiler output to improve its readability and simplicity and further assist the reverse engineers in understanding the binaries better.The Large Language Model (LLM) can mitigate performance degradation with its extraordinary ability endowed by large model size and training set containing rich multi-modal data.However, its potential is difficult to unlock through one-shot use.Thus, we propose the three-role mechanism, which includes referee (R_ref), advisor (R_adv), and operator (R_ope), to adapt the LLM to our optimization tasks.Specifically, R_ref provides the optimization scheme for the target decompiler output, while R_adv gives the rectification measures based on the scheme, and R_ope inspects whether the optimization changes the original function semantics and concludes the final verdict about whether to accept the optimizations.We evaluate DeGPT on the datasets containing decompiler outputs of various software, such as the practical command line tools, malware, a library for audio processing, and implementations of algorithms.The experimental results show that even on the output of the current top-level decompiler (Ghidra), DeGPT can achieve 24.4% reduction in the cognitive burden of understanding the decompiler outputs and provide comments of which 62.9% can provide practical semantics for the reverse engineers to help the understanding of binaries.Our user surveys also show that the optimizations can significantly simplify the code and add helpful semantic information (variable names and comments), facilitating a quick and accurate understanding of the binary.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Peiwei Hu",
        "Ruigang Liang",
        "Kai Chen"
      ],
      "url": "https://openalex.org/W4391725269",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774317",
      "fetched_at": "2026-01-09T13:53:32.651578",
      "classified_at": "2026-01-09T13:58:59.410491",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391725269",
      "doi": "https://doi.org/10.14722/ndss.2024.24401"
    },
    "2409.12699": {
      "paper_id": "2409.12699",
      "title": "PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs)",
      "abstract": "The capability of generating high-quality source code using large language models (LLMs) reduces software development time and costs. However, they often introduce security vulnerabilities due to training on insecure open-source data. This highlights the need for ensuring secure and functional code generation. This paper introduces PromSec, an algorithm for prom optimization for secure and functioning code generation using LLMs. In PromSec, we combine 1) code vulnerability clearing using a generative adversarial graph neural network, dubbed as gGAN, to fix and reduce security vulnerabilities in generated codes and 2) code generation using an LLM into an interactive loop, such that the outcome of the gGAN drives the LLM with enhanced prompts to generate secure codes while preserving their functionality. Introducing a new contrastive learning approach in gGAN, we formulate code-clearing and generation as a dual-objective optimization problem, enabling PromSec to notably reduce the number of LLM inferences. PromSec offers a cost-effective and practical solution for generating secure, functional code. Extensive experiments conducted on Python and Java code datasets confirm that PromSec effectively enhances code security while upholding its intended functionality. Our experiments show that while a state-of-the-art approach fails to address all code vulnerabilities, PromSec effectively resolves them. Moreover, PromSec achieves more than an order-of-magnitude reduction in operation time, number of LLM queries, and security analysis costs. Furthermore, prompts optimized with PromSec for a certain LLM are transferable to other LLMs across programming languages and generalizable to unseen vulnerabilities in training. This study is a step in enhancing the trustworthiness of LLMs for secure and functional code generation, supporting their integration into real-world software development.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Mahmoud Nazzal",
        "Issa Khalil",
        "Abdallah Khreishah",
        "NhatHai Phan"
      ],
      "url": "https://arxiv.org/abs/2409.12699",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML08",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774321",
      "fetched_at": "2026-01-09T12:14:35.774322",
      "classified_at": "2026-01-09T13:24:48.090928",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_4c902d07": {
      "paper_id": "seed_4c902d07",
      "title": "We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs",
      "abstract": "The reliance of popular programming languages such as Python and JavaScript on centralized package repositories and open-source software, combined with the emergence of code-generating Large Language Models (LLMs), has created a new type of threat to the software supply chain: package hallucinations. These hallucinations, which arise from fact-conflicting errors when generating code using LLMs, represent a novel form of package confusion attack that poses a critical threat to the integrity of the software supply chain. This paper conducts a rigorous and comprehensive evaluation of package hallucinations across different programming languages, settings, and parameters, exploring how a diverse set of models and configurations affect the likelihood of generating erroneous package recommendations and identifying the root causes of this phenomenon. Using 16 popular LLMs for code generation and two unique prompt datasets, we generate 576,000 code samples in two programming languages that we analyze for package hallucinations. Our findings reveal that that the average percentage of hallucinated packages is at least 5.2% for commercial models and 21.7% for open-source models, including a staggering 205,474 unique examples of hallucinated package names, further underscoring the severity and pervasiveness of this threat. To overcome this problem, we implement several hallucination mitigation strategies and show that they are able to significantly reduce the number of package hallucinations while maintaining code quality. Our experiments and findings highlight package hallucinations as a persistent and systemic phenomenon while using state-of-the-art LLMs for code generation, and a significant challenge which deserves the research community's urgent attention.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Joseph Spracklen",
        "Raveen Wijewickrama",
        "A. H. M. Nazmus Sakib",
        "Anindya Maiti",
        "Murtuza Jadliwala"
      ],
      "url": "https://openalex.org/W4399795583",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774325",
      "fetched_at": "2026-01-09T13:53:33.185086",
      "classified_at": "2026-01-09T13:59:01.460734",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4399795583",
      "doi": "https://doi.org/10.48550/arxiv.2406.10279"
    },
    "seed_8bf018c0": {
      "paper_id": "seed_8bf018c0",
      "title": "Transferable Multimodal Attack on Vision-Language Pre-training Models",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a102/1Ub239H4xyg",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774327",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2305.12082": {
      "paper_id": "2305.12082",
      "title": "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
      "abstract": "Text-to-image generative models such as Stable Diffusion and DALL$\\cdot$E raise many ethical concerns due to the generation of harmful images such as Not-Safe-for-Work (NSFW) ones. To address these ethical concerns, safety filters are often adopted to prevent the generation of NSFW images. In this work, we propose SneakyPrompt, the first automated attack framework, to jailbreak text-to-image generative models such that they generate NSFW images even if safety filters are adopted. Given a prompt that is blocked by a safety filter, SneakyPrompt repeatedly queries the text-to-image generative model and strategically perturbs tokens in the prompt based on the query results to bypass the safety filter. Specifically, SneakyPrompt utilizes reinforcement learning to guide the perturbation of tokens. Our evaluation shows that SneakyPrompt successfully jailbreaks DALL$\\cdot$E 2 with closed-box safety filters to generate NSFW images. Moreover, we also deploy several state-of-the-art, open-source safety filters on a Stable Diffusion model. Our evaluation shows that SneakyPrompt not only successfully generates NSFW images, but also outperforms existing text adversarial attacks when extended to jailbreak text-to-image generative models, in terms of both the number of queries and qualities of the generated NSFW images. SneakyPrompt is open-source and available at this repository: \\url{https://github.com/Yuchen413/text2image_safety}.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Yuchen Yang",
        "Bo Hui",
        "Haolin Yuan",
        "Neil Gong",
        "Yinzhi Cao"
      ],
      "url": "https://arxiv.org/abs/2305.12082",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774330",
      "fetched_at": "2026-01-09T12:14:35.774331",
      "classified_at": "2026-01-09T13:24:48.922206",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_1ba3949d": {
      "paper_id": "seed_1ba3949d",
      "title": "SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models",
      "abstract": "Text-to-image (T2I) models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from text descriptions in recent years. However, text-to-image models may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexually explicit scenarios. Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper text embeddings, which can block sexually explicit content (e.g., naked) but may still be vulnerable to adversarial prompts -- inputs that appear innocent but are ill-intended. In this paper, we present SafeGen, a framework to mitigate sexual content generation by text-to-image models in a text-agnostic manner. The key idea is to eliminate explicit visual representations from the model regardless of the text input. In this way, the text-to-image model is resistant to adversarial prompts since such unsafe visual representations are obstructed from within. Extensive experiments conducted on four datasets and large-scale user studies demonstrate SafeGen's effectiveness in mitigating sexually explicit content generation while preserving the high-fidelity of benign images. SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.4% sexual content removal performance. Furthermore, our constructed benchmark of adversarial prompts provides a basis for future development and evaluation of anti-NSFW-generation methods.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Xinfeng Li",
        "Yuchen Yang",
        "Jiangyi Deng",
        "Chen Yan",
        "Yanjiao Chen",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "url": "https://arxiv.org/abs/2404.06666",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774334",
      "fetched_at": "2026-01-09T13:53:34.612925",
      "classified_at": "2026-01-09T13:59:03.603437",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "2404.06666"
    },
    "2309.14122": {
      "paper_id": "2309.14122",
      "title": "SurrogatePrompt: Bypassing the Safety Filter of Text-to-Image Models via Substitution",
      "abstract": "Advanced text-to-image models such as DALL$\\cdot$E 2 and Midjourney possess the capacity to generate highly realistic images, raising significant concerns regarding the potential proliferation of unsafe content. This includes adult, violent, or deceptive imagery of political figures. Despite claims of rigorous safety mechanisms implemented in these models to restrict the generation of not-safe-for-work (NSFW) content, we successfully devise and exhibit the first prompt attacks on Midjourney, resulting in the production of abundant photorealistic NSFW images. We reveal the fundamental principles of such prompt attacks and suggest strategically substituting high-risk sections within a suspect prompt to evade closed-source safety measures. Our novel framework, SurrogatePrompt, systematically generates attack prompts, utilizing large language models, image-to-text, and image-to-image modules to automate attack prompt creation at scale. Evaluation results disclose an 88% success rate in bypassing Midjourney's proprietary safety filter with our attack prompts, leading to the generation of counterfeit images depicting political figures in violent scenarios. Both subjective and objective assessments validate that the images generated from our attack prompts present considerable safety hazards.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Zhongjie Ba",
        "Jieming Zhong",
        "Jiachen Lei",
        "Peng Cheng",
        "Qinglong Wang",
        "Zhan Qin",
        "Zhibo Wang",
        "Kui Ren"
      ],
      "url": "https://arxiv.org/abs/2309.14122",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774337",
      "fetched_at": "2026-01-09T12:14:35.774338",
      "classified_at": "2026-01-09T13:24:49.856082",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2408.07728": {
      "paper_id": "2408.07728",
      "title": "Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies",
      "abstract": "We present Moderator, a policy-based model management system that allows administrators to specify fine-grained content moderation policies and modify the weights of a text-to-image (TTI) model to make it significantly more challenging for users to produce images that violate the policies. In contrast to existing general-purpose model editing techniques, which unlearn concepts without considering the associated contexts, Moderator allows admins to specify what content should be moderated, under which context, how it should be moderated, and why moderation is necessary. Given a set of policies, Moderator first prompts the original model to generate images that need to be moderated, then uses these self-generated images to reverse fine-tune the model to compute task vectors for moderation and finally negates the original model with the task vectors to decrease its performance in generating moderated content. We evaluated Moderator with 14 participants to play the role of admins and found they could quickly learn and author policies to pass unit tests in approximately 2.29 policy iterations. Our experiment with 32 stable diffusion users suggested that Moderator can prevent 65% of users from generating moderated content under 15 attempts and require the remaining users an average of 8.3 times more attempts to generate undesired content.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Peiran Wang",
        "Qiyu Li",
        "Longxuan Yu",
        "Ziyao Wang",
        "Ang Li",
        "Haojian Jin"
      ],
      "url": "https://arxiv.org/abs/2408.07728",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML10",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774341",
      "fetched_at": "2026-01-09T12:14:35.774342",
      "classified_at": "2026-01-09T13:24:50.504545",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_097119fe": {
      "paper_id": "seed_097119fe",
      "title": "Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities",
      "abstract": "Vision-language models (VLMs) are increasingly applied to identify unsafe or inappropriate images due to their internal ethical standards and powerful reasoning abilities. However, it is still unclear whether they can recognize various unsafe concepts when presented in different modalities, such as text and images. To address this, we first compile the UnsafeConcepts dataset, featuring 75 unsafe concepts, i.e., ``Swastika,'' ``Sexual Harassment,'' and ``Assaults,'' along with associated 1.5K images. We then conduct a systematic evaluation of VLMs' perception (concept recognition) and alignment (ethical reasoning) capabilities. We assess eight popular VLMs and find that, although most VLMs accurately perceive unsafe concepts, they sometimes mistakenly classify these concepts as safe. We also identify a consistent modality gap among open-source VLMs in distinguishing between visual and textual unsafe concepts. To bridge this gap, we introduce a simplified reinforcement learning (RL)-based approach using proximal policy optimization (PPO) to strengthen the ability to identify unsafe concepts from images. Our approach uses reward scores based directly on VLM responses, bypassing the need for collecting human-annotated preference data to train a new reward model. Experimental results show that our approach effectively enhances VLM alignment on images while preserving general capabilities. It outperforms baselines such as supervised fine-tuning (SFT) and direct preference optimization (DPO). We hope our dataset, evaluation findings, and proposed alignment solution contribute to the community's efforts in advancing safe VLMs.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yiting Qu",
        "Michael Backes",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W4414944889",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774345",
      "fetched_at": "2026-01-09T13:53:35.142836",
      "classified_at": "2026-01-09T13:59:05.536783",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4414944889",
      "doi": "https://doi.org/10.48550/arxiv.2507.11155"
    },
    "seed_04d1b2e1": {
      "paper_id": "seed_04d1b2e1",
      "title": "Are CAPTCHAs Still Bot-hard? Generalized Visual CAPTCHA Solving with Agentic Vision Language Model",
      "abstract": "Abstract Researchers are defining new types of interactions between humans and machine learning algorithms generically called human-in-the-loop machine learning. Depending on who is in control of the learning process, we can identify: active learning, in which the system remains in control; interactive machine learning, in which there is a closer interaction between users and learning systems; and machine teaching, where human domain experts have control over the learning process. Aside from control, humans can also be involved in the learning process in other ways. In curriculum learning human domain experts try to impose some structure on the examples presented to improve the learning; in explainable AI the focus is on the ability of the model to explain to humans why a given solution was chosen. This collaboration between AI models and humans should not be limited only to the learning process; if we go further, we can see other terms that arise such as Usable and Useful AI. In this paper we review the state of the art of the techniques involved in the new forms of relationship between humans and ML algorithms. Our contribution is not merely listing the different approaches, but to provide definitions clarifying confusing, varied and sometimes contradictory terms; to elucidate and determine the boundaries between the different methods; and to correlate all the techniques searching for the connections and influences between them.",
      "year": 2022,
      "venue": "Artificial Intelligence Review",
      "authors": [
        "Eduardo Mosqueira-Rey",
        "Elena Hern\u00e1ndez-Pereira",
        "David Alonso-R\u00edos",
        "Jos\u00e9 Bobes-Bascar\u00e1n",
        "\u00c1ngel Fern\u00e1ndez-Leal"
      ],
      "url": "https://openalex.org/W4292289324",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774348",
      "fetched_at": "2026-01-09T13:53:35.941917",
      "classified_at": "2026-01-09T13:59:07.340751",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4292289324",
      "doi": "https://doi.org/10.1007/s10462-022-10246-w"
    },
    "seed_d50d73f3": {
      "paper_id": "seed_d50d73f3",
      "title": "From Meme to Threat: On the Hateful Meme Understanding and Induced Hateful Content Generation in Open-Source Vision Language Models",
      "abstract": "Data-driven and machine learning based approaches for detecting, categorising and measuring abusive content such as hate speech and harassment have gained traction due to their scalability, robustness and increasingly high performance. Making effective detection systems for abusive content relies on having the right training datasets, reflecting a widely accepted mantra in computer science: Garbage In, Garbage Out. However, creating training datasets which are large, varied, theoretically-informed and that minimize biases is difficult, laborious and requires deep expertise. This paper systematically reviews 63 publicly available training datasets which have been created to train abusive language classifiers. It also reports on creation of a dedicated website for cataloguing abusive language data hatespeechdata.com . We discuss the challenges and opportunities of open science in this field, and argue that although more dataset sharing would bring many benefits it also poses social and ethical risks which need careful consideration. Finally, we provide evidence-based recommendations for practitioners creating new abusive content training datasets.",
      "year": 2020,
      "venue": "PLoS ONE",
      "authors": [
        "Bertie Vidgen",
        "Leon Derczynski"
      ],
      "url": "https://openalex.org/W3014487746",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774350",
      "fetched_at": "2026-01-09T13:53:36.751129",
      "classified_at": "2026-01-09T13:59:09.261170",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3014487746",
      "doi": "https://doi.org/10.1371/journal.pone.0243300"
    },
    "seed_9a1e7666": {
      "paper_id": "seed_9a1e7666",
      "title": "MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots",
      "abstract": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI) services due to their exceptional proficiency in understanding and generating human-like text. LLM chatbots, in particular, have seen widespread adoption, transforming human-machine interactions. However, these LLM chatbots are susceptible to \"jailbreak\" attacks, where malicious users manipulate prompts to elicit inappropriate or sensitive responses, contravening service policies. Despite existing attempts to mitigate such threats, our research reveals a substantial gap in our understanding of these vulnerabilities, largely due to the undisclosed defensive measures implemented by LLM service providers.   In this paper, we present Jailbreaker, a comprehensive framework that offers an in-depth understanding of jailbreak attacks and countermeasures. Our work makes a dual contribution. First, we propose an innovative methodology inspired by time-based SQL injection techniques to reverse-engineer the defensive strategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat. This time-sensitive approach uncovers intricate details about these services' defenses, facilitating a proof-of-concept attack that successfully bypasses their mechanisms. Second, we introduce an automatic generation method for jailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential of automated jailbreak generation across various commercial LLM chatbots. Our method achieves a promising average success rate of 21.58%, significantly outperforming the effectiveness of existing techniques. We have responsibly disclosed our findings to the concerned service providers, underscoring the urgent need for more robust defenses. Jailbreaker thus marks a significant step towards understanding and mitigating jailbreak threats in the realm of LLM chatbots.",
      "year": 2023,
      "venue": "NDSS",
      "authors": [
        "Gelei Deng",
        "Yi Liu",
        "Yuekang Li",
        "Kailong Wang",
        "Ying Zhang",
        "Zefeng Li",
        "Haoyu Wang",
        "Tianwei Zhang",
        "Yang Liu"
      ],
      "url": "https://arxiv.org/abs/2307.08715",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774354",
      "fetched_at": "2026-01-09T13:53:37.270302",
      "classified_at": "2026-01-09T13:59:11.058826",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "2307.08715"
    },
    "seed_060979ef": {
      "paper_id": "seed_060979ef",
      "title": "Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries",
      "abstract": "Recent advances in Large Language Models (LLMs) have led to impressive alignment where models learn to distinguish harmful from harmless queries through supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). In this paper, we reveal a subtle yet impactful weakness in these aligned models. We find that simply appending multiple end of sequence (eos) tokens can cause a phenomenon we call context segmentation, which effectively shifts both harmful and benign inputs closer to the refusal boundary in the hidden space. Building on this observation, we propose a straightforward method to BOOST jailbreak attacks by appending eos tokens. Our systematic evaluation shows that this strategy significantly increases the attack success rate across 8 representative jailbreak techniques and 16 open-source LLMs, ranging from 2B to 72B parameters. Moreover, we develop a novel probing mechanism for commercial APIs and discover that major providers such as OpenAI, Anthropic, and Qwen do not filter eos tokens, making them similarly vulnerable. These findings highlight a hidden yet critical blind spot in existing alignment and content filtering approaches. We call for heightened attention to eos tokens' unintended influence on model behaviors, particularly in production systems. Our work not only calls for an input-filtering based defense, but also points to new defenses that make refusal boundaries more robust and generalizable, as well as fundamental alignment techniques that can defend against context segmentation attacks.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jiahao Yu",
        "Haozheng Luo",
        "Jerry Yao-Chieh",
        "Wenbo Guo",
        "Han Liu",
        "Xinyu Xing"
      ],
      "url": "https://openalex.org/W4399317489",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774357",
      "fetched_at": "2026-01-09T13:53:38.074828",
      "classified_at": "2026-01-09T13:59:12.884574",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4399317489",
      "doi": "https://doi.org/10.48550/arxiv.2405.20653"
    },
    "seed_44804ce5": {
      "paper_id": "seed_44804ce5",
      "title": "Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models",
      "abstract": "Since its emergence in the late 19th century, coloured identity has been pivotal to racial thinking in southern Africa. The nature of colouredness is a highly emotive and controversial issue as it embodies many of the racial antagonisms, ambiguities and derogations prevalent in the subcontinent. Throughout their existence coloured communities have had to contend with being marginal minorities stigmatised as the insalubrious by-products of miscegenation. Burdened By Race showcases recent innovative research and writing on coloured identity in southern Africa. Drawing on a wide range of disciplines and applying fresh theoretical insights, the book brings new levels of understanding to processes of coloured self-identification. It examines diverse manifestations of colouredness, using interlinking themes and case studies from South Africa, Zimbabwe, Zambia and Malawi to present analyses that challenge and overturn much of the conventional wisdom around identity in the current literature.",
      "year": 2009,
      "venue": "Directory of Open access Books (OAPEN Foundation)",
      "authors": [
        "Mohamed Adhikari"
      ],
      "url": "https://openalex.org/W4211044653",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774360",
      "fetched_at": "2026-01-09T13:53:38.835366",
      "classified_at": "2026-01-09T13:59:14.687813",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4211044653",
      "doi": "https://doi.org/10.26530/oapen_628130"
    },
    "seed_0079375d": {
      "paper_id": "seed_0079375d",
      "title": "Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive Analysis and Defense",
      "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities across various domains. Accompanying the evolving capabilities and expanding deployment scenarios of LLMs, their deployment challenges escalate due to their sheer scale and the advanced yet complex activation designs prevalent in notable model series, such as Llama, Gemma, Mistral. These challenges have become particularly pronounced in resource-constrained deployment scenarios, where mitigating inference bottlenecks is imperative. Among various recent efforts, activation approximation has emerged as a promising avenue for pursuing inference efficiency, sometimes considered indispensable in applications such as private inference. Despite achieving substantial speedups with minimal impact on utility, even appearing sound and practical for real-world deployment, the safety implications of activation approximations remain unclear. In this work, we fill this critical gap in LLM safety by conducting the first systematic safety evaluation of activation approximations. Our safety vetting spans seven state-of-the-art techniques across three popular categories (activation polynomialization, activation sparsification, and activation quantization), revealing consistent safety degradation across ten safety-aligned LLMs. To overcome the hurdle of devising a unified defense accounting for diverse activation approximation methods, we perform an in-depth analysis of their shared error patterns and uncover three key findings. We propose QuadA, a novel safety enhancement method tailored to mitigate the safety compromises introduced by activation approximations. Extensive experiments and ablation studies corroborate QuadA's effectiveness in enhancing the safety capabilities of LLMs after activation approximations.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jiawen Zhang",
        "Kejia Chen",
        "Lipeng He",
        "Jian Lou",
        "Dan Li",
        "Zunlei Feng",
        "Mingli Song",
        "Jian Liu",
        "Kui Ren",
        "Xiaohu Yang"
      ],
      "url": "https://openalex.org/W4407124169",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774362",
      "fetched_at": "2026-01-09T13:53:39.372304",
      "classified_at": "2026-01-09T13:59:16.844527",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4407124169",
      "doi": "https://doi.org/10.48550/arxiv.2502.00840"
    },
    "seed_6c86bcd1": {
      "paper_id": "seed_6c86bcd1",
      "title": "Exposing the Guardrails: Reverse-Engineering and Jailbreaking Safety Filters in DALL\u00b7E Text-to-Image Pipelines",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-villa.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774366",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_f81baf3b": {
      "paper_id": "seed_f81baf3b",
      "title": "TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts",
      "abstract": "Machine learning is advancing rapidly, with applications bringing notable benefits, such as improvements in translation and code generation. Models like ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated into daily life. However, alongside these benefits, LLMs also introduce social risks. Malicious users can exploit LLMs by submitting harmful prompts, such as requesting instructions for illegal activities. To mitigate this, models often include a security mechanism that automatically rejects such harmful prompts. However, they can be bypassed through LLM jailbreaks. Current jailbreaks often require significant manual effort, high computational costs, or result in excessive model modifications that may degrade regular utility. We introduce TwinBreak, an innovative safety alignment removal method. Building on the idea that the safety mechanism operates like an embedded backdoor, TwinBreak identifies and prunes parameters responsible for this functionality. By focusing on the most relevant model layers, TwinBreak performs fine-grained analysis of parameters essential to model utility and safety. TwinBreak is the first method to analyze intermediate outputs from prompts with high structural and content similarity to isolate safety parameters. We present the TwinPrompt dataset containing 100 such twin prompts. Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success rates with minimal computational requirements across 16 LLMs from five vendors.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Krau\u00df, Torsten",
        "Dashtbani, Hamid",
        "Dmitrienko, Alexandra"
      ],
      "url": "https://openalex.org/W4417133177",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774370",
      "fetched_at": "2026-01-09T13:53:40.468249",
      "classified_at": "2026-01-09T13:59:18.899654",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4417133177",
      "doi": "https://doi.org/10.48550/arxiv.2506.07596"
    },
    "seed_5c2b6503": {
      "paper_id": "seed_5c2b6503",
      "title": "Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs",
      "abstract": "Natural language prompts serve as an essential interface between users and Large Language Models (LLMs) like GPT-3.5 and GPT-4, which are employed by ChatGPT to produce outputs across various tasks. However, prompts crafted with malicious intent, known as jailbreak prompts, can circumvent the restrictions of LLMs, posing a significant threat to systems integrated with these models. Despite their critical importance, there is a lack of systematic analysis and comprehensive understanding of jailbreak prompts. Our paper aims to address this gap by exploring key research questions to enhance the robustness of LLM systems: 1) What common patterns are present in jailbreak prompts? 2) How effectively can these prompts bypass the restrictions of LLMs? 3) With the evolution of LLMs, how does the effectiveness of jailbreak prompts change? To address our research questions, we embarked on an empirical study targeting the LLMs underpinning ChatGPT, one of today\u2019s most advanced chatbots. Our methodology involved categorizing 78 jailbreak prompts into 10 distinct patterns, further organized into three jailbreak strategy types, and examining their distribution.We assessed the effectiveness of these prompts on GPT-3.5 and GPT-4, using a set of 3,120 questions across 8 scenarios deemed prohibited by OpenAI. Additionally, our study tracked the performance of these prompts over a 3-month period, observing the evolutionary response of ChatGPT to such inputs. Our findings offer a comprehensive view of jailbreak prompts, elucidating their taxonomy, effectiveness, and temporal dynamics. Notably, we discovered that GPT-3.5 and GPT-4 could still generate inappropriate content in response to malicious prompts without the need for jailbreaking. This underscores the critical need for effective prompt management within LLM systems and provides valuable insights and data to spur further research in LLM testing and jailbreak prevention.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Yi Liu",
        "Gelei Deng",
        "Zhengzi Xu",
        "Yuekang Li",
        "Yaowen Zheng",
        "Ying Zhang",
        "Lida Zhao",
        "Tianwei Zhang",
        "Kailong Wang"
      ],
      "url": "https://openalex.org/W4400484590",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774372",
      "fetched_at": "2026-01-09T13:53:41.231265",
      "classified_at": "2026-01-09T13:59:20.782348",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4400484590",
      "doi": "https://doi.org/10.1145/3663530.3665021"
    },
    "seed_fa39728a": {
      "paper_id": "seed_fa39728a",
      "title": "PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs",
      "abstract": "Large Language Models (LLMs) have excelled in various tasks but are still vulnerable to jailbreaking attacks, where attackers create jailbreak prompts to mislead the model to produce harmful or offensive content. Current jailbreak methods either rely heavily on manually crafted templates, which pose challenges in scalability and adaptability, or struggle to generate semantically coherent prompts, making them easy to detect. Additionally, most existing approaches involve lengthy prompts, leading to higher query costs. In this paper, to remedy these challenges, we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework that adapts the black-box fuzz testing approach with a series of customized designs. Instead of relying on manually crafted templates,PAPILLON starts with an empty seed pool, removing the need to search for any related jailbreaking templates. We also develop three novel question-dependent mutation strategies using an LLM helper to generate prompts that maintain semantic coherence while significantly reducing their length. Additionally, we implement a two-level judge module to accurately detect genuine successful jailbreaks. We evaluated PAPILLON on 7 representative LLMs and compared it with 5 state-of-the-art jailbreaking attack strategies. For proprietary LLM APIs, such as GPT-3.5 turbo, GPT-4, and Gemini-Pro, PAPILLONs achieves attack success rates of over 90%, 80%, and 74%, respectively, exceeding existing baselines by more than 60\\%. Additionally, PAPILLON can maintain high semantic coherence while significantly reducing the length of jailbreak prompts. When targeting GPT-4, PAPILLON can achieve over 78% attack success rate even with 100 tokens. Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses. Code: https://github.com/aaFrostnova/Papillon",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Xueluan Gong",
        "Mingzhe Li",
        "Yilin Zhang",
        "Fengyuan Ran",
        "Chen Chen",
        "Yanjiao Chen",
        "Qian Wang",
        "Kwok-Yan Lam"
      ],
      "url": "https://openalex.org/W4403780392",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774375",
      "fetched_at": "2026-01-09T13:53:42.083844",
      "classified_at": "2026-01-09T13:59:22.654792",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4403780392",
      "doi": "https://doi.org/10.48550/arxiv.2409.14866"
    },
    "seed_6c2d273d": {
      "paper_id": "seed_6c2d273d",
      "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
      "abstract": "Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we present Crescendomation, a tool that automates the Crescendo attack and demonstrate its efficacy against state-of-the-art models through our evaluations. Crescendomation surpasses other state-of-the-art jailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher performance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate Crescendo's ability to jailbreak multimodal models.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Mark Russinovich",
        "Ahmed Salem",
        "Ronen Eldan"
      ],
      "url": "https://openalex.org/W4393932428",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774377",
      "fetched_at": "2026-01-09T13:53:42.867313",
      "classified_at": "2026-01-09T13:59:24.498799",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4393932428",
      "doi": "https://doi.org/10.48550/arxiv.2404.01833"
    },
    "seed_1dff79a0": {
      "paper_id": "seed_1dff79a0",
      "title": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner",
      "abstract": "Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs) and has evolved into multiple categories: human-based, optimization-based, generation-based, and the recent indirect and multilingual jailbreaks. However, delivering a practical jailbreak defense is challenging because it needs to not only handle all the above jailbreak attacks but also incur negligible delays to user prompts, as well as be compatible with both open-source and closed-source LLMs. Inspired by how the traditional security concept of shadow stacks defends against memory overflow attacks, this paper introduces a generic LLM jailbreak defense framework called SelfDefend, which establishes a shadow LLM as a defense instance (in detection state) to concurrently protect the target LLM instance (in normal answering state) in the normal stack and collaborate with it for checkpoint-based access control. The effectiveness of SelfDefend builds upon our observation that existing LLMs can identify harmful prompts or intentions in user queries, which we empirically validate using mainstream GPT-3.5/4 models against major jailbreak attacks. To further improve the defense's robustness and minimize costs, we employ a data distillation approach to tune dedicated open-source defense models. When deployed to protect GPT-3.5/4, Claude, Llama-2-7b/13b, and Mistral, these models outperform seven state-of-the-art defenses and match the performance of GPT-4-based SelfDefend, with significantly lower extra delays. Further experiments show that the tuned models are robust to adaptive jailbreaks and prompt injections.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "X Wang",
        "Daoyuan Wu",
        "Zhenlan Ji",
        "Zongjie Li",
        "Pingchuan Ma",
        "Shuai Wang",
        "Yingjiu Li",
        "Yang Liu",
        "Ning Liu",
        "Juergen Rahmel"
      ],
      "url": "https://openalex.org/W4399554837",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774380",
      "fetched_at": "2026-01-09T13:53:43.381114",
      "classified_at": "2026-01-09T13:59:26.365343",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4399554837",
      "doi": "https://doi.org/10.48550/arxiv.2406.05498"
    },
    "seed_ccccfa59": {
      "paper_id": "seed_ccccfa59",
      "title": "JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation",
      "abstract": "Despite the implementation of safety alignment strategies, large language models (LLMs) remain vulnerable to jailbreak attacks, which undermine these safety guardrails and pose significant security threats. Some defenses have been proposed to detect or mitigate jailbreaks, but they are unable to withstand the test of time due to an insufficient understanding of jailbreak mechanisms. In this work, we investigate the mechanisms behind jailbreaks based on the Linear Representation Hypothesis (LRH), which states that neural networks encode high-level concepts as subspaces in their hidden representations. We define the toxic semantics in harmful and jailbreak prompts as toxic concepts and describe the semantics in jailbreak prompts that manipulate LLMs to comply with unsafe requests as jailbreak concepts. Through concept extraction and analysis, we reveal that LLMs can recognize the toxic concepts in both harmful and jailbreak prompts. However, unlike harmful prompts, jailbreak prompts activate the jailbreak concepts and alter the LLM output from rejection to compliance. Building on our analysis, we propose a comprehensive jailbreak defense framework, JBShield, consisting of two key components: jailbreak detection JBShield-D and mitigation JBShield-M. JBShield-D identifies jailbreak prompts by determining whether the input activates both toxic and jailbreak concepts. When a jailbreak prompt is detected, JBShield-M adjusts the hidden representations of the target LLM by enhancing the toxic concept and weakening the jailbreak concept, ensuring LLMs produce safe content. Extensive experiments demonstrate the superior performance of JBShield, achieving an average detection accuracy of 0.95 and reducing the average attack success rate of various jailbreak attacks to 2% from 61% across distinct LLMs.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Shenyi Zhang",
        "Yougang Zhai",
        "Keyan Guo",
        "Hongxin Hu",
        "Shengnan Guo",
        "Fang Zheng",
        "Lingchen Zhao",
        "Chao Shen",
        "Cong Wang",
        "Qian Wang"
      ],
      "url": "https://openalex.org/W4407425033",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774383",
      "fetched_at": "2026-01-09T13:53:44.651741",
      "classified_at": "2026-01-09T13:59:28.270846",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4407425033",
      "doi": "https://doi.org/10.48550/arxiv.2502.07557"
    },
    "2311.17400": {
      "paper_id": "2311.17400",
      "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
      "abstract": "Transformer-based models, such as BERT and GPT, have been widely adopted in natural language processing (NLP) due to their exceptional performance. However, recent studies show their vulnerability to textual adversarial attacks where the model's output can be misled by intentionally manipulating the text inputs. Despite various methods that have been proposed to enhance the model's robustness and mitigate this vulnerability, many require heavy consumption resources (e.g., adversarial training) or only provide limited protection (e.g., defensive dropout). In this paper, we propose a novel method called dynamic attention, tailored for the transformer architecture, to enhance the inherent robustness of the model itself against various adversarial attacks. Our method requires no downstream task knowledge and does not incur additional costs. The proposed dynamic attention consists of two modules: (I) attention rectification, which masks or weakens the attention value of the chosen tokens, and (ii) dynamic modeling, which dynamically builds the set of candidate tokens. Extensive experiments demonstrate that dynamic attention significantly mitigates the impact of adversarial attacks, improving up to 33\\% better performance than previous methods against widely-used adversarial attacks. The model-level design of dynamic attention enables it to be easily combined with other defense methods (e.g., adversarial training) to further enhance the model's robustness. Furthermore, we demonstrate that dynamic attention preserves the state-of-the-art robustness space of the original model compared to other dynamic modeling methods.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Lujia Shen",
        "Yuwen Pu",
        "Shouling Ji",
        "Changjiang Li",
        "Xuhong Zhang",
        "Chunpeng Ge",
        "Ting Wang"
      ],
      "url": "https://arxiv.org/abs/2311.17400",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774385",
      "fetched_at": "2026-01-09T12:14:35.774386",
      "classified_at": "2026-01-09T13:24:51.155616",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2311.05019": {
      "paper_id": "2311.05019",
      "title": "DEMASQ: Unmasking the ChatGPT Wordsmith",
      "abstract": "The potential misuse of ChatGPT and other Large Language Models (LLMs) has raised concerns regarding the dissemination of false information, plagiarism, academic dishonesty, and fraudulent activities. Consequently, distinguishing between AI-generated and human-generated content has emerged as an intriguing research topic. However, current text detection methods lack precision and are often restricted to specific tasks or domains, making them inadequate for identifying content generated by ChatGPT. In this paper, we propose an effective ChatGPT detector named DEMASQ, which accurately identifies ChatGPT-generated content. Our method addresses two critical factors: (i) the distinct biases in text composition observed in human- and machine-generated content and (ii) the alterations made by humans to evade previous detection methods. DEMASQ is an energy-based detection model that incorporates novel aspects, such as (i) optimization inspired by the Doppler effect to capture the interdependence between input text embeddings and output labels, and (ii) the use of explainable AI techniques to generate diverse perturbations. To evaluate our detector, we create a benchmark dataset comprising a mixture of prompts from both ChatGPT and humans, encompassing domains such as medical, open Q&A, finance, wiki, and Reddit. Our evaluation demonstrates that DEMASQ achieves high accuracy in identifying content generated by ChatGPT.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Kavita Kumari",
        "Alessandro Pegoraro",
        "Hossein Fereidooni",
        "Ahmad-Reza Sadeghi"
      ],
      "url": "https://arxiv.org/abs/2311.05019",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774389",
      "fetched_at": "2026-01-09T12:14:35.774390",
      "classified_at": "2026-01-09T13:24:51.855790",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2402.03214": {
      "paper_id": "2402.03214",
      "title": "Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?",
      "abstract": "The advent of generative AI images has completely disrupted the art world. Distinguishing AI generated images from human art is a challenging problem whose impact is growing over time. A failure to address this problem allows bad actors to defraud individuals paying a premium for human art and companies whose stated policies forbid AI imagery. It is also critical for content owners to establish copyright, and for model trainers interested in curating training data in order to avoid potential model collapse.   There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Anna Yoo Jeong Ha",
        "Josephine Passananti",
        "Ronik Bhaskar",
        "Shawn Shan",
        "Reid Southen",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "url": "https://arxiv.org/abs/2402.03214",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774393",
      "fetched_at": "2026-01-09T12:14:35.774394",
      "classified_at": "2026-01-09T13:24:52.543541",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2306.05524": {
      "paper_id": "2306.05524",
      "title": "On the Detectability of ChatGPT Content: Benchmarking, Methodology, and Evaluation through the Lens of Academic Writing",
      "abstract": "With ChatGPT under the spotlight, utilizing large language models (LLMs) to assist academic writing has drawn a significant amount of debate in the community. In this paper, we aim to present a comprehensive study of the detectability of ChatGPT-generated content within the academic literature, particularly focusing on the abstracts of scientific papers, to offer holistic support for the future development of LLM applications and policies in academia. Specifically, we first present GPABench2, a benchmarking dataset of over 2.8 million comparative samples of human-written, GPT-written, GPT-completed, and GPT-polished abstracts of scientific writing in computer science, physics, and humanities and social sciences. Second, we explore the methodology for detecting ChatGPT content. We start by examining the unsatisfactory performance of existing ChatGPT detecting tools and the challenges faced by human evaluators (including more than 240 researchers or students). We then test the hand-crafted linguistic features models as a baseline and develop a deep neural framework named CheckGPT to better capture the subtle and deep semantic and linguistic patterns in ChatGPT written literature. Last, we conduct comprehensive experiments to validate the proposed CheckGPT framework in each benchmarking task over different disciplines. To evaluate the detectability of ChatGPT content, we conduct extensive experiments on the transferability, prompt engineering, and robustness of CheckGPT.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Zeyan Liu",
        "Zijun Yao",
        "Fengjun Li",
        "Bo Luo"
      ],
      "url": "https://arxiv.org/abs/2306.05524",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774397",
      "fetched_at": "2026-01-09T12:14:35.774398",
      "classified_at": "2026-01-09T13:59:30.761510",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_e5270083": {
      "paper_id": "seed_e5270083",
      "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors",
      "abstract": "In this paper, we introduce GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the undifferentiable computation problem, caused by the discrete nature of text, by introducing a novel approach to construct weighted embeddings for the detector input. It then updates the evader model parameters using feedback from victim detectors, achieving high attack success with minimal text modification. To address the issue of tokenizer mismatch between the evader and the detector, we introduce a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture. Moreover, we employ novel tokenizer inference and model extraction techniques, facilitating effective evasion even in query-only access. We evaluate GradEscape on four datasets and three widely-used language models, benchmarking it against four state-of-the-art AIGT evaders. Experimental results demonstrate that GradEscape outperforms existing evaders in various scenarios, including with an 11B paraphrase model, while utilizing only 139M parameters. We have successfully applied GradEscape to two real-world commercial AIGT detectors. Our analysis reveals that the primary vulnerability stems from disparity in text expression styles within the training data. We also propose a potential defense strategy to mitigate the threat of AIGT evaders. We open-source our GradEscape for developing more robust AIGT detectors.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Wenlong Meng",
        "Shao\u2010Hua Fan",
        "Chengkun Wei",
        null,
        "Yuwei Li",
        "Yuanchao Zhang",
        "Zhikun Zhang",
        "Wenzhi Chen"
      ],
      "url": "https://openalex.org/W4417255476",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774401",
      "fetched_at": "2026-01-09T13:53:45.152826",
      "classified_at": "2026-01-09T13:59:32.603375",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4417255476",
      "doi": "https://doi.org/10.48550/arxiv.2506.08188"
    },
    "seed_73a2c708": {
      "paper_id": "seed_73a2c708",
      "title": "Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI",
      "abstract": "Generative AI technology has become increasingly integrated into our daily lives, offering powerful capabilities to enhance productivity. However, these same capabilities can be exploited by adversaries for malicious purposes. While existing research on adversarial applications of generative AI predominantly focuses on cyberattacks, less attention has been given to attacks targeting deep learning models. In this paper, we introduce the use of generative AI for facilitating model-related attacks, including model extraction, membership inference, and model inversion. Our study reveals that adversaries can launch a variety of model-related attacks against both image and text models in a data-free and black-box manner, achieving comparable performance to baseline methods that have access to the target models' training data and parameters in a white-box manner. This research serves as an important early warning to the community about the potential risks associated with generative AI-powered attacks on deep learning models.",
      "year": 2025,
      "venue": "CISPA Helmholtz Center",
      "authors": [
        "Ye, Dayong",
        "Zhu, Tianqing",
        "Wang Shang",
        "Liu Bo",
        "Yu Zhang, Leo",
        "Zhou, Wanlei",
        "Zhang Yang"
      ],
      "url": "https://openalex.org/W7104672862",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774404",
      "fetched_at": "2026-01-09T13:53:45.679625",
      "classified_at": "2026-01-09T13:59:34.451142",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W7104672862",
      "doi": "https://doi.org/10.60882/cispa.30581237.v1"
    },
    "seed_d2ccbbdf": {
      "paper_id": "seed_d2ccbbdf",
      "title": "\"I Cannot Write This Because It Violates Our Content Policy\": Understanding Content Moderation Policies and User Experiences in Generative AI Products",
      "abstract": "While recent research has focused on developing safeguards for generative AI (GAI) model-level content safety, little is known about how content moderation to prevent malicious content performs for end-users in real-world GAI products. To bridge this gap, we investigated content moderation policies and their enforcement in GAI online tools -- consumer-facing web-based GAI applications. We first analyzed content moderation policies of 14 GAI online tools. While these policies are comprehensive in outlining moderation practices, they usually lack details on practical implementations and are not specific about how users can aid in moderation or appeal moderation decisions. Next, we examined user-experienced content moderation successes and failures through Reddit discussions on GAI online tools. We found that although moderation systems succeeded in blocking malicious generations pervasively, users frequently experienced frustration in failures of both moderation systems and user support after moderation. Based on these findings, we suggest improvements for content moderation policy and user experiences in real-world GAI products.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Lan Gao",
        "Oscar Chen",
        "Rachel Lee",
        "Nick Feamster",
        "Chenhao Tan",
        "Marshini Chetty"
      ],
      "url": "https://openalex.org/W4415311374",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774407",
      "fetched_at": "2026-01-09T13:53:46.276592",
      "classified_at": "2026-01-09T13:59:36.835788",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4415311374",
      "doi": "https://doi.org/10.48550/arxiv.2506.14018"
    },
    "seed_88d094a2": {
      "paper_id": "seed_88d094a2",
      "title": "Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data",
      "abstract": "Large language models (LLMs) have demonstrated significant success in various domain-specific tasks, with their performance often improving substantially after fine-tuning. However, fine-tuning with real-world data introduces privacy risks. To mitigate these risks, developers increasingly rely on synthetic data generation as an alternative to using real data, as data generated by traditional models is believed to be different from real-world data. However, with the advanced capabilities of LLMs, the distinction between real data and data generated by these models has become nearly indistinguishable. This convergence introduces similar privacy risks for generated data to those associated with real data. Our study investigates whether fine-tuning with LLM-generated data truly enhances privacy or introduces additional privacy risks by examining the structural characteristics of data generated by LLMs, focusing on two primary fine-tuning approaches: supervised fine-tuning (SFT) with unstructured (plain-text) generated data and self-instruct tuning. In the scenario of SFT, the data is put into a particular instruction tuning format used by previous studies. We use Personal Information Identifier (PII) leakage and Membership Inference Attacks (MIAs) on the Pythia Model Suite and Open Pre-trained Transformer (OPT) to measure privacy risks. Notably, after fine-tuning with unstructured generated data, the rate of successful PII extractions for Pythia increased by over 20%, highlighting the potential privacy implications of such approaches. Furthermore, the ROC-AUC score of MIAs for Pythia-6.9b, the second biggest model of the suite, increases over 40% after self-instruct tuning. Our results indicate the potential privacy risks associated with fine-tuning LLMs using generated data, underscoring the need for careful consideration of privacy safeguards in such approaches.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Atilla Akkus",
        "Mingjie Li",
        "Junjie Chu",
        "Michael Backes",
        "Yang Zhang",
        "Sinem Sav"
      ],
      "url": "https://openalex.org/W4403709727",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774411",
      "fetched_at": "2026-01-09T13:53:46.802914",
      "classified_at": "2026-01-09T13:59:38.682020",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4403709727",
      "doi": "https://doi.org/10.48550/arxiv.2409.11423"
    },
    "2308.13904": {
      "paper_id": "2308.13904",
      "title": "LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors",
      "abstract": "Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inverting the triggers, LMSanitator aims to invert the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prompt-tuning's property of freezing the pretrained model to perform accurate and fast output monitoring and input purging during the inference phase. Extensive experiments on multiple language models and NLP tasks illustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves 92.8% backdoor detection accuracy on 960 models and decreases the attack success rate to less than 1% in most scenarios.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Chengkun Wei",
        "Wenlong Meng",
        "Zhikun Zhang",
        "Min Chen",
        "Minghu Zhao",
        "Wenjing Fang",
        "Lei Wang",
        "Zihui Zhang",
        "Wenzhi Chen"
      ],
      "url": "https://arxiv.org/abs/2308.13904",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774414",
      "fetched_at": "2026-01-09T12:14:35.774415",
      "classified_at": "2026-01-09T13:25:53.962399",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_efec62bb": {
      "paper_id": "seed_efec62bb",
      "title": "EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-yan-nan.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774417",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_47c10bec": {
      "paper_id": "seed_47c10bec",
      "title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs",
      "abstract": "Recent advancements in Large Language Models (LLMs) have established them as agentic systems capable of planning and interacting with various tools. These LLM agents are often paired with web-based tools, enabling access to diverse sources and real-time information. Although these advancements offer significant benefits across various applications, they also increase the risk of malicious use, particularly in cyberattacks involving personal information. In this work, we investigate the risks associated with misuse of LLM agents in cyberattacks involving personal data. Specifically, we aim to understand: 1) how potent LLM agents can be when directed to conduct cyberattacks, 2) how cyberattacks are enhanced by web-based tools, and 3) how affordable and easy it becomes to launch cyberattacks using LLM agents. We examine three attack scenarios: the collection of Personally Identifiable Information (PII), the generation of impersonation posts, and the creation of spear-phishing emails. Our experiments reveal the effectiveness of LLM agents in these attacks: LLM agents achieved a precision of up to 95.9% in collecting PII, generated impersonation posts where 93.9% of them were deemed authentic, and boosted click rate of phishing links in spear phishing emails by 46.67%. Additionally, our findings underscore the limitations of existing safeguards in contemporary commercial LLMs, emphasizing the urgent need for robust security measures to prevent the misuse of LLM agents.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Hanna Kim",
        "Minkyoo Song",
        "Seung Ho Na",
        "Seungwon Shin",
        "K.-H Lee"
      ],
      "url": "https://openalex.org/W4403996051",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774420",
      "fetched_at": "2026-01-09T13:53:47.876181",
      "classified_at": "2026-01-09T13:59:40.495117",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4403996051",
      "doi": "https://doi.org/10.48550/arxiv.2410.14569"
    },
    "seed_156f5616": {
      "paper_id": "seed_156f5616",
      "title": "Make Agent Defeat Agent: Automatic Detection of Taint-Style Vulnerabilities in LLM-based Agents",
      "abstract": "Language Models (LMs) are becoming increasingly popular in real-world applications. Outsourcing model training and data hosting to third-party platforms has become a standard method for reducing costs. In such a situation, the attacker can manipulate the training process or data to inject a backdoor into models. Backdoor attacks are a serious threat where malicious behavior is activated when triggers are present, otherwise, the model operates normally. However, there is still no systematic and comprehensive review of LMs from the attacker's capabilities and purposes on different backdoor attack surfaces. Moreover, there is a shortage of analysis and comparison of the diverse emerging backdoor countermeasures. Therefore, this work aims to provide the NLP community with a timely review of backdoor attacks and countermeasures. According to the attackers' capability and affected stage of the LMs, the attack surfaces are formalized into four categorizations: attacking the pre-trained model with fine-tuning (APMF) or parameter-efficient fine-tuning (APMP), attacking the final model with training (AFMT), and attacking Large Language Models (ALLM). Thus, attacks under each categorization are combed. The countermeasures are categorized into two general classes: sample inspection and model inspection. Thus, we review countermeasures and analyze their advantages and disadvantages. Also, we summarize the benchmark datasets and provide comparable evaluations for representative attacks and defenses. Drawing the insights from the review, we point out the crucial areas for future research on the backdoor, especially soliciting more efficient and practical countermeasures.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Pengzhou Cheng",
        "Zongru Wu",
        "Wei Du",
        "Gongshen Liu"
      ],
      "url": "https://openalex.org/W4386754881",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774423",
      "fetched_at": "2026-01-09T13:53:48.412255",
      "classified_at": "2026-01-09T13:59:42.560333",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4386754881",
      "doi": "https://doi.org/10.48550/arxiv.2309.06055"
    },
    "seed_4c582dd0": {
      "paper_id": "seed_4c582dd0",
      "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs",
      "abstract": "Long context large language models (LLMs) are deployed in many real-world applications such as RAG, agent, and broad LLM-integrated applications. Given an instruction and a long context (e.g., documents, PDF files, webpages), a long context LLM can generate an output grounded in the provided context, aiming to provide more accurate, up-to-date, and verifiable outputs while reducing hallucinations and unsupported claims. This raises a research question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs) in the context that contribute most to or are responsible for the generated output by an LLM? This process, which we call context traceback, has various real-world applications, such as 1) debugging LLM-based systems, 2) conducting post-attack forensic analysis for attacks (e.g., prompt injection attack, knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources to enhance the trust of users towards outputs generated by LLMs. When applied to context traceback for long context LLMs, existing feature attribution methods such as Shapley have sub-optimal performance and/or incur a large computational cost. In this work, we develop TracLLM, the first generic context traceback framework tailored to long context LLMs. Our framework can improve the effectiveness and efficiency of existing feature attribution methods. To improve the efficiency, we develop an informed search based algorithm in TracLLM. We also develop contribution score ensemble/denoising techniques to improve the accuracy of TracLLM. Our evaluation results show TracLLM can effectively identify texts in a long context that lead to the output of an LLM. Our code and data are at: https://github.com/Wang-Yanting/TracLLM.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yanting Wang",
        "Wei Zou",
        "Rongli Geng",
        "Jinyuan Jia"
      ],
      "url": "https://openalex.org/W4416075237",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774425",
      "fetched_at": "2026-01-09T13:53:48.902248",
      "classified_at": "2026-01-09T13:59:44.479337",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4416075237",
      "doi": "https://doi.org/10.48550/arxiv.2506.04202"
    },
    "seed_769bf67c": {
      "paper_id": "seed_769bf67c",
      "title": "Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search",
      "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of AI-Powered Search Engines (AIPSEs), offering precise and efficient responses by integrating external databases with pre-existing knowledge. However, we observe that these AIPSEs raise risks such as quoting malicious content or citing malicious websites, leading to harmful or unverified information dissemination. In this study, we conduct the first safety risk quantification on seven production AIPSEs by systematically defining the threat model, risk type, and evaluating responses to various query types. With data collected from PhishTank, ThreatBook, and LevelBlue, our findings reveal that AIPSEs frequently generate harmful content that contains malicious URLs even with benign queries (e.g., with benign keywords). We also observe that directly querying a URL will increase the number of main risk-inclusive responses, while querying with natural language will slightly mitigate such risk. Compared to traditional search engines, AIPSEs outperform in both utility and safety. We further perform two case studies on online document spoofing and phishing to show the ease of deceiving AIPSEs in the real-world setting. To mitigate these risks, we develop an agent-based defense with a GPT-4.1-based content refinement tool and a URL detector. Our evaluation shows that our defense can effectively reduce the risk, with only a minor cost of reducing available information by approximately 10.7%. Our research highlights the urgent need for robust safety measures in AIPSEs.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Z. Luo",
        "Zhaoyun Peng",
        "Yule Liu",
        "Zhen Sun",
        "M Li",
        "Jingyi Zheng",
        "Xinlei He"
      ],
      "url": "https://openalex.org/W4407310131",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774428",
      "fetched_at": "2026-01-09T13:53:49.427874",
      "classified_at": "2026-01-09T13:59:46.490604",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4407310131",
      "doi": "https://doi.org/10.48550/arxiv.2502.04951"
    },
    "seed_5f7dbceb": {
      "paper_id": "seed_5f7dbceb",
      "title": "Cloak, Honey, Trap: Proactive Defenses Against LLM Agents",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-ayzenshteyn.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774432",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_c9e495b7": {
      "paper_id": "seed_c9e495b7",
      "title": "Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants",
      "abstract": "Generative AI (GenAI) browser assistants integrate powerful capabilities of GenAI in web browsers to provide rich experiences such as question answering, content summarization, and agentic navigation. These assistants, available today as browser extensions, can not only track detailed browsing activity such as search and click data, but can also autonomously perform tasks such as filling forms, raising significant privacy concerns. It is crucial to understand the design and operation of GenAI browser extensions, including how they collect, store, process, and share user data. To this end, we study their ability to profile users and personalize their responses based on explicit or inferred demographic attributes and interests of users. We perform network traffic analysis and use a novel prompting framework to audit tracking, profiling, and personalization by the ten most popular GenAI browser assistant extensions. We find that instead of relying on local in-browser models, these assistants largely depend on server-side APIs, which can be auto-invoked without explicit user interaction. When invoked, they collect and share webpage content, often the full HTML DOM and sometimes even the user's form inputs, with their first-party servers. Some assistants also share identifiers and user prompts with third-party trackers such as Google Analytics. The collection and sharing continues even if a webpage contains sensitive information such as health or personal information such as name or SSN entered in a web form. We find that several GenAI browser assistants infer demographic attributes such as age, gender, income, and interests and use this profile--which carries across browsing contexts--to personalize responses. In summary, our work shows that GenAI browser assistants can and do collect personal and sensitive information for profiling and personalization with little to no safeguards.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Aurelio Loris Canino",
        "Alex Ciechonski",
        "Patricia Callejo",
        "Zubair Shafiq"
      ],
      "url": "https://openalex.org/W4416455452",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774435",
      "fetched_at": "2026-01-09T13:53:51.220050",
      "classified_at": "2026-01-09T13:59:48.482304",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4416455452",
      "doi": "https://doi.org/10.48550/arxiv.2503.16586"
    },
    "2509.07764": {
      "paper_id": "2509.07764",
      "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents",
      "abstract": "Large Language Models (LLMs) have been increasingly integrated into computer-use agents, which can autonomously operate tools on a user's computer to accomplish complex tasks. However, due to the inherently unstable and unpredictable nature of LLM outputs, they may issue unintended tool commands or incorrect inputs, leading to potentially harmful operations. Unlike traditional security risks stemming from insecure user prompts, tool execution results from LLM-driven decisions introduce new and unique security challenges. These vulnerabilities span across all components of a computer-use agent. To mitigate these risks, we propose AgentSentinel, an end-to-end, real-time defense framework designed to mitigate potential security threats on a user's computer. AgentSentinel intercepts all sensitive operations within agent-related services and halts execution until a comprehensive security audit is completed. Our security auditing mechanism introduces a novel inspection process that correlates the current task context with system traces generated during task execution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a benchmark consisting of 60 diverse attack scenarios across six attack categories. The benchmark demonstrates a 87% average attack success rate on four state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an average defense success rate of 79.6%, significantly outperforming all baseline defenses.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Haitao Hu",
        "Peng Chen",
        "Yanpeng Zhao",
        "Yuqi Chen"
      ],
      "url": "https://arxiv.org/abs/2509.07764",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774438",
      "fetched_at": "2026-01-09T12:14:35.774439",
      "classified_at": "2026-01-09T13:25:54.587058",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_ad39c3c9": {
      "paper_id": "seed_ad39c3c9",
      "title": "StruQ: Defending Against Prompt Injection with Structured Queries",
      "abstract": "Recent advances in Large Language Models (LLMs) enable exciting LLM-integrated applications, which perform text-based tasks by utilizing their advanced language understanding capabilities. However, as LLMs have improved, so have the attacks against them. Prompt injection attacks are an important threat: they trick the model into deviating from the original application's instructions and instead follow user directives. These attacks rely on the LLM's ability to follow instructions and inability to separate prompts and user data. We introduce structured queries, a general approach to tackle this problem. Structured queries separate prompts and data into two channels. We implement a system that supports structured queries. This system is made of (1) a secure front-end that formats a prompt and user data into a special format, and (2) a specially trained LLM that can produce high-quality outputs from these inputs. The LLM is trained using a novel fine-tuning strategy: we convert a base (non-instruction-tuned) LLM to a structured instruction-tuned model that will only follow instructions in the prompt portion of a query. To do so, we augment standard instruction tuning datasets with examples that also include instructions in the data portion of the query, and fine-tune the model to ignore these. Our system significantly improves resistance to prompt injection attacks, with little or no impact on utility. Our code is released at https://github.com/Sizhe-Chen/StruQ.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Sizhe Chen",
        "Julien Piet",
        "Chawin Sitawarin",
        "David Wagner"
      ],
      "url": "https://openalex.org/W4391766701",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774441",
      "fetched_at": "2026-01-09T13:53:51.738032",
      "classified_at": "2026-01-09T13:59:50.415364",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391766701",
      "doi": "https://doi.org/10.48550/arxiv.2402.06363"
    },
    "seed_fd1628f4": {
      "paper_id": "seed_fd1628f4",
      "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents",
      "abstract": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevant documents from a knowledge database and applying an LLM to the retrieved documents. We demonstrate that RAG systems that operate on databases with untrusted content are vulnerable to denial-of-service attacks we call jamming. An adversary can add a single ``blocker'' document to the database that will be retrieved in response to a specific query and result in the RAG system not answering this query, ostensibly because it lacks relevant information or because the answer is unsafe. We describe and measure the efficacy of several methods for generating blocker documents, including a new method based on black-box optimization. Our method (1) does not rely on instruction injection, (2) does not require the adversary to know the embedding or LLM used by the target RAG system, and (3) does not employ an auxiliary LLM. We evaluate jamming attacks on several embeddings and LLMs and demonstrate that the existing safety metrics for LLMs do not capture their vulnerability to jamming. We then discuss defenses against blocker documents.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Avital Shafran",
        "Roei Schuster",
        "Vitaly Shmatikov"
      ],
      "url": "https://openalex.org/W4399553840",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774444",
      "fetched_at": "2026-01-09T13:53:52.296562",
      "classified_at": "2026-01-09T13:59:52.440811",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4399553840",
      "doi": "https://doi.org/10.48550/arxiv.2406.05870"
    },
    "2410.05451": {
      "paper_id": "2410.05451",
      "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
      "abstract": "Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be injected into external data sources to override the system's intended instruction and instead execute a malicious instruction. To mitigate this vulnerability, we propose a new defense called SecAlign based on the technique of preference optimization. Our defense first constructs a preference dataset with prompt-injected inputs, secure outputs (ones that respond to the legitimate instruction), and insecure outputs (ones that respond to the injection). We then perform preference optimization on this dataset to teach the LLM to prefer the secure output over the insecure one. This provides the first known method that reduces the success rates of various prompt injections to <10%, even against attacks much more sophisticated than ones seen during training. This indicates our defense generalizes well against unknown and yet-to-come attacks. Also, SecAlign models are still practical with similar utility to the one before defensive training in our evaluations. Our code is at https://github.com/facebookresearch/SecAlign",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Sizhe Chen",
        "Arman Zharmagambetov",
        "Saeed Mahloujifar",
        "Kamalika Chaudhuri",
        "David Wagner",
        "Chuan Guo"
      ],
      "url": "https://arxiv.org/abs/2410.05451",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774447",
      "fetched_at": "2026-01-09T12:14:35.774448",
      "classified_at": "2026-01-09T13:25:55.252249",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_74186653": {
      "paper_id": "seed_74186653",
      "title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink",
      "abstract": "Fusing visual understanding into language generation, Multi-modal Large Language Models (MLLMs) are revolutionizing visual-language applications. Yet, these models are often plagued by the hallucination problem, which involves generating inaccurate objects, attributes, and relationships that do not match the visual content. In this work, we delve into the internal attention mechanisms of MLLMs to reveal the underlying causes of hallucination, exposing the inherent vulnerabilities in the instruction-tuning process. We propose a novel hallucination attack against MLLMs that exploits attention sink behaviors to trigger hallucinated content with minimal image-text relevance, posing a significant threat to critical downstream applications. Distinguished from previous adversarial methods that rely on fixed patterns, our approach generates dynamic, effective, and highly transferable visual adversarial inputs, without sacrificing the quality of model responses. Comprehensive experiments on 6 prominent MLLMs demonstrate the efficacy of our attack in compromising black-box MLLMs even with extensive mitigating mechanisms, as well as the promising results against cutting-edge commercial APIs, such as GPT-4o and Gemini 1.5. Our code is available at https://huggingface.co/RachelHGF/Mirage-in-the-Eyes.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yining Wang",
        "Mi Zhang",
        "Junjie Sun",
        "Chenyue Wang",
        "Min Yang",
        "Hui Xue",
        "Jingli Tao",
        "Ranjie Duan",
        "Jiexi Liu"
      ],
      "url": "https://openalex.org/W4406880263",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774450",
      "fetched_at": "2026-01-09T13:53:53.103289",
      "classified_at": "2026-01-09T13:59:54.292339",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4406880263",
      "doi": "https://doi.org/10.48550/arxiv.2501.15269"
    },
    "seed_bd37232d": {
      "paper_id": "seed_bd37232d",
      "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
      "abstract": "Machine learning (ML) has progressed rapidly during the past decade and the major factor that drives such development is the unprecedented large-scale data. As data generation is a continuous process, this leads to ML model owners updating their models frequently with newly-collected data in an online learning scenario. In consequence, if an ML model is queried with the same set of data samples at two different points in time, it will provide different results. In this paper, we investigate whether the change in the output of a black-box ML model before and after being updated can leak information of the dataset used to perform the update, namely the updating set. This constitutes a new attack surface against black-box ML models and such information leakage may compromise the intellectual property and data privacy of the ML model owner. We propose four attacks following an encoder-decoder formulation, which allows inferring diverse information of the updating set. Our new attacks are facilitated by state-of-the-art deep learning techniques. In particular, we propose a hybrid generative model ({\\UGAN}) that is based on generative adversarial networks (GANs) but includes a reconstructive loss that allows reconstructing accurate samples. Our experiments show that the proposed attacks achieve strong performance.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Ahmed Salem",
        "Apratim Bhattacharya",
        "Michael Backes",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W2926319231",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774453",
      "fetched_at": "2026-01-09T13:53:53.906521",
      "classified_at": "2026-01-09T13:59:56.128521",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2926319231",
      "doi": "https://doi.org/10.60882/cispa.24613164"
    },
    "seed_9ab242f3": {
      "paper_id": "seed_9ab242f3",
      "title": "Extracting Training Data from Large Language Models",
      "abstract": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Nicholas Carlini",
        "Florian Tram\u00e8r",
        "Eric Wallace",
        "Matthew Jagielski",
        "Ariel Herbert-Voss",
        "Katherine Lee",
        "Adam P. Roberts",
        "T. B. Brown",
        "Dawn Song",
        "\u00dalfar Erlingsson",
        "Alina Oprea",
        "Colin Raffel"
      ],
      "url": "https://openalex.org/W3112689365",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML05",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774456",
      "fetched_at": "2026-01-09T13:53:54.814057",
      "classified_at": "2026-01-09T13:59:57.992340",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3112689365",
      "doi": "https://doi.org/10.48550/arxiv.2012.07805"
    },
    "seed_bb7e186a": {
      "paper_id": "seed_bb7e186a",
      "title": "Analyzing Information Leakage of Updates to Natural Language Models",
      "abstract": "To continuously improve quality and reflect changes in data, machine learning\\napplications have to regularly retrain and update their core models. We show\\nthat a differential analysis of language model snapshots before and after an\\nupdate can reveal a surprising amount of detailed information about changes in\\nthe training data. We propose two new metrics---\\\\emph{differential score} and\\n\\\\emph{differential rank}---for analyzing the leakage due to updates of natural\\nlanguage models. We perform leakage analysis using these metrics across models\\ntrained on several different datasets using different methods and\\nconfigurations. We discuss the privacy implications of our findings, propose\\nmitigation strategies and evaluate their effect.\\n",
      "year": 2020,
      "venue": null,
      "authors": [
        "Santiago Zanella-B\u00e9guelin",
        "Lukas Wutschitz",
        "Shruti Tople",
        "Victor R\u00fchle",
        "Andrew Paverd",
        "Olga Ohrimenko",
        "Boris K\u00f6pf",
        "Marc Brockschmidt"
      ],
      "url": "https://openalex.org/W3027379683",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML05",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774458",
      "fetched_at": "2026-01-09T13:53:55.765619",
      "classified_at": "2026-01-09T14:00:00.080428",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3027379683",
      "doi": "https://doi.org/10.1145/3372297.3417880"
    },
    "2107.13190": {
      "paper_id": "2107.13190",
      "title": "TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized Tabular Data Releasing",
      "abstract": "Generative Adversarial Networks (GAN)-synthesized table publishing lets people privately learn insights without access to the private table. However, existing studies on Membership Inference (MI) Attacks show promising results on disclosing membership of training datasets of GAN-synthesized tables. Different from those works focusing on discovering membership of a given data point, in this paper, we propose a novel Membership Collision Attack against GANs (TableGAN-MCA), which allows an adversary given only synthetic entries randomly sampled from a black-box generator to recover partial GAN training data. Namely, a GAN-synthesized table immune to state-of-the-art MI attacks is vulnerable to the TableGAN-MCA. The success of TableGAN-MCA is boosted by an observation that GAN-synthesized tables potentially collide with the training data of the generator.   Our experimental evaluations on TableGAN-MCA have five main findings. First, TableGAN-MCA has a satisfying training data recovery rate on three commonly used real-world datasets against four generative models. Second, factors, including the size of GAN training data, GAN training epochs and the number of synthetic samples available to the adversary, are positively correlated to the success of TableGAN-MCA. Third, highly frequent data points have high risks of being recovered by TableGAN-MCA. Fourth, some unique data are exposed to unexpected high recovery risks in TableGAN-MCA, which may attribute to GAN's generalization. Fifth, as expected, differential privacy, without the consideration of the correlations between features, does not show commendable mitigation effect against the TableGAN-MCA. Finally, we propose two mitigation methods and show promising privacy and utility trade-offs when protecting against TableGAN-MCA.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Aoting Hu",
        "Renjie Xie",
        "Zhigang Lu",
        "Aiqun Hu",
        "Minhui Xue"
      ],
      "url": "https://arxiv.org/abs/2107.13190",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774462",
      "fetched_at": "2026-01-09T12:14:35.774463",
      "classified_at": "2026-01-09T13:25:55.970510",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2103.11109": {
      "paper_id": "2103.11109",
      "title": "DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation",
      "abstract": "Recent success of deep neural networks (DNNs) hinges on the availability of large-scale dataset; however, training on such dataset often poses privacy risks for sensitive training information. In this paper, we aim to explore the power of generative models and gradient sparsity, and propose a scalable privacy-preserving generative model DATALENS. Comparing with the standard PATE privacy-preserving framework which allows teachers to vote on one-dimensional predictions, voting on the high dimensional gradient vectors is challenging in terms of privacy preservation. As dimension reduction techniques are required, we need to navigate a delicate tradeoff space between (1) the improvement of privacy preservation and (2) the slowdown of SGD convergence. To tackle this, we take advantage of communication efficient learning and propose a novel noise compression and aggregation approach TOPAGG by combining top-k compression for dimension reduction with a corresponding noise injection mechanism. We theoretically prove that the DATALENS framework guarantees differential privacy for its generated data, and provide analysis on its convergence. To demonstrate the practical usage of DATALENS, we conduct extensive experiments on diverse datasets including MNIST, Fashion-MNIST, and high dimensional CelebA, and we show that, DATALENS significantly outperforms other baseline DP generative models. In addition, we adapt the proposed TOPAGG approach, which is one of the key building blocks in DATALENS, to DP SGD training, and show that it is able to achieve higher utility than the state-of-the-art DP SGD approach in most cases. Our code is publicly available at https://github.com/AI-secure/DataLens.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Boxin Wang",
        "Fan Wu",
        "Yunhui Long",
        "Luka Rimanic",
        "Ce Zhang",
        "Bo Li"
      ],
      "url": "https://arxiv.org/abs/2103.11109",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774466",
      "fetched_at": "2026-01-09T12:14:35.774467",
      "classified_at": "2026-01-09T13:25:56.580813",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_086ec9de": {
      "paper_id": "seed_086ec9de",
      "title": "Property Inference Attacks Against GANs",
      "abstract": "While machine learning (ML) has made tremendous progress during the past decade, recent research has shown that ML models are vulnerable to various security and privacy attacks. So far, most of the attacks in this field focus on discriminative models, represented by classifiers. Meanwhile, little attention has been paid to the security and privacy risks of generative models, such as generative adversarial networks (GANs). In this paper, we propose the first set of training dataset property inference attacks against GANs. Concretely, the adversary aims to infer the macro-level training dataset property, i.e., the proportion of samples used to train a target GAN with respect to a certain attribute. A successful property inference attack can allow the adversary to gain extra knowledge of the target GAN\u2019s training dataset, thereby directly violating the intellectual property of the target model owner. Also, it can be used as a fairness auditor to check whether the target GAN is trained with a biased dataset. Besides, property inference can serve as a building block for other advanced attacks, such as membership inference. We propose a general attack pipeline that can be tailored to two attack scenarios, including the full black-box setting and partial black-box setting. For the latter, we introduce a novel optimization framework to increase the attack efficacy. Extensive experiments over four representative GAN models on five property inference tasks show that our attacks achieve strong performance. In addition, we show that our attacks can be used to enhance the performance of membership inference against GANs.",
      "year": 2022,
      "venue": null,
      "authors": [
        "Junhao Zhou",
        "Yufei Chen",
        "Chao Shen",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W3213807399",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774470",
      "fetched_at": "2026-01-09T13:53:56.553494",
      "classified_at": "2026-01-09T14:00:02.122222",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3213807399",
      "doi": "https://doi.org/10.14722/ndss.2022.23019"
    },
    "seed_908a8f14": {
      "paper_id": "seed_908a8f14",
      "title": "MIRROR: Model Inversion for Deep Learning Network with High Fidelity",
      "abstract": "Model inversion reverse-engineers input samples from a given model, and hence poses serious threats to information confidentiality.We propose a novel inversion technique based on StyleGAN, whose generator has a special architecture that forces the decomposition of an input to styles of various granularities such that the model can learn them separately in training.During sample generation, the generator transforms a latent value to parameters controlling these styles to compose a sample.In our inversion, given a target label of some subject model to invert (e.g., a private face based identity recognition model), our technique leverages a StyleGAN trained on public data from the same domain (e.g., a public human face dataset), uses the gradient descent or genetic search algorithm, together with distribution based clipping, to find a proper parameterization of the styles such that the generated sample is correctly classified to the target label (by the subject model) and recognized by humans.The results show that our inverted samples have high fidelity, substantially better than those by existing state-of-the-art techniques.",
      "year": 2022,
      "venue": null,
      "authors": [
        "Shengwei An",
        "Guanhong Tao",
        "Qiuling Xu",
        "Yingqi Liu",
        "Guangyu Shen",
        "Yuan Yao",
        "Jingwei Xu",
        "Xiangyu Zhang"
      ],
      "url": "https://openalex.org/W4226117300",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774472",
      "fetched_at": "2026-01-09T13:53:57.336860",
      "classified_at": "2026-01-09T14:00:04.158558",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4226117300",
      "doi": "https://doi.org/10.14722/ndss.2022.24335"
    },
    "2302.00539": {
      "paper_id": "2302.00539",
      "title": "Analyzing Leakage of Personally Identifiable Information in Language Models",
      "abstract": "Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10$\\times$ more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing_pii_leakage.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Nils Lukas",
        "Ahmed Salem",
        "Robert Sim",
        "Shruti Tople",
        "Lukas Wutschitz",
        "Santiago Zanella-B\u00e9guelin"
      ],
      "url": "https://arxiv.org/abs/2302.00539",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML05",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774475",
      "fetched_at": "2026-01-09T12:14:35.774476",
      "classified_at": "2026-01-09T13:25:57.222908",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_974e8369": {
      "paper_id": "seed_974e8369",
      "title": "Timing Channels in Adaptive Neural Networks",
      "abstract": "Current machine learning systems offer great predictive power but also require significant computational resources.As a result, the promise of a class of optimized machine learning models, called adaptive neural networks (ADNNs), has seen recent wide appeal.These models make dynamic decisions about the amount of computation to perform based on the given input, allowing for fast predictions on \"easy\" input.While various considerations of ADNNs have been extensively researched, how these input-dependent optimizations might introduce vulnerabilities has been hitherto under-explored.Our work is the first to demonstrate and evaluate timing channels due to the optimizations of ADNNs with the capacity to leak sensitive attributes about a user's input.We empirically study six ADNNs types and demonstrate how an attacker can significantly improve their ability to infer sensitive attributes, such as class label, of another user's input from an observed timing measurement.Our results show that timing information can increase an attacker's probability of correctly inferring the attribute of the user's input by up to a factor of 9.89x.Our empirical evaluation uses four different datasets, including those containing sensitive medical and demographic information, and considers leakage across a variety of sensitive attributes of the user's input.We conclude by demonstrating how timing channels can be exploited across the public internet in two fictitious web applications -Fictitious Health Company and Fictitious HR -that make use of ADNNs for serving predictions to their clients.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Ayomide Akinsanya",
        "Tegan Brennan"
      ],
      "url": "https://openalex.org/W4391724747",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774479",
      "fetched_at": "2026-01-09T13:53:57.894886",
      "classified_at": "2026-01-09T14:00:06.930381",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391724747",
      "doi": "https://doi.org/10.14722/ndss.2024.24125"
    },
    "2401.07205": {
      "paper_id": "2401.07205",
      "title": "Crafter: Facial Feature Crafting against Inversion-based Identity Theft on Deep Models",
      "abstract": "With the increased capabilities at the edge (e.g., mobile device) and more stringent privacy requirement, it becomes a recent trend for deep learning-enabled applications to pre-process sensitive raw data at the edge and transmit the features to the backend cloud for further processing. A typical application is to run machine learning (ML) services on facial images collected from different individuals. To prevent identity theft, conventional methods commonly rely on an adversarial game-based approach to shed the identity information from the feature. However, such methods can not defend against adaptive attacks, in which an attacker takes a countermove against a known defence strategy. We propose Crafter, a feature crafting mechanism deployed at the edge, to protect the identity information from adaptive model inversion attacks while ensuring the ML tasks are properly carried out in the cloud. The key defence strategy is to mislead the attacker to a non-private prior from which the attacker gains little about the private identity. In this case, the crafted features act like poison training samples for attackers with adaptive model updates. Experimental results indicate that Crafter successfully defends both basic and possible adaptive attacks, which can not be achieved by state-of-the-art adversarial game-based methods.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Shiming Wang",
        "Zhe Ji",
        "Liyao Xiang",
        "Hao Zhang",
        "Xinbing Wang",
        "Chenghu Zhou",
        "Bo Li"
      ],
      "url": "https://arxiv.org/abs/2401.07205",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774482",
      "fetched_at": "2026-01-09T12:14:35.774483",
      "classified_at": "2026-01-09T13:25:57.974979",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2311.07389": {
      "paper_id": "2311.07389",
      "title": "Transpose Attack: Stealing Datasets with Bidirectional Training",
      "abstract": "Deep neural networks are normally executed in the forward direction. However, in this work, we identify a vulnerability that enables models to be trained in both directions and on different tasks. Adversaries can exploit this capability to hide rogue models within seemingly legitimate models. In addition, in this work we show that neural networks can be taught to systematically memorize and retrieve specific samples from datasets. Together, these findings expose a novel method in which adversaries can exfiltrate datasets from protected learning environments under the guise of legitimate models. We focus on the data exfiltration attack and show that modern architectures can be used to secretly exfiltrate tens of thousands of samples with high fidelity, high enough to compromise data privacy and even train new models. Moreover, to mitigate this threat we propose a novel approach for detecting infected models.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Guy Amit",
        "Mosh Levy",
        "Yisroel Mirsky"
      ],
      "url": "https://arxiv.org/abs/2311.07389",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML05",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774485",
      "fetched_at": "2026-01-09T12:14:35.774486",
      "classified_at": "2026-01-09T13:25:58.870191",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2406.14114": {
      "paper_id": "2406.14114",
      "title": "Dye4AI: Assuring Data Boundary on Generative AI Services",
      "abstract": "Generative artificial intelligence (AI) is versatile for various applications, but security and privacy concerns with third-party AI vendors hinder its broader adoption in sensitive scenarios. Hence, it is essential for users to validate the AI trustworthiness and ensure the security of data boundaries. In this paper, we present a dye testing system named Dye4AI, which injects crafted trigger data into human-AI dialogue and observes AI responses towards specific prompts to diagnose data flow in AI model evolution. Our dye testing procedure contains 3 stages: trigger generation, trigger insertion, and trigger retrieval. First, to retain both uniqueness and stealthiness, we design a new trigger that transforms a pseudo-random number to a intelligible format. Second, with a custom-designed three-step conversation strategy, we insert each trigger item into dialogue and confirm the model memorizes the new trigger knowledge in the current session. Finally, we routinely try to recover triggers with specific prompts in new sessions, as triggers can present in new sessions only if AI vendors leverage user data for model fine-tuning. Extensive experiments on six LLMs demonstrate our dye testing scheme is effective in ensuring the data boundary, even for models with various architectures and parameter sizes. Also, larger and premier models tend to be more suitable for Dye4AI, e.g., trigger can be retrieved in OpenLLaMa-13B even with only 2 insertions per trigger item. Moreover, we analyze the prompt selection in dye testing, providing insights for future testing systems on generative AI services.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Shu Wang",
        "Kun Sun",
        "Yan Zhai"
      ],
      "url": "https://arxiv.org/abs/2406.14114",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML05",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774490",
      "fetched_at": "2026-01-09T12:14:35.774491",
      "classified_at": "2026-01-09T13:25:59.515400",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2404.17399": {
      "paper_id": "2404.17399",
      "title": "Evaluations of Machine Learning Privacy Defenses are Misleading",
      "abstract": "Empirical defenses for machine learning privacy forgo the provable guarantees of differential privacy in the hope of achieving higher utility while resisting realistic adversaries. We identify severe pitfalls in existing empirical privacy evaluations (based on membership inference attacks) that result in misleading conclusions. In particular, we show that prior evaluations fail to characterize the privacy leakage of the most vulnerable samples, use weak attacks, and avoid comparisons with practical differential privacy baselines. In 5 case studies of empirical privacy defenses, we find that prior evaluations underestimate privacy leakage by an order of magnitude. Under our stronger evaluation, none of the empirical defenses we study are competitive with a properly tuned, high-utility DP-SGD baseline (with vacuous provable guarantees).",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Michael Aerni",
        "Jie Zhang",
        "Florian Tram\u00e8r"
      ],
      "url": "https://arxiv.org/abs/2404.17399",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774494",
      "fetched_at": "2026-01-09T12:14:35.774495",
      "classified_at": "2026-01-09T13:26:00.252361",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_11237db9": {
      "paper_id": "seed_11237db9",
      "title": "Towards a Re-evaluation of Data Forging Attacks in Practice",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-suliman.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774499",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_5a89b9a8": {
      "paper_id": "seed_5a89b9a8",
      "title": "SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark",
      "abstract": "Data reconstruction attacks, which aim to recover the training dataset of a target model with limited access, have gained increasing attention in recent years. However, there is currently no consensus on a formal definition of data reconstruction attacks or appropriate evaluation metrics for measuring their quality. This lack of rigorous definitions and universal metrics has hindered further advancement in this field. In this paper, we address this issue in the vision domain by proposing a unified attack taxonomy and formal definitions of data reconstruction attacks. We first propose a set of quantitative evaluation metrics that consider important criteria such as quantifiability, consistency, precision, and diversity. Additionally, we leverage large language models (LLMs) as a substitute for human judgment, enabling visual evaluation with an emphasis on high-quality reconstructions. Using our proposed taxonomy and metrics, we present a unified framework for systematically evaluating the strengths and limitations of existing attacks and establishing a benchmark for future research. Empirical results, primarily from a memorization perspective, not only validate the effectiveness of our metrics but also offer valuable insights for designing new attacks.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Wen Rui",
        "Liu, Yiyong",
        "Backes, Michael",
        "Zhang Yang"
      ],
      "url": "https://openalex.org/W4417136254",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774502",
      "fetched_at": "2026-01-09T13:53:58.990064",
      "classified_at": "2026-01-09T14:00:08.890390",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4417136254",
      "doi": "https://doi.org/10.48550/arxiv.2506.07888"
    },
    "2409.06280": {
      "paper_id": "2409.06280",
      "title": "Anonymity Unveiled: A Practical Framework for Auditing Data Use in Deep Learning Models",
      "abstract": "The rise of deep learning (DL) has led to a surging demand for training data, which incentivizes the creators of DL models to trawl through the Internet for training materials. Meanwhile, users often have limited control over whether their data (e.g., facial images) are used to train DL models without their consent, which has engendered pressing concerns.   This work proposes MembershipTracker, a practical data auditing tool that can empower ordinary users to reliably detect the unauthorized use of their data in training DL models. We view data auditing through the lens of membership inference (MI). MembershipTracker consists of a lightweight data marking component to mark the target data with small and targeted changes, which can be strongly memorized by the model trained on them; and a specialized MI-based verification process to audit whether the model exhibits strong memorization on the target samples.   MembershipTracker only requires the users to mark a small fraction of data (0.005% to 0.1% in proportion to the training set), and it enables the users to reliably detect the unauthorized use of their data (average 0% FPR@100% TPR). We show that MembershipTracker is highly effective across various settings, including industry-scale training on the full-size ImageNet-1k dataset. We finally evaluate MembershipTracker under multiple classes of countermeasures.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Zitao Chen",
        "Karthik Pattabiraman"
      ],
      "url": "https://arxiv.org/abs/2409.06280",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774504",
      "fetched_at": "2026-01-09T12:14:35.774505",
      "classified_at": "2026-01-09T13:26:00.861649",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2506.24033": {
      "paper_id": "2506.24033",
      "title": "Poisoning Attacks to Local Differential Privacy for Ranking Estimation",
      "abstract": "Local differential privacy (LDP) involves users perturbing their inputs to provide plausible deniability of their data. However, this also makes LDP vulnerable to poisoning attacks. In this paper, we first introduce novel poisoning attacks for ranking estimation. These attacks are intricate, as fake attackers do not merely adjust the frequency of target items. Instead, they leverage a limited number of fake users to precisely modify frequencies, effectively altering item rankings to maximize gains. To tackle this challenge, we introduce the concepts of attack cost and optimal attack item (set), and propose corresponding strategies for kRR, OUE, and OLH protocols. For kRR, we iteratively select optimal attack items and allocate suitable fake users. For OUE, we iteratively determine optimal attack item sets and consider the incremental changes in item frequencies across different sets. Regarding OLH, we develop a harmonic cost function based on the pre-image of a hash to select that supporting a larger number of effective attack items. Lastly, we present an attack strategy based on confidence levels to quantify the probability of a successful attack and the number of attack iterations more precisely. We demonstrate the effectiveness of our attacks through theoretical and empirical evidence, highlighting the necessity for defenses against these attacks. The source code and data have been made available at https://github.com/LDP-user/LDP-Ranking.git.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Pei Zhan",
        "Peng Tang",
        "Yangzhuo Li",
        "Puwen Wei",
        "Shanqing Guo"
      ],
      "url": "https://arxiv.org/abs/2506.24033",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774508",
      "fetched_at": "2026-01-09T12:14:35.774509",
      "classified_at": "2026-01-09T13:26:01.713198",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_cb751896": {
      "paper_id": "seed_cb751896",
      "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
      "abstract": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\u03b5$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\u03b5$ that are sometimes used in practice (e.g., $\u03b5=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
      "year": 2019,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "K. Rustan M. Leino",
        "Matt Fredrikson"
      ],
      "url": "https://openalex.org/W2956128647",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774512",
      "fetched_at": "2026-01-09T13:53:59.756788",
      "classified_at": "2026-01-09T14:00:10.702840",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2956128647",
      "doi": "https://doi.org/10.48550/arxiv.1906.11798"
    },
    "seed_ea780f94": {
      "paper_id": "seed_ea780f94",
      "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
      "abstract": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. Next, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to identify samples with high privacy risks and perform attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score and demonstrate that the distribution of privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risks, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Liwei Song",
        "Prateek Mittal"
      ],
      "url": "https://openalex.org/W3013068160",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774514",
      "fetched_at": "2026-01-09T13:54:00.583629",
      "classified_at": "2026-01-09T14:00:12.593309",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3013068160",
      "doi": "https://doi.org/10.48550/arxiv.2003.10595"
    },
    "2101.01341": {
      "paper_id": "2101.01341",
      "title": "Practical Blind Membership Inference Attack via Differential Comparisons",
      "abstract": "Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information.   In this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa.   BlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses.",
      "year": 2021,
      "venue": "NDSS",
      "authors": [
        "Bo Hui",
        "Yuchen Yang",
        "Haolin Yuan",
        "Philippe Burlina",
        "Neil Zhenqiang Gong",
        "Yinzhi Cao"
      ],
      "url": "https://arxiv.org/abs/2101.01341",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774517",
      "fetched_at": "2026-01-09T12:14:35.774518",
      "classified_at": "2026-01-09T13:26:02.369976",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "1909.03935": {
      "paper_id": "1909.03935",
      "title": "GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models",
      "abstract": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
      "year": 2020,
      "venue": "ACM CCS",
      "authors": [
        "Dingfan Chen",
        "Ning Yu",
        "Yang Zhang",
        "Mario Fritz"
      ],
      "url": "https://arxiv.org/abs/1909.03935",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774521",
      "fetched_at": "2026-01-09T12:14:35.774522",
      "classified_at": "2026-01-09T13:26:03.303728",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_3132e6a6": {
      "paper_id": "seed_3132e6a6",
      "title": "Quantifying and Mitigating Privacy Risks of Contrastive Learning",
      "abstract": "Data is the key factor to drive the development of machine learning (ML) during the past decade. However, high-quality data, in particular labeled data, is often hard and expensive to collect. To leverage large-scale unlabeled data, self-supervised learning, represented by contrastive learning, is introduced. The objective of contrastive learning is to map different views derived from a training sample (e.g., through data augmentation) closer in their representation space, while different views derived from different samples more distant. In this way, a contrastive model learns to generate informative representations for data samples, which are then used to perform downstream ML tasks. Recent research has shown that machine learning models are vulnerable to various privacy attacks. However, most of the current efforts concentrate on models trained with supervised learning. Meanwhile, data samples' informative representations learned with contrastive learning may cause severe privacy risks as well. In this paper, we perform the first privacy analysis of contrastive learning through the lens of membership inference and attribute inference. Our experimental results show that contrastive models trained on image datasets are less vulnerable to membership inference attacks but more vulnerable to attribute inference attacks compared to supervised models. The former is due to the fact that contrastive models are less prone to overfitting, while the latter is caused by contrastive models' capability of representing data samples expressively. To remedy this situation, we propose the first privacy-preserving contrastive learning mechanism, Talos, relying on adversarial training. Empirical results show that Talos can successfully mitigate attribute inference risks for contrastive models while maintaining their membership privacy and model utility.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Xinlei He",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W3211574353",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774525",
      "fetched_at": "2026-01-09T13:54:01.372603",
      "classified_at": "2026-01-09T14:00:14.408823",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3211574353",
      "doi": "https://doi.org/10.1145/3460120.3484571"
    },
    "seed_5570446a": {
      "paper_id": "seed_5570446a",
      "title": "Membership Inference Attacks Against Recommender Systems",
      "abstract": "Recently, recommender systems have achieved promising performances and become one of the most widely used web applications. However, recommender systems are often trained on highly sensitive user data, thus potential data leakage from recommender systems may lead to severe privacy problems.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Minxing Zhang",
        "Zhaochun Ren",
        "Zihan Wang",
        "Pengjie Ren",
        "Zhunmin Chen",
        "Pengfei Hu",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W3211930400",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774528",
      "fetched_at": "2026-01-09T13:54:02.154826",
      "classified_at": "2026-01-09T14:00:16.352519",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3211930400",
      "doi": "https://doi.org/10.1145/3460120.3484770"
    },
    "2108.11023": {
      "paper_id": "2108.11023",
      "title": "EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning",
      "abstract": "Given a set of unlabeled images or (image, text) pairs, contrastive learning aims to pre-train an image encoder that can be used as a feature extractor for many downstream tasks. In this work, we propose EncoderMI, the first membership inference method against image encoders pre-trained by contrastive learning. In particular, given an input and a black-box access to an image encoder, EncoderMI aims to infer whether the input is in the training dataset of the image encoder. EncoderMI can be used 1) by a data owner to audit whether its (public) data was used to pre-train an image encoder without its authorization or 2) by an attacker to compromise privacy of the training data when it is private/sensitive. Our EncoderMI exploits the overfitting of the image encoder towards its training data. In particular, an overfitted image encoder is more likely to output more (or less) similar feature vectors for two augmented versions of an input in (or not in) its training dataset. We evaluate EncoderMI on image encoders pre-trained on multiple datasets by ourselves as well as the Contrastive Language-Image Pre-training (CLIP) image encoder, which is pre-trained on 400 million (image, text) pairs collected from the Internet and released by OpenAI. Our results show that EncoderMI can achieve high accuracy, precision, and recall. We also explore a countermeasure against EncoderMI via preventing overfitting through early stopping. Our results show that it achieves trade-offs between accuracy of EncoderMI and utility of the image encoder, i.e., it can reduce the accuracy of EncoderMI, but it also incurs classification accuracy loss of the downstream classifiers built based on the image encoder.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Hongbin Liu",
        "Jinyuan Jia",
        "Wenjie Qu",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://arxiv.org/abs/2108.11023",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774533",
      "fetched_at": "2026-01-09T12:14:35.774534",
      "classified_at": "2026-01-09T13:26:03.999291",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2208.11180": {
      "paper_id": "2208.11180",
      "title": "Auditing Membership Leakages of Multi-Exit Networks",
      "abstract": "Relying on the fact that not all inputs require the same amount of computation to yield a confident prediction, multi-exit networks are gaining attention as a prominent approach for pushing the limits of efficient deployment. Multi-exit networks endow a backbone model with early exits, allowing to obtain predictions at intermediate layers of the model and thus save computation time and/or energy. However, current various designs of multi-exit networks are only considered to achieve the best trade-off between resource usage efficiency and prediction accuracy, the privacy risks stemming from them have never been explored. This prompts the need for a comprehensive investigation of privacy risks in multi-exit networks.   In this paper, we perform the first privacy analysis of multi-exit networks through the lens of membership leakages. In particular, we first leverage the existing attack methodologies to quantify the multi-exit networks' vulnerability to membership leakages. Our experimental results show that multi-exit networks are less vulnerable to membership leakages and the exit (number and depth) attached to the backbone model is highly correlated with the attack performance. Furthermore, we propose a hybrid attack that exploits the exit information to improve the performance of existing attacks. We evaluate membership leakage threat caused by our hybrid attack under three different adversarial setups, ultimately arriving at a model-free and data-free adversary. These results clearly demonstrate that our hybrid attacks are very broadly applicable, thereby the corresponding risks are much more severe than shown by existing membership inference attacks. We further present a defense mechanism called TimeGuard specifically for multi-exit networks and show that TimeGuard mitigates the newly proposed attacks perfectly.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Zheng Li",
        "Yiyong Liu",
        "Xinlei He",
        "Ning Yu",
        "Michael Backes",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2208.11180",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774537",
      "fetched_at": "2026-01-09T12:14:35.774538",
      "classified_at": "2026-01-09T13:26:04.595368",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2208.14933": {
      "paper_id": "2208.14933",
      "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
      "abstract": "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called \\system, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely \\emph{distilled loss trajectory}, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6$\\times$ higher true-positive rate at a low false-positive rate of 0.1\\% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Yiyong Liu",
        "Zhengyu Zhao",
        "Michael Backes",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2208.14933",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774541",
      "fetched_at": "2026-01-09T12:14:35.774542",
      "classified_at": "2026-01-09T13:26:05.255191",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2209.01688": {
      "paper_id": "2209.01688",
      "title": "On the Privacy Risks of Cell-Based NAS Architectures",
      "abstract": "Existing studies on neural architecture search (NAS) mainly focus on efficiently and effectively searching for network architectures with better performance. Little progress has been made to systematically understand if the NAS-searched architectures are robust to privacy attacks while abundant work has already shown that human-designed architectures are prone to privacy attacks. In this paper, we fill this gap and systematically measure the privacy risks of NAS architectures. Leveraging the insights from our measurement study, we further explore the cell patterns of cell-based NAS architectures and evaluate how the cell patterns affect the privacy risks of NAS-searched architectures. Through extensive experiments, we shed light on how to design robust NAS architectures against privacy attacks, and also offer a general methodology to understand the hidden correlation between the NAS-searched architectures and other privacy risks.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Hai Huang",
        "Zhikun Zhang",
        "Yun Shen",
        "Michael Backes",
        "Qi Li",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2209.01688",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774545",
      "fetched_at": "2026-01-09T12:14:35.774546",
      "classified_at": "2026-01-09T13:26:05.915841",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_171a82d9": {
      "paper_id": "seed_171a82d9",
      "title": "Membership Inference Attacks and Defenses in Neural Network Pruning",
      "abstract": "Neural network pruning has been an essential technique to reduce the computation and memory requirements for using deep neural networks for resource-constrained devices. Most existing research focuses primarily on balancing the sparsity and accuracy of a pruned neural network by strategically removing insignificant parameters and retraining the pruned model. Such efforts on reusing training samples pose serious privacy risks due to increased memorization, which, however, has not been investigated yet. In this paper, we conduct the first analysis of privacy risks in neural network pruning. Specifically, we investigate the impacts of neural network pruning on training data privacy, i.e., membership inference attacks. We first explore the impact of neural network pruning on prediction divergence, where the pruning process disproportionately affects the pruned model's behavior for members and non-members. Meanwhile, the influence of divergence even varies among different classes in a fine-grained manner. Enlighten by such divergence, we proposed a self-attention membership inference attack against the pruned neural networks. Extensive experiments are conducted to rigorously evaluate the privacy impacts of different pruning approaches, sparsity levels, and adversary knowledge. The proposed attack shows the higher attack performance on the pruned models when compared with eight existing membership inference attacks. In addition, we propose a new defense mechanism to protect the pruning process by mitigating the prediction divergence based on KL-divergence distance, whose effectiveness has been experimentally demonstrated to effectively mitigate the privacy risks while maintaining the sparsity and accuracy of the pruned models.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Xiaoyong Yuan",
        "Lan Zhang"
      ],
      "url": "https://openalex.org/W4221154610",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774548",
      "fetched_at": "2026-01-09T13:54:02.978291",
      "classified_at": "2026-01-09T14:00:19.533768",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4221154610",
      "doi": "https://doi.org/10.48550/arxiv.2202.03335"
    },
    "seed_ae64a5dd": {
      "paper_id": "seed_ae64a5dd",
      "title": "Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture",
      "abstract": "Membership inference attacks are a key measure to evaluate privacy leakage in machine learning (ML) models. These attacks aim to distinguish training members from non-members by exploiting differential behavior of the models on member and non-member inputs. The goal of this work is to train ML models that have high membership privacy while largely preserving their utility; we therefore aim for an empirical membership privacy guarantee as opposed to the provable privacy guarantees provided by techniques like differential privacy, as such techniques are shown to deteriorate model utility. Specifically, we propose a new framework to train privacy-preserving models that induces similar behavior on member and non-member inputs to mitigate membership inference attacks. Our framework, called SELENA, has two major components. The first component and the core of our defense is a novel ensemble architecture for training. This architecture, which we call Split-AI, splits the training data into random subsets, and trains a model on each subset of the data. We use an adaptive inference strategy at test time: our ensemble architecture aggregates the outputs of only those models that did not contain the input sample in their training data. We prove that our Split-AI architecture defends against a large family of membership inference attacks, however, it is susceptible to new adaptive attacks. Therefore, we use a second component in our framework called Self-Distillation to protect against such stronger attacks. The Self-Distillation component (self-)distills the training dataset through our Split-AI ensemble, without using any external public datasets. Through extensive experiments on major benchmark datasets we show that SELENA presents a superior trade-off between membership privacy and utility compared to the state of the art.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Xinyu Tang",
        "Saeed Mahloujifar",
        "Liwei Song",
        "Virat Shejwalkar",
        "Milad Nasr",
        "Amir Houmansadr",
        "Prateek Mittal"
      ],
      "url": "https://openalex.org/W3205533264",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774551",
      "fetched_at": "2026-01-09T13:54:03.776935",
      "classified_at": "2026-01-09T14:00:21.550228",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3205533264",
      "doi": "https://doi.org/10.48550/arxiv.2110.08324"
    },
    "2111.09679": {
      "paper_id": "2111.09679",
      "title": "Enhanced Membership Inference Attacks against Machine Learning Models",
      "abstract": "How much does a machine learning algorithm leak about its training data, and why? Membership inference attacks are used as an auditing tool to quantify this leakage. In this paper, we present a comprehensive \\textit{hypothesis testing framework} that enables us not only to formally express the prior work in a consistent way, but also to design new membership inference attacks that use reference models to achieve a significantly higher power (true positive rate) for any (false positive rate) error. More importantly, we explain \\textit{why} different attacks perform differently. We present a template for indistinguishability games, and provide an interpretation of attack success rate across different instances of the game. We discuss various uncertainties of attackers that arise from the formulation of the problem, and show how our approach tries to minimize the attack uncertainty to the one bit secret about the presence or absence of a data point in the training set. We perform a \\textit{differential analysis} between all types of attacks, explain the gap between them, and show what causes data points to be vulnerable to an attack (as the reasons vary due to different granularities of memorization, from overfitting to conditional memorization). Our auditing framework is openly accessible as part of the \\textit{Privacy Meter} software tool.",
      "year": 2022,
      "venue": "USENIX Security",
      "authors": [
        "Jiayuan Ye",
        "Aadyaa Maddi",
        "Sasi Kumar Murakonda",
        "Vincent Bindschaedler",
        "Reza Shokri"
      ],
      "url": "https://arxiv.org/abs/2111.09679",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774554",
      "fetched_at": "2026-01-09T12:14:35.774555",
      "classified_at": "2026-01-09T13:26:06.577289",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2209.08615": {
      "paper_id": "2209.08615",
      "title": "Membership Inference Attacks and Generalization: A Causal Perspective",
      "abstract": "Membership inference (MI) attacks highlight a privacy weakness in present stochastic training methods for neural networks. It is not well understood, however, why they arise. Are they a natural consequence of imperfect generalization only? Which underlying causes should we address during training to mitigate these attacks? Towards answering such questions, we propose the first approach to explain MI attacks and their connection to generalization based on principled causal reasoning. We offer causal graphs that quantitatively explain the observed MI attack performance achieved for $6$ attack variants. We refute several prior non-quantitative hypotheses that over-simplify or over-estimate the influence of underlying causes, thereby failing to capture the complex interplay between several factors. Our causal models also show a new connection between generalization and MI attacks via their shared causal factors. Our causal models have high predictive power ($0.90$), i.e., their analytical predictions match with observations in unseen experiments often, which makes analysis via them a pragmatic alternative.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Teodora Baluta",
        "Shiqi Shen",
        "S. Hitarth",
        "Shruti Tople",
        "Prateek Saxena"
      ],
      "url": "https://arxiv.org/abs/2209.08615",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774558",
      "fetched_at": "2026-01-09T12:14:35.774559",
      "classified_at": "2026-01-09T13:26:07.392662",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2309.07983": {
      "paper_id": "2309.07983",
      "title": "SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker Recognition Systems",
      "abstract": "Membership inference attacks allow adversaries to determine whether a particular example was contained in the model's training dataset. While previous works have confirmed the feasibility of such attacks in various applications, none has focused on speaker recognition (SR), a promising voice-based biometric recognition technique. In this work, we propose SLMIA-SR, the first membership inference attack tailored to SR. In contrast to conventional example-level attack, our attack features speaker-level membership inference, i.e., determining if any voices of a given speaker, either the same as or different from the given inference voices, have been involved in the training of a model. It is particularly useful and practical since the training and inference voices are usually distinct, and it is also meaningful considering the open-set nature of SR, namely, the recognition speakers were often not present in the training data. We utilize intra-similarity and inter-dissimilarity, two training objectives of SR, to characterize the differences between training and non-training speakers and quantify them with two groups of features driven by carefully-established feature engineering to mount the attack. To improve the generalizability of our attack, we propose a novel mixing ratio training strategy to train attack models. To enhance the attack performance, we introduce voice chunk splitting to cope with the limited number of inference voices and propose to train attack models dependent on the number of inference voices. Our attack is versatile and can work in both white-box and black-box scenarios. Additionally, we propose two novel techniques to reduce the number of black-box queries while maintaining the attack performance. Extensive experiments demonstrate the effectiveness of SLMIA-SR.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Guangke Chen",
        "Yedi Zhang",
        "Fu Song"
      ],
      "url": "https://arxiv.org/abs/2309.07983",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774561",
      "fetched_at": "2026-01-09T12:14:35.774562",
      "classified_at": "2026-01-09T13:26:08.105888",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2307.01610": {
      "paper_id": "2307.01610",
      "title": "Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction",
      "abstract": "Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving high accuracy. To further reduce privacy risk, HAMP uniformly modifies all the prediction outputs to become low-confidence outputs while preserving the accuracy, which effectively obscures the differences between the prediction on members and non-members. We conduct extensive evaluation on five benchmark datasets, and show that HAMP provides consistently high accuracy and strong membership privacy. Our comparison with seven state-of-the-art defenses shows that HAMP achieves a superior privacy-utility trade off than those techniques.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Zitao Chen",
        "Karthik Pattabiraman"
      ],
      "url": "https://arxiv.org/abs/2307.01610",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774565",
      "fetched_at": "2026-01-09T12:14:35.774566",
      "classified_at": "2026-01-09T13:26:08.831478",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_cbc0385f": {
      "paper_id": "seed_cbc0385f",
      "title": "Membership Inference Attacks Against Vision-Language Models",
      "abstract": "Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxR\u00e9nyi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Zhan Li",
        "Yongtao Wu",
        "Yihang Chen",
        "Francesco Tonin",
        "Elias Abad Rocamora",
        "Volkan Cevher"
      ],
      "url": "https://openalex.org/W4404395433",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774570",
      "fetched_at": "2026-01-09T13:54:04.308089",
      "classified_at": "2026-01-09T14:00:23.734515",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4404395433",
      "doi": "https://doi.org/10.48550/arxiv.2411.02902"
    },
    "seed_dba7327d": {
      "paper_id": "seed_dba7327d",
      "title": "Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models",
      "abstract": "Membership Inference Attacks (MIAs) aim to predict whether a data sample belongs to the model's training set or not. Although prior research has extensively explored MIAs in Large Language Models (LLMs), they typically require accessing to complete output logits (\\ie, \\textit{logits-based attacks}), which are usually not available in practice. In this paper, we study the vulnerability of pre-trained LLMs to MIAs in the \\textit{label-only setting}, where the adversary can only access generated tokens (text). We first reveal that existing label-only MIAs have minor effects in attacking pre-trained LLMs, although they are highly effective in inferring fine-tuning datasets used for personalized LLMs. We find that their failure stems from two main reasons, including better generalization and overly coarse perturbation. Specifically, due to the extensive pre-training corpora and exposing each sample only a few times, LLMs exhibit minimal robustness differences between members and non-members. This makes token-level perturbations too coarse to capture such differences. To alleviate these problems, we propose \\textbf{PETAL}: a label-only membership inference attack based on \\textbf{PE}r-\\textbf{T}oken sem\\textbf{A}ntic simi\\textbf{L}arity. Specifically, PETAL leverages token-level semantic similarity to approximate output probabilities and subsequently calculate the perplexity. It finally exposes membership based on the common assumption that members are `better' memorized and have smaller perplexity. We conduct extensive experiments on the WikiMIA benchmark and the more challenging MIMIR benchmark. Empirically, our PETAL performs better than the extensions of existing label-only attacks against personalized LLMs and even on par with other advanced logit-based attacks across all metrics on five prevalent open-source LLMs.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yu He",
        "Boheng Li",
        "Lili Liu",
        "Zhongjie Ba",
        "Wei Dong",
        "Yiming Li",
        "Zhan Qin",
        "Kui Ren",
        "Chun Chen"
      ],
      "url": "https://openalex.org/W4416043568",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774573",
      "fetched_at": "2026-01-09T13:54:04.863140",
      "classified_at": "2026-01-09T14:00:25.540594",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4416043568",
      "doi": "https://doi.org/10.48550/arxiv.2502.18943"
    },
    "seed_6f528159": {
      "paper_id": "seed_6f528159",
      "title": "Enhanced Label-Only Membership Inference Attacks with Fewer Queries",
      "abstract": "A comprehensive review of an area of machine learning that deals with the use of unlabeled data in classification problems: state-of-the-art algorithms, a taxonomy of the field, applications, benchmark experiments, and directions for future research. In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research.Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction.",
      "year": 2006,
      "venue": "The MIT Press eBooks",
      "authors": [
        "Olivier Chapelle",
        "Bernhard Schlkopf",
        "Alexander Zien"
      ],
      "url": "https://openalex.org/W1479807131",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774576",
      "fetched_at": "2026-01-09T13:54:05.668315",
      "classified_at": "2026-01-09T14:00:27.390690",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W1479807131",
      "doi": "https://doi.org/10.7551/mitpress/9780262033589.001.0001"
    },
    "seed_1e6f5476": {
      "paper_id": "seed_1e6f5476",
      "title": "SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks",
      "abstract": "Large language models (LLMs) have achieved remarkable success and are widely adopted for diverse applications. However, fine-tuning these models often involves private or sensitive information, raising critical privacy concerns. In this work, we conduct the first comprehensive study evaluating the vulnerability of fine-tuned LLMs to membership inference attacks (MIAs). Our empirical analysis demonstrates that MIAs exploit the loss reduction during fine-tuning, making them highly effective in revealing membership information. These findings motivate the development of our defense. We propose SOFT (\\textbf{S}elective data \\textbf{O}bfuscation in LLM \\textbf{F}ine-\\textbf{T}uning), a novel defense technique that mitigates privacy leakage by leveraging influential data selection with an adjustable parameter to balance utility preservation and privacy protection. Our extensive experiments span six diverse domains and multiple LLM architectures and scales. Results show that SOFT effectively reduces privacy risks while maintaining competitive model performance, offering a practical and scalable solution to safeguard sensitive information in fine-tuned LLMs.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Zhang, Kaiyuan",
        "Cheng, Siyuan",
        "Guo, Hanxi",
        "Chen, Yuetian",
        "Su, Zian",
        "An, Shengwei",
        "Du, Yuntao",
        "Fleming, Charles",
        "Kundu, Ashish",
        "Zhang, Xiangyu",
        "Li, Ninghui"
      ],
      "url": "https://openalex.org/W4417351543",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774579",
      "fetched_at": "2026-01-09T13:54:06.215653",
      "classified_at": "2026-01-09T14:00:29.673644",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4417351543",
      "doi": "https://doi.org/10.48550/arxiv.2506.10424"
    },
    "2506.13972": {
      "paper_id": "2506.13972",
      "title": "Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble",
      "abstract": "Membership inference attacks (MIAs) pose a significant threat to the privacy of machine learning models and are widely used as tools for privacy assessment, auditing, and machine unlearning. While prior MIA research has primarily focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either by developing new methods to enhance these metrics or using them to evaluate privacy solutions - we found that it overlooks the disparities among different attacks. These disparities, both between distinct attack methods and between multiple instantiations of the same method, have crucial implications for the reliability and completeness of MIAs as privacy evaluation tools. In this paper, we systematically investigate these disparities through a novel framework based on coverage and stability analysis. Extensive experiments reveal significant disparities in MIAs, their potential causes, and their broader implications for privacy evaluation. To address these challenges, we propose an ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities. This framework not only enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Zhiqi Wang",
        "Chengyu Zhang",
        "Yuetian Chen",
        "Nathalie Baracaldo",
        "Swanand Kadhe",
        "Lei Yu"
      ],
      "url": "https://arxiv.org/abs/2506.13972",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774581",
      "fetched_at": "2026-01-09T12:14:35.774582",
      "classified_at": "2026-01-09T13:26:09.457650",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_ee9c7760": {
      "paper_id": "seed_ee9c7760",
      "title": "Label Inference Attacks Against Vertical Federated Learning",
      "abstract": null,
      "year": 2022,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec22summer_fu.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774585",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "1906.09679": {
      "paper_id": "1906.09679",
      "title": "The Value of Collaboration in Convex Machine Learning with Differential Privacy",
      "abstract": "In this paper, we apply machine learning to distributed private data owned by multiple data owners, entities with access to non-overlapping training datasets. We use noisy, differentially-private gradients to minimize the fitness cost of the machine learning model using stochastic gradient descent. We quantify the quality of the trained model, using the fitness cost, as a function of privacy budget and size of the distributed datasets to capture the trade-off between privacy and utility in machine learning. This way, we can predict the outcome of collaboration among privacy-aware data owners prior to executing potentially computationally-expensive machine learning algorithms. Particularly, we show that the difference between the fitness of the trained machine learning model using differentially-private gradient queries and the fitness of the trained machine model in the absence of any privacy concerns is inversely proportional to the size of the training datasets squared and the privacy budget squared. We successfully validate the performance prediction with the actual performance of the proposed privacy-aware learning algorithms, applied to: financial datasets for determining interest rates of loans using regression; and detecting credit card frauds using support vector machines.",
      "year": 2020,
      "venue": "IEEE S&P",
      "authors": [
        "Nan Wu",
        "Farhad Farokhi",
        "David Smith",
        "Mohamed Ali Kaafar"
      ],
      "url": "https://arxiv.org/abs/1906.09679",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774588",
      "fetched_at": "2026-01-09T12:14:35.774589",
      "classified_at": "2026-01-09T13:26:10.327355",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_30d1ae53": {
      "paper_id": "seed_30d1ae53",
      "title": "Leakage of Dataset Properties in Multi-Party Machine Learning",
      "abstract": "Secure multi-party machine learning allows several parties to build a model on their pooled data to increase utility while not explicitly sharing data with each other. We show that such multi-party computation can cause leakage of global dataset properties between the parties even when parties obtain only black-box access to the final model. In particular, a ``curious'' party can infer the distribution of sensitive attributes in other parties' data with high accuracy. This raises concerns regarding the confidentiality of properties pertaining to the whole dataset as opposed to individual data records. We show that our attack can leak population-level properties in datasets of different types, including tabular, text, and graph data. To understand and measure the source of leakage, we consider several models of correlation between a sensitive attribute and the rest of the data. Using multiple machine learning models, we show that leakage occurs even if the sensitive attribute is not included in the training data and has a low correlation with other attributes or the target variable.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Wanrong Zhang",
        "Shruti Tople",
        "Olga Ohrimenko"
      ],
      "url": "https://openalex.org/W3171497996",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML05",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774592",
      "fetched_at": "2026-01-09T13:54:07.509841",
      "classified_at": "2026-01-09T14:00:31.675281",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3171497996",
      "doi": "https://doi.org/10.48550/arxiv.2006.07267"
    },
    "2012.02670": {
      "paper_id": "2012.02670",
      "title": "Unleashing the Tiger: Inference Attacks on Split Learning",
      "abstract": "We investigate the security of Split Learning -- a novel collaborative machine learning framework that enables peak performance by requiring minimal resources consumption. In the present paper, we expose vulnerabilities of the protocol and demonstrate its inherent insecurity by introducing general attack strategies targeting the reconstruction of clients' private training sets. More prominently, we show that a malicious server can actively hijack the learning process of the distributed model and bring it into an insecure state that enables inference attacks on clients' data. We implement different adaptations of the attack and test them on various datasets as well as within realistic threat scenarios. We demonstrate that our attack is able to overcome recently proposed defensive techniques aimed at enhancing the security of the split learning protocol. Finally, we also illustrate the protocol's insecurity against malicious clients by extending previously devised attacks for Federated Learning. To make our results reproducible, we made our code available at https://github.com/pasquini-dario/SplitNN_FSHA.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Dario Pasquini",
        "Giuseppe Ateniese",
        "Massimo Bernaschi"
      ],
      "url": "https://arxiv.org/abs/2012.02670",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774595",
      "fetched_at": "2026-01-09T12:14:35.774596",
      "classified_at": "2026-01-09T13:26:11.031030",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2009.03561": {
      "paper_id": "2009.03561",
      "title": "Local and Central Differential Privacy for Robustness and Privacy in Federated Learning",
      "abstract": "Federated Learning (FL) allows multiple participants to train machine learning models collaboratively by keeping their datasets local while only exchanging model updates. Alas, this is not necessarily free from privacy and robustness vulnerabilities, e.g., via membership, property, and backdoor attacks. This paper investigates whether and to what extent one can use differential Privacy (DP) to protect both privacy and robustness in FL. To this end, we present a first-of-its-kind evaluation of Local and Central Differential Privacy (LDP/CDP) techniques in FL, assessing their feasibility and effectiveness. Our experiments show that both DP variants do d fend against backdoor attacks, albeit with varying levels of protection-utility trade-offs, but anyway more effectively than other robustness defenses. DP also mitigates white-box membership inference attacks in FL, and our work is the first to show it empirically. Neither LDP nor CDP, however, defend against property inference. Overall, our work provides a comprehensive, re-usable measurement methodology to quantify the trade-offs between robustness/privacy and utility in differentially private FL.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Mohammad Naseri",
        "Jamie Hayes",
        "Emiliano De Cristofaro"
      ],
      "url": "https://arxiv.org/abs/2009.03561",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774599",
      "fetched_at": "2026-01-09T12:14:35.774600",
      "classified_at": "2026-01-09T13:26:11.995552",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_28ac2152": {
      "paper_id": "seed_28ac2152",
      "title": "Gradient Obfuscation Gives a False Sense of Security in Federated Learning",
      "abstract": "Federated learning has been proposed as a privacy-preserving machine learning framework that enables multiple clients to collaborate without sharing raw data. However, client privacy protection is not guaranteed by design in this framework. Prior work has shown that the gradient sharing strategies in federated learning can be vulnerable to data reconstruction attacks. In practice, though, clients may not transmit raw gradients considering the high communication cost or due to privacy enhancement requirements. Empirical studies have demonstrated that gradient obfuscation, including intentional obfuscation via gradient noise injection and unintentional obfuscation via gradient compression, can provide more privacy protection against reconstruction attacks. In this work, we present a new data reconstruction attack framework targeting the image classification task in federated learning. We show that commonly adopted gradient postprocessing procedures, such as gradient quantization, gradient sparsification, and gradient perturbation, may give a false sense of security in federated learning. Contrary to prior studies, we argue that privacy enhancement should not be treated as a byproduct of gradient compression. Additionally, we design a new method under the proposed framework to reconstruct the image at the semantic level. We quantify the semantic privacy leakage and compare with conventional based on image similarity scores. Our comparisons challenge the image data leakage evaluation schemes in the literature. The results emphasize the importance of revisiting and redesigning the privacy protection mechanisms for client data in existing federated learning algorithms.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Kai Yue",
        "Richeng Jin",
        "Chau-Wai Wong",
        "Dror Baron",
        "Huaiyu Dai"
      ],
      "url": "https://openalex.org/W4281609047",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774603",
      "fetched_at": "2026-01-09T13:54:08.279503",
      "classified_at": "2026-01-09T14:00:33.531842",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4281609047",
      "doi": "https://doi.org/10.48550/arxiv.2206.04055"
    },
    "seed_440edd44": {
      "paper_id": "seed_440edd44",
      "title": "PPA: Preference Profiling Attack Against Federated Learning",
      "abstract": null,
      "year": 2023,
      "venue": "NDSS",
      "authors": [],
      "url": "https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_s171_paper.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774606",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2205.08443": {
      "paper_id": "2205.08443",
      "title": "On the (In)security of Peer-to-Peer Decentralized Machine Learning",
      "abstract": "In this work, we carry out the first, in-depth, privacy analysis of Decentralized Learning -- a collaborative machine learning framework aimed at addressing the main limitations of federated learning. We introduce a suite of novel attacks for both passive and active decentralized adversaries. We demonstrate that, contrary to what is claimed by decentralized learning proposers, decentralized learning does not offer any security advantage over federated learning. Rather, it increases the attack surface enabling any user in the system to perform privacy attacks such as gradient inversion, and even gain full control over honest users' local model. We also show that, given the state of the art in protections, privacy-preserving configurations of decentralized learning require fully connected networks, losing any practical advantage over the federated setup and therefore completely defeating the objective of the decentralized approach.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Dario Pasquini",
        "Mathilde Raynal",
        "Carmela Troncoso"
      ],
      "url": "https://arxiv.org/abs/2205.08443",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774609",
      "fetched_at": "2026-01-09T12:14:35.774610",
      "classified_at": "2026-01-09T13:26:12.660880",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2107.03311": {
      "paper_id": "2107.03311",
      "title": "RoFL: Robustness of Secure Federated Learning",
      "abstract": "Even though recent years have seen many attacks exposing severe vulnerabilities in Federated Learning (FL), a holistic understanding of what enables these attacks and how they can be mitigated effectively is still lacking. In this work, we demystify the inner workings of existing (targeted) attacks. We provide new insights into why these attacks are possible and why a definitive solution to FL robustness is challenging. We show that the need for ML algorithms to memorize tail data has significant implications for FL integrity. This phenomenon has largely been studied in the context of privacy; our analysis sheds light on its implications for ML integrity. We show that certain classes of severe attacks can be mitigated effectively by enforcing constraints such as norm bounds on clients' updates. We investigate how to efficiently incorporate these constraints into secure FL protocols in the single-server setting. Based on this, we propose RoFL, a new secure FL system that extends secure aggregation with privacy-preserving input validation. Specifically, RoFL can enforce constraints such as $L_2$ and $L_\\infty$ bounds on high-dimensional encrypted model updates.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Hidde Lycklama",
        "Lukas Burkhalter",
        "Alexander Viand",
        "Nicolas K\u00fcchler",
        "Anwar Hithnawi"
      ],
      "url": "https://arxiv.org/abs/2107.03311",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774613",
      "fetched_at": "2026-01-09T12:14:35.774614",
      "classified_at": "2026-01-09T13:26:13.614378",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2304.00129": {
      "paper_id": "2304.00129",
      "title": "Scalable and Privacy-Preserving Federated Principal Component Analysis",
      "abstract": "Principal component analysis (PCA) is an essential algorithm for dimensionality reduction in many data science domains. We address the problem of performing a federated PCA on private data distributed among multiple data providers while ensuring data confidentiality. Our solution, SF-PCA, is an end-to-end secure system that preserves the confidentiality of both the original data and all intermediate results in a passive-adversary model with up to all-but-one colluding parties. SF-PCA jointly leverages multiparty homomorphic encryption, interactive protocols, and edge computing to efficiently interleave computations on local cleartext data with operations on collectively encrypted data. SF-PCA obtains results as accurate as non-secure centralized solutions, independently of the data distribution among the parties. It scales linearly or better with the dataset dimensions and with the number of data providers. SF-PCA is more precise than existing approaches that approximate the solution by combining local analysis results, and between 3x and 250x faster than privacy-preserving alternatives based solely on secure multiparty computation or homomorphic encryption. Our work demonstrates the practical applicability of secure and federated PCA on private distributed datasets.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "David Froelicher",
        "Hyunghoon Cho",
        "Manaswitha Edupalli",
        "Joao Sa Sousa",
        "Jean-Philippe Bossuat",
        "Apostolos Pyrgelis",
        "Juan R. Troncoso-Pastoriza",
        "Bonnie Berger",
        "Jean-Pierre Hubaux"
      ],
      "url": "https://arxiv.org/abs/2304.00129",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774617",
      "fetched_at": "2026-01-09T12:14:35.774618",
      "classified_at": "2026-01-09T13:26:14.215699",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_bbdae0dd": {
      "paper_id": "seed_bbdae0dd",
      "title": "Protecting Label Distribution in Cross-Silo Federated Learning",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a113/1Ub23mqt0hG",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774620",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2303.12233": {
      "paper_id": "2303.12233",
      "title": "LOKI: Large-scale Data Reconstruction Attack against Federated Learning through Model Manipulation",
      "abstract": "Federated learning was introduced to enable machine learning over large decentralized datasets while promising privacy by eliminating the need for data sharing. Despite this, prior work has shown that shared gradients often contain private information and attackers can gain knowledge either through malicious modification of the architecture and parameters or by using optimization to approximate user data from the shared gradients. However, prior data reconstruction attacks have been limited in setting and scale, as most works target FedSGD and limit the attack to single-client gradients. Many of these attacks fail in the more practical setting of FedAVG or if updates are aggregated together using secure aggregation. Data reconstruction becomes significantly more difficult, resulting in limited attack scale and/or decreased reconstruction quality. When both FedAVG and secure aggregation are used, there is no current method that is able to attack multiple clients concurrently in a federated learning setting. In this work we introduce LOKI, an attack that overcomes previous limitations and also breaks the anonymity of aggregation as the leaked data is identifiable and directly tied back to the clients they come from. Our design sends clients customized convolutional parameters, and the weight gradients of data points between clients remain separate even through aggregation. With FedAVG and aggregation across 100 clients, prior work can leak less than 1% of images on MNIST, CIFAR-100, and Tiny ImageNet. Using only a single training round, LOKI is able to leak 76-86% of all data samples.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Joshua C. Zhao",
        "Atul Sharma",
        "Ahmed Roushdy Elkordy",
        "Yahya H. Ezzeldin",
        "Salman Avestimehr",
        "Saurabh Bagchi"
      ],
      "url": "https://arxiv.org/abs/2303.12233",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774623",
      "fetched_at": "2026-01-09T12:14:35.774624",
      "classified_at": "2026-01-09T13:26:14.821136",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2408.16913": {
      "paper_id": "2408.16913",
      "title": "Analyzing Inference Privacy Risks Through Gradients In Machine Learning",
      "abstract": "In distributed learning settings, models are iteratively updated with shared gradients computed from potentially sensitive user data. While previous work has studied various privacy risks of sharing gradients, our paper aims to provide a systematic approach to analyze private information leakage from gradients. We present a unified game-based framework that encompasses a broad range of attacks including attribute, property, distributional, and user disclosures. We investigate how different uncertainties of the adversary affect their inferential power via extensive experiments on five datasets across various data modalities. Our results demonstrate the inefficacy of solely relying on data aggregation to achieve privacy against inference attacks in distributed learning. We further evaluate five types of defenses, namely, gradient pruning, signed gradient descent, adversarial perturbations, variational information bottleneck, and differential privacy, under both static and adaptive adversary settings. We provide an information-theoretic view for analyzing the effectiveness of these defenses against inference from gradients. Finally, we introduce a method for auditing attribute inference privacy, improving the empirical estimation of worst-case privacy through crafting adversarial canary records.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Zhuohang Li",
        "Andrew Lowy",
        "Jing Liu",
        "Toshiaki Koike-Akino",
        "Kieran Parsons",
        "Bradley Malin",
        "Ye Wang"
      ],
      "url": "https://arxiv.org/abs/2408.16913",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774627",
      "fetched_at": "2026-01-09T12:14:35.774628",
      "classified_at": "2026-01-09T13:26:15.559894",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_1a68fdb2": {
      "paper_id": "seed_1a68fdb2",
      "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings",
      "abstract": "Federated learning (FL) enables collaborative model training among multiple clients without the need to expose raw data. Its ability to safeguard privacy, at the heart of FL, has recently been a hot-button debate topic. To elaborate, several studies have introduced a type of attacks known as gradient leakage attacks (GLAs), which exploit the gradients shared during training to reconstruct clients' raw data. On the flip side, some literature, however, contends no substantial privacy risk in practical FL environments due to the effectiveness of such GLAs being limited to overly relaxed conditions, such as small batch sizes and knowledge of clients' data distributions. This paper bridges this critical gap by empirically demonstrating that clients' data can still be effectively reconstructed, even within realistic FL environments. Upon revisiting GLAs, we recognize that their performance failures stem from their inability to handle the gradient matching problem. To alleviate the performance bottlenecks identified above, we develop FedLeak, which introduces two novel techniques, partial gradient matching and gradient regularization. Moreover, to evaluate the performance of FedLeak in real-world FL environments, we formulate a practical evaluation protocol grounded in a thorough review of extensive FL literature and industry practices. Under this protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby underscoring the significant vulnerability in FL systems and the urgent need for more effective defense methods.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Mingyuan Fan",
        "Fuyi Wang",
        "Cen Chen",
        "Jianying Zhou"
      ],
      "url": "https://openalex.org/W4417256413",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774631",
      "fetched_at": "2026-01-09T13:54:10.159027",
      "classified_at": "2026-01-09T14:00:35.647843",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4417256413",
      "doi": "https://doi.org/10.48550/arxiv.2506.08435"
    },
    "seed_5d66c988": {
      "paper_id": "seed_5d66c988",
      "title": "Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning",
      "abstract": "Recent works have brought attention to the vulnerability of Federated Learning (FL) systems to gradient leakage attacks. Such attacks exploit clients' uploaded gradients to reconstruct their sensitive data, thereby compromising the privacy protection capability of FL. In response, various defense mechanisms have been proposed to mitigate this threat by manipulating the uploaded gradients. Unfortunately, empirical evaluations have demonstrated limited resilience of these defenses against sophisticated attacks, indicating an urgent need for more effective defenses. In this paper, we explore a novel defensive paradigm that departs from conventional gradient perturbation approaches and instead focuses on the construction of robust data. Intuitively, if robust data exhibits low semantic similarity with clients' raw data, the gradients associated with robust data can effectively obfuscate attackers. To this end, we design Refiner that jointly optimizes two metrics for privacy protection and performance maintenance. The utility metric is designed to promote consistency between the gradients of key parameters associated with robust data and those derived from clients' data, thus maintaining model performance. Furthermore, the privacy metric guides the generation of robust data towards enlarging the semantic gap with clients' data. Theoretical analysis supports the effectiveness of Refiner, and empirical evaluations on multiple benchmark datasets demonstrate the superior defense effectiveness of Refiner at defending against state-of-the-art attacks.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Mingyuan Fan",
        "Cen Chen",
        "Chengyu Wang",
        "Wenmeng Zhou",
        "Jun Huang",
        "Ximeng Liu",
        "Wenzhong Guo"
      ],
      "url": "https://openalex.org/W4310827164",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774633",
      "fetched_at": "2026-01-09T13:54:10.918821",
      "classified_at": "2026-01-09T14:00:37.474577",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4310827164",
      "doi": "https://doi.org/10.48550/arxiv.2212.02042"
    },
    "seed_908ad2d1": {
      "paper_id": "seed_908ad2d1",
      "title": "Aion: Robust and Efficient Multi-Round Single-Mask Secure Aggregation Against Malicious Participants",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-liu-yizhong.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774637",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_c09c2704": {
      "paper_id": "seed_c09c2704",
      "title": "SoK: On Gradient Leakage in Federated Learning",
      "abstract": "Federated learning (FL) facilitates collaborative model training among multiple clients without raw data exposure. However, recent studies have shown that clients' private training data can be reconstructed from shared gradients in FL, a vulnerability known as gradient inversion attacks (GIAs). While GIAs have demonstrated effectiveness under \\emph{ideal settings and auxiliary assumptions}, their actual efficacy against \\emph{practical FL systems} remains under-explored. To address this gap, we conduct a comprehensive study on GIAs in this work. We start with a survey of GIAs that establishes a timeline to trace their evolution and develops a systematization to uncover their inherent threats. By rethinking GIA in practical FL systems, three fundamental aspects influencing GIA's effectiveness are identified: \\textit{training setup}, \\textit{model}, and \\textit{post-processing}. Guided by these aspects, we perform extensive theoretical and empirical evaluations of SOTA GIAs across diverse settings. Our findings highlight that GIA is notably \\textit{constrained}, \\textit{fragile}, and \\textit{easily defensible}. Specifically, GIAs exhibit inherent limitations against practical local training settings. Additionally, their effectiveness is highly sensitive to the trained model, and even simple post-processing techniques applied to gradients can serve as effective defenses. Our work provides crucial insights into the limited threats of GIAs in practical FL systems. By rectifying prior misconceptions, we hope to inspire more accurate and realistic investigations on this topic.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jiacheng Du",
        "Jiahui Hu",
        "Zhibo Wang",
        "Peng Sun",
        "Neil Zhenqiang Gong",
        "Kui Ren"
      ],
      "url": "https://openalex.org/W4394642568",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774640",
      "fetched_at": "2026-01-09T13:54:12.005008",
      "classified_at": "2026-01-09T14:00:39.293664",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4394642568",
      "doi": "https://doi.org/10.48550/arxiv.2404.05403"
    },
    "seed_15a4851d": {
      "paper_id": "seed_15a4851d",
      "title": "DP-BREM: Differentially-Private and Byzantine-Robust Federated Learning with Client Momentum",
      "abstract": "Federated Learning (FL) allows multiple participating clients to train machine learning models collaboratively while keeping their datasets local and only exchanging the gradient or model updates with a coordinating server. Existing FL protocols are vulnerable to attacks that aim to compromise data privacy and/or model robustness. Recently proposed defenses focused on ensuring either privacy or robustness, but not both. In this paper, we focus on simultaneously achieving differential privacy (DP) and Byzantine robustness for cross-silo FL, based on the idea of learning from history. The robustness is achieved via client momentum, which averages the updates of each client over time, thus reducing the variance of the honest clients and exposing the small malicious perturbations of Byzantine clients that are undetectable in a single round but accumulate over time. In our initial solution DP-BREM, DP is achieved by adding noise to the aggregated momentum, and we account for the privacy cost from the momentum, which is different from the conventional DP-SGD that accounts for the privacy cost from the gradient. Since DP-BREM assumes a trusted server (who can obtain clients' local models or updates), we further develop the final solution called DP-BREM+, which achieves the same DP and robustness properties as DP-BREM without a trusted server by utilizing secure aggregation techniques, where DP noise is securely and jointly generated by the clients. Both theoretical analysis and experimental results demonstrate that our proposed protocols achieve better privacy-utility tradeoff and stronger Byzantine robustness than several baseline methods, under different DP budgets and attack settings.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Xiaolan Gu",
        "Ming Li",
        "Li Xiong"
      ],
      "url": "https://openalex.org/W4381826963",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774648",
      "fetched_at": "2026-01-09T13:54:12.560162",
      "classified_at": "2026-01-09T14:00:41.206036",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4381826963",
      "doi": "https://doi.org/10.48550/arxiv.2306.12608"
    },
    "seed_22da7c14": {
      "paper_id": "seed_22da7c14",
      "title": "SLOTHE : Lazy Approximation of Non-Arithmetic Neural Network Functions over Encrypted Data",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-nam-slothe.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774652",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_a056458d": {
      "paper_id": "seed_a056458d",
      "title": "Sharpness-Aware Initialization: Improving Differentially Private Machine Learning from First Principles",
      "abstract": "Artificial intelligence research is ushering in a new era of sophisticated, mass-market transportation technology. While computers can already fly a passenger jet better than a trained human pilot, people are still faced with the dangerous yet tedious task of driving automobiles. Intelligent Transportation Systems (ITS) is the field that focuses on integrating information technology with vehicles and transportation infrastructure to make transportation safer, cheaper, and more efficient. Recent advances in ITS point to a future in which vehicles themselves handle the vast majority of the driving task. Once autonomous vehicles become popular, autonomous interactions amongst multiple vehicles will be possible. Current methods of vehicle coordination, which are all designed to work with human drivers, will be outdated. The bottleneck for roadway efficiency will no longer be the drivers, but rather the mechanism by which those drivers' actions are coordinated. While open-road driving is a well-studied and more-or-less-solved problem, urban traffic scenarios, especially intersections, are much more challenging. We believe current methods for controlling traffic, specifically at intersections, will not be able to take advantage of the increased sensitivity and precision of autonomous vehicles as compared to human drivers. In this article, we suggest an alternative mechanism for coordinating the movement of autonomous vehicles through intersections. Drivers and intersections in this mechanism are treated as autonomous agents in a multiagent system. In this multiagent system, intersections use a new reservation-based approach built around a detailed communication protocol, which we also present. We demonstrate in simulation that our new mechanism has the potential to significantly outperform current intersection control technology -- traffic lights and stop signs. Because our mechanism can emulate a traffic light or stop sign, it subsumes the most popular current methods of intersection control. This article also presents two extensions to the mechanism. The first extension allows the system to control human-driven vehicles in addition to autonomous vehicles. The second gives priority to emergency vehicles without significant cost to civilian vehicles. The mechanism, including both extensions, is implemented and tested in simulation, and we present experimental results that strongly attest to the efficacy of this approach.",
      "year": 2008,
      "venue": "Journal of Artificial Intelligence Research",
      "authors": [
        "Kurt Dresner",
        "Peter Stone"
      ],
      "url": "https://openalex.org/W2137514195",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774655",
      "fetched_at": "2026-01-09T13:54:13.891374",
      "classified_at": "2026-01-09T14:00:43.107693",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2137514195",
      "doi": "https://doi.org/10.1613/jair.2502"
    },
    "seed_a823e69e": {
      "paper_id": "seed_a823e69e",
      "title": "Task-Oriented Training Data Privacy Protection for Cloud-based Model Training",
      "abstract": "The Internet of Things (IoT) makes smart objects the ultimate building blocks in the development of cyber-physical smart pervasive frameworks. The IoT has a variety of application domains, including health care. The IoT revolution is redesigning modern health care with promising technological, economic, and social prospects. This paper surveys advances in IoT-based health care technologies and reviews the state-of-the-art network architectures/platforms, applications, and industrial trends in IoT-based health care solutions. In addition, this paper analyzes distinct IoT security and privacy features, including security requirements, threat models, and attack taxonomies from the health care perspective. Further, this paper proposes an intelligent collaborative security model to minimize security risk; discusses how different innovations such as big data, ambient intelligence, and wearables can be leveraged in a health care context; addresses various IoT and eHealth policies and regulations across the world to determine how they can facilitate economies and societies in terms of sustainable development; and provides some avenues for future research on IoT-based health care based on a set of open issues and challenges.",
      "year": 2015,
      "venue": "IEEE Access",
      "authors": [
        "S. M. Riazul Islam",
        "Daehan Kwak",
        "Md. Humaun Kabir",
        "Mahmud Hossain",
        "Kyung Sup Kwak"
      ],
      "url": "https://openalex.org/W1943579973",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774657",
      "fetched_at": "2026-01-09T13:54:14.798127",
      "classified_at": "2026-01-09T14:00:44.901367",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W1943579973",
      "doi": "https://doi.org/10.1109/access.2015.2437951"
    },
    "seed_5f19093b": {
      "paper_id": "seed_5f19093b",
      "title": "From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning",
      "abstract": "Data Reconstruction Attacks (DRA) pose a significant threat to Federated Learning (FL) systems by enabling adversaries to infer sensitive training data from local clients. Despite extensive research, the question of how to characterize and assess the risk of DRAs in FL systems remains unresolved due to the lack of a theoretically-grounded risk quantification framework. In this work, we address this gap by introducing Invertibility Loss (InvLoss) to quantify the maximum achievable effectiveness of DRAs for a given data instance and FL model. We derive a tight and computable upper bound for InvLoss and explore its implications from three perspectives. First, we show that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods. Second, we develop InvRE, an InvLoss-based DRA risk estimator that offers attack method-agnostic, comprehensive risk evaluation across data instances and model architectures. Third, we propose two adaptive noise perturbation defenses that enhance FL privacy without harming classification accuracy. Extensive experiments on real-world datasets validate our framework, demonstrating its potential for systematic DRA risk evaluation and mitigation in FL systems.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Xu, Xiangrui",
        "Li, Zhize",
        "Han, Yufei",
        "Wang, Bin",
        "Liu, Jiqiang",
        "Wang, Wei"
      ],
      "url": "https://openalex.org/W7116111594",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774660",
      "fetched_at": "2026-01-09T13:54:15.573397",
      "classified_at": "2026-01-09T14:00:46.714081",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W7116111594",
      "doi": "https://doi.org/10.48550/arxiv.2512.15460"
    },
    "seed_22a31921": {
      "paper_id": "seed_22a31921",
      "title": "SoK: Gradient Inversion Attacks in Federated Learning",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-carletti.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774663",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_fa4cdd9d": {
      "paper_id": "seed_fa4cdd9d",
      "title": "Privacy Risks of General-Purpose Language Models",
      "abstract": "Recently, a new paradigm of building general-purpose language models (e.g., Google's Bert and OpenAI's GPT-2) in Natural Language Processing (NLP) for text feature extraction, a standard procedure in NLP systems that converts texts to vectors (i.e., embeddings) for downstream modeling, has arisen and starts to find its application in various downstream NLP tasks and real world systems (e.g., Google's search engine [6]). To obtain general-purpose text embeddings, these language models have highly complicated architectures with millions of learnable parameters and are usually pretrained on billions of sentences before being utilized. As is widely recognized, such a practice indeed improves the state-of-the-art performance of many downstream NLP tasks. However, the improved utility is not for free. We find the text embeddings from general-purpose language models would capture much sensitive information from the plain text. Once being accessed by the adversary, the embeddings can be reverse-engineered to disclose sensitive information of the victims for further harassment. Although such a privacy risk can impose a real threat to the future leverage of these promising NLP tools, there are neither published attacks nor systematic evaluations by far for the mainstream industry-level language models. To bridge this gap, we present the first systematic study on the privacy risks of 8 state-of-the-art language models with 4 diverse case studies. By constructing 2 novel attack classes, our study demonstrates the aforementioned privacy risks do exist and can impose practical threats to the application of general-purpose language models on sensitive data covering identity, genome, healthcare and location. For example, we show the adversary with nearly no prior knowledge can achieve about 75% accuracy when inferring the precise disease site from Bert embeddings of patients' medical descriptions. As possible countermeasures, we propose 4 different defenses (via rounding, differential privacy, adversarial training and subspace projection) to obfuscate the unprotected embeddings for mitigation purpose. With extensive evaluations, we also provide a preliminary analysis on the utility-privacy trade-off brought by each defense, which we hope may foster future mitigation researches.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Xudong Pan",
        "Mi Zhang",
        "Shouling Ji",
        "Min Yang"
      ],
      "url": "https://openalex.org/W3046764764",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774666",
      "fetched_at": "2026-01-09T13:54:17.140309",
      "classified_at": "2026-01-09T14:00:48.785588",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3046764764",
      "doi": "https://doi.org/10.1109/sp40000.2020.00095"
    },
    "2004.00053": {
      "paper_id": "2004.00053",
      "title": "Information Leakage in Embedding Models",
      "abstract": "Embeddings are functions that map raw input data to low-dimensional vector representations, while preserving important semantic information about the inputs. Pre-training embeddings on a large amount of unlabeled data and fine-tuning them for downstream tasks is now a de facto standard in achieving state of the art learning in many domains.   We demonstrate that embeddings, in addition to encoding generic semantics, often also present a vector that leaks sensitive information about the input data. We develop three classes of attacks to systematically study information that might be leaked by embeddings. First, embedding vectors can be inverted to partially recover some of the input data. As an example, we show that our attacks on popular sentence embeddings recover between 50\\%--70\\% of the input words (F1 scores of 0.5--0.7). Second, embeddings may reveal sensitive attributes inherent in inputs and independent of the underlying semantic task at hand. Attributes such as authorship of text can be easily extracted by training an inference model on just a handful of labeled embedding vectors. Third, embedding models leak moderate amount of membership information for infrequent training data inputs. We extensively evaluate our attacks on various state-of-the-art embedding models in the text domain. We also propose and evaluate defenses that can prevent the leakage to some extent at a minor cost in utility.",
      "year": 2020,
      "venue": "ACM CCS",
      "authors": [
        "Congzheng Song",
        "Ananth Raghunathan"
      ],
      "url": "https://arxiv.org/abs/2004.00053",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774669",
      "fetched_at": "2026-01-09T12:14:35.774670",
      "classified_at": "2026-01-09T13:26:16.273810",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2105.12049": {
      "paper_id": "2105.12049",
      "title": "Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs",
      "abstract": "It is known that deep neural networks, trained for the classification of non-sensitive target attributes, can reveal sensitive attributes of their input data through internal representations extracted by the classifier. We take a step forward and show that deep classifiers can be trained to secretly encode a sensitive attribute of their input data into the classifier's outputs for the target attribute, at inference time. Our proposed attack works even if users have a full white-box view of the classifier, can keep all internal representations hidden, and only release the classifier's estimations for the target attribute. We introduce an information-theoretical formulation for such attacks and present efficient empirical implementations for training honest-but-curious (HBC) classifiers: classifiers that can be accurate in predicting their target attribute, but can also exploit their outputs to secretly encode a sensitive attribute. Our work highlights a vulnerability that can be exploited by malicious machine learning service providers to attack their user's privacy in several seemingly safe scenarios; such as encrypted inferences, computations at the edge, or private knowledge distillation. Experimental results on several attributes in two face-image datasets show that a semi-trusted server can train classifiers that are not only perfectly honest but also accurately curious. We conclude by showing the difficulties in distinguishing between standard and HBC classifiers, discussing challenges in defending against this vulnerability of deep classifiers, and enumerating related open directions for future studies.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Mohammad Malekzadeh",
        "Anastasia Borovykh",
        "Deniz G\u00fcnd\u00fcz"
      ],
      "url": "https://arxiv.org/abs/2105.12049",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774674",
      "fetched_at": "2026-01-09T12:14:35.774675",
      "classified_at": "2026-01-09T13:26:16.892030",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_e8f3ac3f": {
      "paper_id": "seed_e8f3ac3f",
      "title": "Stealing Links from Graph Neural Networks",
      "abstract": "It is known that deep neural networks, trained for the classification of non-sensitive target attributes, can reveal sensitive attributes of their input data through internal representations extracted by the classifier. We take a step forward and show that deep classifiers can be trained to secretly encode a sensitive attribute of their input data into the classifier's outputs for the target attribute, at inference time. Our proposed attack works even if users have a full white-box view of the classifier, can keep all internal representations hidden, and only release the classifier's estimations for the target attribute. We introduce an information-theoretical formulation for such attacks and present efficient empirical implementations for training honest-but-curious (HBC) classifiers: classifiers that can be accurate in predicting their target attribute, but can also exploit their outputs to secretly encode a sensitive attribute. Our work highlights a vulnerability that can be exploited by malicious machine learning service providers to attack their user's privacy in several seemingly safe scenarios; such as encrypted inferences, computations at the edge, or private knowledge distillation. Experimental results on several attributes in two face-image datasets show that a semi-trusted server can train classifiers that are not only perfectly honest but also accurately curious. We conclude by showing the difficulties in distinguishing between standard and HBC classifiers, discussing challenges in defending against this vulnerability of deep classifiers, and enumerating related open directions for future studies.",
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [
        "Mohammad Malekzadeh",
        "Anastasia Borovykh",
        "Deniz G\u00fcnd\u00fcz"
      ],
      "url": "https://arxiv.org/abs/2105.12049",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774677",
      "fetched_at": "2026-01-09T13:54:17.694086",
      "classified_at": "2026-01-09T14:00:51.336450",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "2105.12049"
    },
    "seed_e8af6e93": {
      "paper_id": "seed_e8af6e93",
      "title": "Inference Attacks Against Graph Neural Networks",
      "abstract": "Many real-world data comes in the form of graphs, such as social networks and protein structure. To fully utilize the information contained in graph data, a new family of machine learning (ML) models, namely graph neural networks (GNNs), has been introduced. Previous studies have shown that machine learning models are vulnerable to privacy attacks. However, most of the current efforts concentrate on ML models trained on data from the Euclidean space, like images and texts. On the other hand, privacy risks stemming from GNNs remain largely unstudied. In this paper, we fill the gap by performing the first comprehensive analysis of node-level membership inference attacks against GNNs. We systematically define the threat models and propose three node-level membership inference attacks based on an adversary's background knowledge. Our evaluation on three GNN structures and four benchmark datasets shows that GNNs are vulnerable to node-level membership inference even when the adversary has minimal background knowledge. Besides, we show that graph density and feature similarity have a major impact on the attack's success. We further investigate two defense mechanisms and the empirical results indicate that these defenses can reduce the attack performance but with moderate utility loss.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Xinlei He",
        "Rui Wen",
        "Yixin Wu",
        "Michael Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W3126787694",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774680",
      "fetched_at": "2026-01-09T13:54:18.479015",
      "classified_at": "2026-01-09T14:00:55.240999",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3126787694",
      "doi": "https://doi.org/10.48550/arxiv.2102.05429"
    },
    "2108.06504": {
      "paper_id": "2108.06504",
      "title": "LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis",
      "abstract": "Graph structured data have enabled several successful applications such as recommendation systems and traffic prediction, given the rich node features and edges information. However, these high-dimensional features and high-order adjacency information are usually heterogeneous and held by different data holders in practice. Given such vertical data partition (e.g., one data holder will only own either the node features or edge information), different data holders have to develop efficient joint training protocols rather than directly transfer data to each other due to privacy concerns. In this paper, we focus on the edge privacy, and consider a training scenario where Bob with node features will first send training node features to Alice who owns the adjacency information. Alice will then train a graph neural network (GNN) with the joint information and release an inference API. During inference, Bob is able to provide test node features and query the API to obtain the predictions for test nodes. Under this setting, we first propose a privacy attack LinkTeller via influence analysis to infer the private edge information held by Alice via designing adversarial queries for Bob. We then empirically show that LinkTeller is able to recover a significant amount of private edges, outperforming existing baselines. To further evaluate the privacy leakage, we adapt an existing algorithm for differentially private graph convolutional network (DP GCN) training and propose a new DP GCN mechanism LapGraph. We show that these DP GCN mechanisms are not always resilient against LinkTeller empirically under mild privacy guarantees ($\\varepsilon>5$). Our studies will shed light on future research towards designing more resilient privacy-preserving GCN models; in the meantime, provide an in-depth understanding of the tradeoff between GCN model utility and robustness against potential privacy attacks.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Fan Wu",
        "Yunhui Long",
        "Ce Zhang",
        "Bo Li"
      ],
      "url": "https://arxiv.org/abs/2108.06504",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774683",
      "fetched_at": "2026-01-09T12:14:35.774684",
      "classified_at": "2026-01-09T13:26:17.510760",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2006.05535": {
      "paper_id": "2006.05535",
      "title": "Locally Private Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated superior performance in learning node representations for various graph inference tasks. However, learning over graph data can raise privacy concerns when nodes represent people or human-related variables that involve sensitive or personal information. While numerous techniques have been proposed for privacy-preserving deep learning over non-relational data, there is less work addressing the privacy issues pertained to applying deep learning algorithms on graphs. In this paper, we study the problem of node data privacy, where graph nodes have potentially sensitive data that is kept private, but they could be beneficial for a central server for training a GNN over the graph. To address this problem, we develop a privacy-preserving, architecture-agnostic GNN learning algorithm with formal privacy guarantees based on Local Differential Privacy (LDP). Specifically, we propose an LDP encoder and an unbiased rectifier, by which the server can communicate with the graph nodes to privately collect their data and approximate the GNN's first layer. To further reduce the effect of the injected noise, we propose to prepend a simple graph convolution layer, called KProp, which is based on the multi-hop aggregation of the nodes' features acting as a denoising mechanism. Finally, we propose a robust training framework, in which we benefit from KProp's denoising capability to increase the accuracy of inference in the presence of noisy labels. Extensive experiments conducted over real-world datasets demonstrate that our method can maintain a satisfying level of accuracy with low privacy loss.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Sina Sajadmanesh",
        "Daniel Gatica-Perez"
      ],
      "url": "https://arxiv.org/abs/2006.05535",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774687",
      "fetched_at": "2026-01-09T12:14:35.774688",
      "classified_at": "2026-01-09T13:26:18.151947",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2204.06963": {
      "paper_id": "2204.06963",
      "title": "Finding MNEMON: Reviving Memories of Node Embeddings",
      "abstract": "Previous security research efforts orbiting around graphs have been exclusively focusing on either (de-)anonymizing the graphs or understanding the security and privacy issues of graph neural networks. Little attention has been paid to understand the privacy risks of integrating the output from graph embedding models (e.g., node embeddings) with complex downstream machine learning pipelines. In this paper, we fill this gap and propose a novel model-agnostic graph recovery attack that exploits the implicit graph structural information preserved in the embeddings of graph nodes. We show that an adversary can recover edges with decent accuracy by only gaining access to the node embedding matrix of the original graph without interactions with the node embedding models. We demonstrate the effectiveness and applicability of our graph recovery attack through extensive experiments.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Yun Shen",
        "Yufei Han",
        "Zhikun Zhang",
        "Min Chen",
        "Ting Yu",
        "Michael Backes",
        "Yang Zhang",
        "Gianluca Stringhini"
      ],
      "url": "https://arxiv.org/abs/2204.06963",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774690",
      "fetched_at": "2026-01-09T12:14:35.774691",
      "classified_at": "2026-01-09T13:26:18.774980",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2209.01100": {
      "paper_id": "2209.01100",
      "title": "Group Property Inference Attacks Against Graph Neural Networks",
      "abstract": "With the fast adoption of machine learning (ML) techniques, sharing of ML models is becoming popular. However, ML models are vulnerable to privacy attacks that leak information about the training data. In this work, we focus on a particular type of privacy attacks named property inference attack (PIA) which infers the sensitive properties of the training data through the access to the target ML model. In particular, we consider Graph Neural Networks (GNNs) as the target model, and distribution of particular groups of nodes and links in the training graph as the target property. While the existing work has investigated PIAs that target at graph-level properties, no prior works have studied the inference of node and link properties at group level yet.   In this work, we perform the first systematic study of group property inference attacks (GPIA) against GNNs. First, we consider a taxonomy of threat models under both black-box and white-box settings with various types of adversary knowledge, and design six different attacks for these settings. We evaluate the effectiveness of these attacks through extensive experiments on three representative GNN models and three real-world graphs. Our results demonstrate the effectiveness of these attacks whose accuracy outperforms the baseline approaches. Second, we analyze the underlying factors that contribute to GPIA's success, and show that the target model trained on the graphs with or without the target property represents some dissimilarity in model parameters and/or model outputs, which enables the adversary to infer the existence of the property. Further, we design a set of defense mechanisms against the GPIA attacks, and demonstrate that these mechanisms can reduce attack accuracy effectively with small loss on GNN model accuracy.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Xiuling Wang",
        "Wendy Hui Wang"
      ],
      "url": "https://arxiv.org/abs/2209.01100",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774694",
      "fetched_at": "2026-01-09T12:14:35.774695",
      "classified_at": "2026-01-09T13:26:19.403384",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2205.03105": {
      "paper_id": "2205.03105",
      "title": "LPGNet: Link Private Graph Networks for Node Classification",
      "abstract": "Classification tasks on labeled graph-structured data have many important applications ranging from social recommendation to financial modeling. Deep neural networks are increasingly being used for node classification on graphs, wherein nodes with similar features have to be given the same label. Graph convolutional networks (GCNs) are one such widely studied neural network architecture that perform well on this task. However, powerful link-stealing attacks on GCNs have recently shown that even with black-box access to the trained model, inferring which links (or edges) are present in the training graph is practical. In this paper, we present a new neural network architecture called LPGNet for training on graphs with privacy-sensitive edges. LPGNet provides differential privacy (DP) guarantees for edges using a novel design for how graph edge structure is used during training. We empirically show that LPGNet models often lie in the sweet spot between providing privacy and utility: They can offer better utility than \"trivially\" private architectures which use no edge information (e.g., vanilla MLPs) and better resilience against existing link-stealing attacks than vanilla GCNs which use the full edge structure. LPGNet also offers consistently better privacy-utility tradeoffs than DPGCN, which is the state-of-the-art mechanism for retrofitting differential privacy into conventional GCNs, in most of our evaluated datasets.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Aashish Kolluri",
        "Teodora Baluta",
        "Bryan Hooi",
        "Prateek Saxena"
      ],
      "url": "https://arxiv.org/abs/2205.03105",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774698",
      "fetched_at": "2026-01-09T12:14:35.774699",
      "classified_at": "2026-01-09T13:26:20.187903",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2312.07861": {
      "paper_id": "2312.07861",
      "title": "GraphGuard: Detecting and Counteracting Training Data Misuse in Graph Neural Networks",
      "abstract": "The emergence of Graph Neural Networks (GNNs) in graph data analysis and their deployment on Machine Learning as a Service platforms have raised critical concerns about data misuse during model training. This situation is further exacerbated due to the lack of transparency in local training processes, potentially leading to the unauthorized accumulation of large volumes of graph data, thereby infringing on the intellectual property rights of data owners. Existing methodologies often address either data misuse detection or mitigation, and are primarily designed for local GNN models rather than cloud-based MLaaS platforms. These limitations call for an effective and comprehensive solution that detects and mitigates data misuse without requiring exact training data while respecting the proprietary nature of such data. This paper introduces a pioneering approach called GraphGuard, to tackle these challenges. We propose a training-data-free method that not only detects graph data misuse but also mitigates its impact via targeted unlearning, all without relying on the original training data. Our innovative misuse detection technique employs membership inference with radioactive data, enhancing the distinguishability between member and non-member data distributions. For mitigation, we utilize synthetic graphs that emulate the characteristics previously learned by the target model, enabling effective unlearning even in the absence of exact graph data. We conduct comprehensive experiments utilizing four real-world graph datasets to demonstrate the efficacy of GraphGuard in both detection and unlearning. We show that GraphGuard attains a near-perfect detection rate of approximately 100% across these datasets with various GNN models. In addition, it performs unlearning by eliminating the impact of the unlearned graph with a marginal decrease in accuracy (less than 5%).",
      "year": 2024,
      "venue": "MDSS",
      "authors": [
        "Bang Wu",
        "He Zhang",
        "Xiangwen Yang",
        "Shuo Wang",
        "Minhui Xue",
        "Shirui Pan",
        "Xingliang Yuan"
      ],
      "url": "https://arxiv.org/abs/2312.07861",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774702",
      "fetched_at": "2026-01-09T12:14:35.774703",
      "classified_at": "2026-01-09T13:26:21.080704",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2501.10985": {
      "paper_id": "2501.10985",
      "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
      "abstract": "Graph neural networks (GNNs) have exhibited superior performance in various classification tasks on graph-structured data. However, they encounter the potential vulnerability from the link stealing attacks, which can infer the presence of a link between two nodes via measuring the similarity of its incident nodes' prediction vectors produced by a GNN model. Such attacks pose severe security and privacy threats to the training graph used in GNN models. In this work, we propose a novel solution, called Graph Link Disguise (GRID), to defend against link stealing attacks with the formal guarantee of GNN model utility for retaining prediction accuracy. The key idea of GRID is to add carefully crafted noises to the nodes' prediction vectors for disguising adjacent nodes as n-hop indirect neighboring nodes. We take into account the graph topology and select only a subset of nodes (called core nodes) covering all links for adding noises, which can avert the noises offset and have the further advantages of reducing both the distortion loss and the computation cost. Our crafted noises can ensure 1) the noisy prediction vectors of any two adjacent nodes have their similarity level like that of two non-adjacent nodes and 2) the model prediction is unchanged to ensure zero utility loss. Extensive experiments on five datasets are conducted to show the effectiveness of our proposed GRID solution against different representative link-stealing attacks under transductive settings and inductive settings respectively, as well as two influence-based attacks. Meanwhile, it achieves a much better privacy-utility trade-off than existing methods when extended to GNNs.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Jiadong Lou",
        "Xu Yuan",
        "Rui Zhang",
        "Xingliang Yuan",
        "Neil Gong",
        "Nian-Feng Tzeng"
      ],
      "url": "https://arxiv.org/abs/2501.10985",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774705",
      "fetched_at": "2026-01-09T12:14:35.774706",
      "classified_at": "2026-01-09T13:26:21.716443",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "1912.03817": {
      "paper_id": "1912.03817",
      "title": "Machine Unlearning",
      "abstract": "Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult. We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning. Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63x, and 2.45x for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36x in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.",
      "year": 2020,
      "venue": "IEEE S&P",
      "authors": [
        "Lucas Bourtoule",
        "Varun Chandrasekaran",
        "Christopher A. Choquette-Choo",
        "Hengrui Jia",
        "Adelin Travers",
        "Baiwu Zhang",
        "David Lie",
        "Nicolas Papernot"
      ],
      "url": "https://arxiv.org/abs/1912.03817",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML05",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774710",
      "fetched_at": "2026-01-09T12:14:35.774711",
      "classified_at": "2026-01-09T13:26:22.492625",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2005.02205": {
      "paper_id": "2005.02205",
      "title": "When Machine Unlearning Jeopardizes Privacy",
      "abstract": "The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known as machine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. Our code is available at https://github.com/MinChen00/UnlearningLeaks.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Min Chen",
        "Zhikun Zhang",
        "Tianhao Wang",
        "Michael Backes",
        "Mathias Humbert",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2005.02205",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774714",
      "fetched_at": "2026-01-09T12:14:35.774715",
      "classified_at": "2026-01-09T13:26:23.347596",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2103.14991": {
      "paper_id": "2103.14991",
      "title": "Graph Unlearning",
      "abstract": "Machine unlearning is a process of removing the impact of some training data from the machine learning (ML) models upon receiving removal requests. While straightforward and legitimate, retraining the ML model from scratch incurs a high computational overhead. To address this issue, a number of approximate algorithms have been proposed in the domain of image and text data, among which SISA is the state-of-the-art solution. It randomly partitions the training set into multiple shards and trains a constituent model for each shard. However, directly applying SISA to the graph data can severely damage the graph structural information, and thereby the resulting ML model utility. In this paper, we propose GraphEraser, a novel machine unlearning framework tailored to graph data. Its contributions include two novel graph partition algorithms and a learning-based aggregation method. We conduct extensive experiments on five real-world graph datasets to illustrate the unlearning efficiency and model utility of GraphEraser. It achieves 2.06$\\times$ (small dataset) to 35.94$\\times$ (large dataset) unlearning time improvement. On the other hand, GraphEraser achieves up to $62.5\\%$ higher F1 score and our proposed learning-based aggregation method achieves up to $112\\%$ higher F1 score.\\footnote{Our code is available at \\url{https://github.com/MinChen00/Graph-Unlearning}.}",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Min Chen",
        "Zhikun Zhang",
        "Tianhao Wang",
        "Michael Backes",
        "Mathias Humbert",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2103.14991",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774718",
      "fetched_at": "2026-01-09T12:14:35.774719",
      "classified_at": "2026-01-09T13:26:23.983975",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_0e016306": {
      "paper_id": "seed_0e016306",
      "title": "On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning",
      "abstract": "Machine unlearning, i.e. having a model forget about some of its training data, has become increasingly more important as privacy legislation promotes variants of the right-to-be-forgotten. In the context of deep learning, approaches for machine unlearning are broadly categorized into two classes: exact unlearning methods, where an entity has formally removed the data point's impact on the model by retraining the model from scratch, and approximate unlearning, where an entity approximates the model parameters one would obtain by exact unlearning to save on compute costs. In this paper, we first show that the definition that underlies approximate unlearning, which seeks to prove the approximately unlearned model is close to an exactly retrained model, is incorrect because one can obtain the same model using different datasets. Thus one could unlearn without modifying the model at all. We then turn to exact unlearning approaches and ask how to verify their claims of unlearning. Our results show that even for a given training trajectory one cannot formally prove the absence of certain data points used during training. We thus conclude that unlearning is only well-defined at the algorithmic level, where an entity's only possible auditable claim to unlearning is that they used a particular algorithm designed to allow for external scrutiny during an audit.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Anvith Thudi",
        "Hengrui Jia",
        "Ilia Shumailov",
        "Nicolas Papernot"
      ],
      "url": "https://openalex.org/W3208439705",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774721",
      "fetched_at": "2026-01-09T13:54:19.243629",
      "classified_at": "2026-01-09T14:00:57.431602",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3208439705",
      "doi": "https://doi.org/10.48550/arxiv.2110.11891"
    },
    "seed_c6706fde": {
      "paper_id": "seed_c6706fde",
      "title": "Machine Unlearning of Features and Labels",
      "abstract": "Removing information from a machine learning model is a non-trivial task that requires to partially revert the training process.This task is unavoidable when sensitive data, such as credit card numbers or passwords, accidentally enter the model and need to be removed afterwards.Recently, different concepts for machine unlearning have been proposed to address this problem.While these approaches are effective in removing individual data points, they do not scale to scenarios where larger groups of features and labels need to be reverted.In this paper, we propose the first method for unlearning features and labels.Our approach builds on the concept of influence functions and realizes unlearning through closed-form updates of model parameters.It enables to adapt the influence of training data on a learning model retrospectively, thereby correcting data leaks and privacy issues.For learning models with strongly convex loss functions, our method provides certified unlearning with theoretical guarantees.For models with non-convex losses, we empirically show that the unlearning of features and labels is effective and significantly faster than other strategies.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Alexander Warnecke",
        "Lukas Pirch",
        "Christian Wressnegger",
        "Konrad Rieck"
      ],
      "url": "https://openalex.org/W3193974325",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774724",
      "fetched_at": "2026-01-09T13:54:19.988068",
      "classified_at": "2026-01-09T14:01:00.150814",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3193974325",
      "doi": "https://doi.org/10.14722/ndss.2023.23087"
    },
    "2309.08230": {
      "paper_id": "2309.08230",
      "title": "A Duty to Forget, a Right to be Assured? Exposing Vulnerabilities in Machine Unlearning Services",
      "abstract": "The right to be forgotten requires the removal or \"unlearning\" of a user's data from machine learning models. However, in the context of Machine Learning as a Service (MLaaS), retraining a model from scratch to fulfill the unlearning request is impractical due to the lack of training data on the service provider's side (the server). Furthermore, approximate unlearning further embraces a complex trade-off between utility (model performance) and privacy (unlearning performance). In this paper, we try to explore the potential threats posed by unlearning services in MLaaS, specifically over-unlearning, where more information is unlearned than expected. We propose two strategies that leverage over-unlearning to measure the impact on the trade-off balancing, under black-box access settings, in which the existing machine unlearning attacks are not applicable. The effectiveness of these strategies is evaluated through extensive experiments on benchmark datasets, across various model architectures and representative unlearning approaches. Results indicate significant potential for both strategies to undermine model efficacy in unlearning scenarios. This study uncovers an underexplored gap between unlearning and contemporary MLaaS, highlighting the need for careful considerations in balancing data unlearning, model utility, and security.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Hongsheng Hu",
        "Shuo Wang",
        "Jiamin Chang",
        "Haonan Zhong",
        "Ruoxi Sun",
        "Shuang Hao",
        "Haojin Zhu",
        "Minhui Xue"
      ],
      "url": "https://arxiv.org/abs/2309.08230",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774727",
      "fetched_at": "2026-01-09T12:14:35.774728",
      "classified_at": "2026-01-09T13:26:24.628135",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2311.16136": {
      "paper_id": "2311.16136",
      "title": "ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach",
      "abstract": "Over the past years, Machine Learning-as-a-Service (MLaaS) has received a surging demand for supporting Machine Learning-driven services to offer revolutionized user experience across diverse application areas. MLaaS provides inference service with low inference latency based on an ML model trained using a dataset collected from numerous individual data owners. Recently, for the sake of data owners' privacy and to comply with the \"right to be forgotten (RTBF)\" as enacted by data protection legislation, many machine unlearning methods have been proposed to remove data owners' data from trained models upon their unlearning requests. However, despite their promising efficiency, almost all existing machine unlearning methods handle unlearning requests independently from inference requests, which unfortunately introduces a new security issue of inference service obsolescence and a privacy vulnerability of undesirable exposure for machine unlearning in MLaaS.   In this paper, we propose the ERASER framework for machinE unleaRning in MLaAS via an inferencE seRving-aware approach. ERASER strategically choose appropriate unlearning execution timing to address the inference service obsolescence issue. A novel inference consistency certification mechanism is proposed to avoid the violation of RTBF principle caused by postponed unlearning executions, thereby mitigating the undesirable exposure vulnerability. ERASER offers three groups of design choices to allow for tailor-made variants that best suit the specific environments and preferences of various MLaaS systems. Extensive empirical evaluations across various settings confirm ERASER's effectiveness, e.g., it can effectively save up to 99% of inference latency and 31% of computation overhead over the inference-oblivion baseline.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Yuke Hu",
        "Jian Lou",
        "Jiaqi Liu",
        "Wangze Ni",
        "Feng Lin",
        "Zhan Qin",
        "Kui Ren"
      ],
      "url": "https://arxiv.org/abs/2311.16136",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML05",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774731",
      "fetched_at": "2026-01-09T12:14:35.774732",
      "classified_at": "2026-01-09T13:26:25.521041",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_ef429cfe": {
      "paper_id": "seed_ef429cfe",
      "title": "Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A New Inference Attack Perspective",
      "abstract": "Machine unlearning focuses on efficiently removing specific data from trained models, addressing privacy and compliance concerns with reasonable costs. Although exact unlearning ensures complete data removal equivalent to retraining, it is impractical for large-scale models, leading to growing interest in inexact unlearning methods. However, the lack of formal guarantees in these methods necessitates the need for robust evaluation frameworks to assess their privacy and effectiveness. In this work, we first identify several key pitfalls of the existing unlearning evaluation frameworks, e.g., focusing on average-case evaluation or targeting random samples for evaluation, incomplete comparisons with the retraining baseline. Then, we propose RULI (Rectified Unlearning Evaluation Framework via Likelihood Inference), a novel framework to address critical gaps in the evaluation of inexact unlearning methods. RULI introduces a dual-objective attack to measure both unlearning efficacy and privacy risks at a per-sample granularity. Our findings reveal significant vulnerabilities in state-of-the-art unlearning methods, where RULI achieves higher attack success rates, exposing privacy risks underestimated by existing methods. Built on a game-based foundation and validated through empirical evaluations on both image and text data (spanning tasks from classification to generation), RULI provides a rigorous, scalable, and fine-grained methodology for evaluating unlearning techniques.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Nima Naderloui",
        "Shenao Yan",
        "Binghui Wang",
        "J. Fu",
        "Hui Wang",
        "Weiran Liu",
        "Yuan Hong"
      ],
      "url": "https://openalex.org/W4415125319",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774734",
      "fetched_at": "2026-01-09T13:54:20.508176",
      "classified_at": "2026-01-09T14:01:02.050018",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4415125319",
      "doi": "https://doi.org/10.48550/arxiv.2506.13009"
    },
    "seed_7adf2996": {
      "paper_id": "seed_7adf2996",
      "title": "Data Duplication: A Novel Multi-Purpose Attack Paradigm in Machine Unlearning",
      "abstract": "Duplication is a prevalent issue within datasets. Existing research has demonstrated that the presence of duplicated data in training datasets can significantly influence both model performance and data privacy. However, the impact of data duplication on the unlearning process remains largely unexplored. This paper addresses this gap by pioneering a comprehensive investigation into the role of data duplication, not only in standard machine unlearning but also in federated and reinforcement unlearning paradigms. Specifically, we propose an adversary who duplicates a subset of the target model's training set and incorporates it into the training set. After training, the adversary requests the model owner to unlearn this duplicated subset, and analyzes the impact on the unlearned model. For example, the adversary can challenge the model owner by revealing that, despite efforts to unlearn it, the influence of the duplicated subset remains in the model. Moreover, to circumvent detection by de-duplication techniques, we propose three novel near-duplication methods for the adversary, each tailored to a specific unlearning paradigm. We then examine their impacts on the unlearning process when de-duplication techniques are applied. Our findings reveal several crucial insights: 1) the gold standard unlearning method, retraining from scratch, fails to effectively conduct unlearning under certain conditions; 2) unlearning duplicated data can lead to significant model degradation in specific scenarios; and 3) meticulously crafted duplicates can evade detection by de-duplication methods.",
      "year": 2025,
      "venue": "CISPA Helmholtz Center",
      "authors": [
        "Ye, Dayong",
        "Zhu, Tianqing",
        "Li Jiayang",
        "Gao Kun",
        "Liu Bo",
        "Yu Zhang, Leo",
        "Zhou, Wanlei",
        "Zhang Yang"
      ],
      "url": "https://openalex.org/W7104649343",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774737",
      "fetched_at": "2026-01-09T13:54:21.060960",
      "classified_at": "2026-01-09T14:01:04.166714",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W7104649343",
      "doi": "https://doi.org/10.60882/cispa.30581246.v1"
    },
    "seed_1dec002a": {
      "paper_id": "seed_1dec002a",
      "title": "Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness",
      "abstract": "By adopting a more flexible definition of unlearning and adjusting the model distribution to simulate training without the targeted data, approximate machine unlearning provides a less resource-demanding alternative to the more laborious exact unlearning methods. Yet, the unlearning completeness of target samples-even when the approximate algorithms are executed faithfully without external threats-remains largely unexamined, raising questions about those approximate algorithms' ability to fulfill their commitment of unlearning during the lifecycle. In this paper, we introduce the task of Lifecycle Unlearning Commitment Management (LUCM) for approximate unlearning and outline its primary challenges. We propose an efficient metric designed to assess the sample-level unlearning completeness. Our empirical results demonstrate its superiority over membership inference techniques in two key areas: the strong correlation of its measurements with unlearning completeness across various unlearning tasks, and its computational efficiency, making it suitable for real-time applications. Additionally, we show that this metric is able to serve as a tool for monitoring unlearning anomalies throughout the unlearning lifecycle, including both under-unlearning and over-unlearning. We apply this metric to evaluate the unlearning commitments of current approximate algorithms. Our analysis, conducted across multiple unlearning benchmarks, reveals that these algorithms inconsistently fulfill their unlearning commitments due to two main issues: 1) unlearning new data can significantly affect the unlearning utility of previously requested data, and 2) approximate algorithms fail to ensure equitable unlearning utility across different groups. These insights emphasize the crucial importance of LUCM throughout the unlearning lifecycle. We will soon open-source our newly developed benchmark.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Chenglong Wang",
        "Qi Li",
        "Zihang Xiang",
        "Di Wang"
      ],
      "url": "https://openalex.org/W4393027608",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774740",
      "fetched_at": "2026-01-09T13:54:21.615422",
      "classified_at": "2026-01-09T14:01:06.090603",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4393027608",
      "doi": "https://doi.org/10.48550/arxiv.2403.12830"
    },
    "2308.10422": {
      "paper_id": "2308.10422",
      "title": "Split Unlearning",
      "abstract": "We introduce Split Unlearning, a novel machine unlearning technology designed for Split Learning (SL), enabling the first-ever implementation of Sharded, Isolated, Sliced, and Aggregated (SISA) unlearning in SL frameworks. Particularly, the tight coupling between clients and the server in existing SL frameworks results in frequent bidirectional data flows and iterative training across all clients, violating the \"Isolated\" principle and making them struggle to implement SISA for independent and efficient unlearning. To address this, we propose SplitWiper with a new one-way-one-off propagation scheme, which leverages the inherently \"Sharded\" structure of SL and decouples neural signal propagation between clients and the server, enabling effective SISA unlearning even in scenarios with absent clients. We further design SplitWiper+ to enhance client label privacy, which integrates differential privacy and label expansion strategy to defend the privacy of client labels against the server and other potential adversaries. Experiments across diverse data distributions and tasks demonstrate that SplitWiper achieves 0% accuracy for unlearned labels, and 8% better accuracy for retained labels than non-SISA unlearning in SL. Moreover, the one-way-one-off propagation maintains constant overhead, reducing computational and communication costs by 99%. SplitWiper+ preserves 90% of label privacy when sharing masked labels with the server.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Guangsheng Yu",
        "Yanna Jiang",
        "Qin Wang",
        "Xu Wang",
        "Baihe Ma",
        "Caijun Sun",
        "Wei Ni",
        "Ren Ping Liu"
      ],
      "url": "https://arxiv.org/abs/2308.10422",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774743",
      "fetched_at": "2026-01-09T12:14:35.774744",
      "classified_at": "2026-01-09T13:26:26.223610",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2506.02761": {
      "paper_id": "2506.02761",
      "title": "Rethinking Machine Unlearning in Image Generation Models",
      "abstract": "With the surge and widespread application of image generation models, data privacy and content safety have become major concerns and attracted great attention from users, service providers, and policymakers. Machine unlearning (MU) is recognized as a cost-effective and promising means to address these challenges. Despite some advancements, image generation model unlearning (IGMU) still faces remarkable gaps in practice, e.g., unclear task discrimination and unlearning guidelines, lack of an effective evaluation framework, and unreliable evaluation metrics. These can hinder the understanding of unlearning mechanisms and the design of practical unlearning algorithms. We perform exhaustive assessments over existing state-of-the-art unlearning algorithms and evaluation standards, and discover several critical flaws and challenges in IGMU tasks. Driven by these limitations, we make several core contributions, to facilitate the comprehensive understanding, standardized categorization, and reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel hierarchical task categorization framework. It provides detailed implementation guidance for IGMU, assisting in the design of unlearning algorithms and the construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation framework. It includes reliable quantitative metrics across five critical aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can be used for extensive evaluations of IGMU, training content detectors for judgment, and benchmarking the state-of-the-art unlearning algorithms. With EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot handle the unlearning well across different evaluation dimensions, especially for preservation and robustness. Code and models are available at https://github.com/ryliu68/IGMU.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Renyang Liu",
        "Wenjie Feng",
        "Tianwei Zhang",
        "Wei Zhou",
        "Xueqi Cheng",
        "See-Kiong Ng"
      ],
      "url": "https://arxiv.org/abs/2506.02761",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774747",
      "fetched_at": "2026-01-09T12:14:35.774748",
      "classified_at": "2026-01-09T14:01:07.926615",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_4093b576": {
      "paper_id": "seed_4093b576",
      "title": "Prototype Surgery: Tailoring Neural Prototypes via Soft Labels for Efficient Machine Unlearning",
      "abstract": null,
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [],
      "url": "https://ccs25files.zoolab.org/main/ccsfa/sV74FPQe/3719027.3744827.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774751",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2209.01292": {
      "paper_id": "2209.01292",
      "title": "Are Attribute Inference Attacks Just Imputation?",
      "abstract": "Models can expose sensitive information about their training data. In an attribute inference attack, an adversary has partial knowledge of some training records and access to a model trained on those records, and infers the unknown values of a sensitive feature of those records. We study a fine-grained variant of attribute inference we call \\emph{sensitive value inference}, where the adversary's goal is to identify with high confidence some records from a candidate set where the unknown attribute has a particular sensitive value. We explicitly compare attribute inference with data imputation that captures the training distribution statistics, under various assumptions about the training data available to the adversary. Our main conclusions are: (1) previous attribute inference methods do not reveal more about the training data from the model than can be inferred by an adversary without access to the trained model, but with the same knowledge of the underlying distribution as needed to train the attribute inference attack; (2) black-box attribute inference attacks rarely learn anything that cannot be learned without the model; but (3) white-box attacks, which we introduce and evaluate in the paper, can reliably identify some records with the sensitive value attribute that would not be predicted without having access to the model. Furthermore, we show that proposed defenses such as differentially private training and removing vulnerable records from training do not mitigate this privacy risk. The code for our experiments is available at \\url{https://github.com/bargavj/EvaluatingDPML}.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Bargav Jayaraman",
        "David Evans"
      ],
      "url": "https://arxiv.org/abs/2209.01292",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774753",
      "fetched_at": "2026-01-09T12:14:35.774754",
      "classified_at": "2026-01-09T13:27:27.198507",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_316d3721": {
      "paper_id": "seed_316d3721",
      "title": "Feature Inference Attack on Shapley Values",
      "abstract": "As a solution concept in cooperative game theory, Shapley value is highly\\nrecognized in model interpretability studies and widely adopted by the leading\\nMachine Learning as a Service (MLaaS) providers, such as Google, Microsoft, and\\nIBM. However, as the Shapley value-based model interpretability methods have\\nbeen thoroughly studied, few researchers consider the privacy risks incurred by\\nShapley values, despite that interpretability and privacy are two foundations\\nof machine learning (ML) models.\\n In this paper, we investigate the privacy risks of Shapley value-based model\\ninterpretability methods using feature inference attacks: reconstructing the\\nprivate model inputs based on their Shapley value explanations. Specifically,\\nwe present two adversaries. The first adversary can reconstruct the private\\ninputs by training an attack model based on an auxiliary dataset and black-box\\naccess to the model interpretability services. The second adversary, even\\nwithout any background knowledge, can successfully reconstruct most of the\\nprivate features by exploiting the local linear correlations between the model\\ninputs and outputs. We perform the proposed attacks on the leading MLaaS\\nplatforms, i.e., Google Cloud, Microsoft Azure, and IBM aix360. The\\nexperimental results demonstrate the vulnerability of the state-of-the-art\\nShapley value-based model interpretability methods used in the leading MLaaS\\nplatforms and highlight the significance and necessity of designing\\nprivacy-preserving model interpretability methods in future studies. To our\\nbest knowledge, this is also the first work that investigates the privacy risks\\nof Shapley values.\\n",
      "year": 2022,
      "venue": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
      "authors": [
        "Xinjian Luo",
        "Yang-Fan Jiang",
        "Xiaokui Xiao"
      ],
      "url": "https://openalex.org/W4308361263",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774758",
      "fetched_at": "2026-01-09T13:54:22.931169",
      "classified_at": "2026-01-09T14:01:09.744944",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4308361263",
      "doi": "https://doi.org/10.1145/3548606.3560573"
    },
    "2211.05249": {
      "paper_id": "2211.05249",
      "title": "QuerySnout: Automating the Discovery of Attribute Inference Attacks against Query-Based Systems",
      "abstract": "Although query-based systems (QBS) have become one of the main solutions to share data anonymously, building QBSes that robustly protect the privacy of individuals contributing to the dataset is a hard problem. Theoretical solutions relying on differential privacy guarantees are difficult to implement correctly with reasonable accuracy, while ad-hoc solutions might contain unknown vulnerabilities. Evaluating the privacy provided by QBSes must thus be done by evaluating the accuracy of a wide range of privacy attacks. However, existing attacks require time and expertise to develop, need to be manually tailored to the specific systems attacked, and are limited in scope. In this paper, we develop QuerySnout (QS), the first method to automatically discover vulnerabilities in QBSes. QS takes as input a target record and the QBS as a black box, analyzes its behavior on one or more datasets, and outputs a multiset of queries together with a rule to combine answers to them in order to reveal the sensitive attribute of the target record. QS uses evolutionary search techniques based on a novel mutation operator to find a multiset of queries susceptible to lead to an attack, and a machine learning classifier to infer the sensitive attribute from answers to the queries selected. We showcase the versatility of QS by applying it to two attack scenarios, three real-world datasets, and a variety of protection mechanisms. We show the attacks found by QS to consistently equate or outperform, sometimes by a large margin, the best attacks from the literature. We finally show how QS can be extended to QBSes that require a budget, and apply QS to a simple QBS based on the Laplace mechanism. Taken together, our results show how powerful and accurate attacks against QBSes can already be found by an automated system, allowing for highly complex QBSes to be automatically tested \"at the pressing of a button\".",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Ana-Maria Cretu",
        "Florimond Houssiau",
        "Antoine Cully",
        "Yves-Alexandre de Montjoye"
      ],
      "url": "https://arxiv.org/abs/2211.05249",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774761",
      "fetched_at": "2026-01-09T12:14:35.774762",
      "classified_at": "2026-01-09T13:27:27.964911",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_07bac000": {
      "paper_id": "seed_07bac000",
      "title": "Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and Defenses",
      "abstract": "As machine learning (ML) technologies become more prevalent in privacy-sensitive areas like healthcare and finance, eventually incorporating sensitive information in building data-driven algorithms, it is vital to scrutinize whether these data face any privacy leakage risks. One potential threat arises from an adversary querying trained models using the public, non-sensitive attributes of entities in the training data to infer their private, sensitive attributes, a technique known as the attribute inference attack. This attack is particularly deceptive because, while it may perform poorly in predicting sensitive attributes across the entire dataset, it excels at predicting the sensitive attributes of records from a few vulnerable groups, a phenomenon known as disparate vulnerability. This paper illustrates that an adversary can take advantage of this disparity to carry out a series of new attacks, showcasing a threat level beyond previous imagination. We first develop a novel inference attack called the disparity inference attack, which targets the identification of high-risk groups within the dataset. We then introduce two targeted variations of the attribute inference attack that can identify and exploit a vulnerable subset of the training data, marking the first instances of targeted attacks in this category, achieving significantly higher accuracy than untargeted versions. We are also the first to introduce a novel and effective disparity mitigation technique that simultaneously preserves model performance and prevents any risk of targeted attacks.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Ehsanul Kabir",
        "Leone Craig",
        "Shagufta Mehnaz"
      ],
      "url": "https://openalex.org/W4415981201",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774766",
      "fetched_at": "2026-01-09T13:54:23.487168",
      "classified_at": "2026-01-09T14:01:11.835120",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4415981201",
      "doi": "https://doi.org/10.48550/arxiv.2504.04033"
    },
    "2208.12348": {
      "paper_id": "2208.12348",
      "title": "SNAP: Efficient Extraction of Private Properties with Poisoning",
      "abstract": "Property inference attacks allow an adversary to extract global properties of the training dataset from a machine learning model. Such attacks have privacy implications for data owners sharing their datasets to train machine learning models. Several existing approaches for property inference attacks against deep neural networks have been proposed, but they all rely on the attacker training a large number of shadow models, which induces a large computational overhead.   In this paper, we consider the setting of property inference attacks in which the attacker can poison a subset of the training dataset and query the trained target model. Motivated by our theoretical analysis of model confidences under poisoning, we design an efficient property inference attack, SNAP, which obtains higher attack success and requires lower amounts of poisoning than the state-of-the-art poisoning-based property inference attack by Mahloujifar et al. For example, on the Census dataset, SNAP achieves 34% higher success rate than Mahloujifar et al. while being 56.5x faster. We also extend our attack to infer whether a certain property was present at all during training and estimate the exact proportion of a property of interest efficiently. We evaluate our attack on several properties of varying proportions from four datasets and demonstrate SNAP's generality and effectiveness. An open-source implementation of SNAP can be found at https://github.com/johnmath/snap-sp23.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Harsh Chaudhari",
        "John Abascal",
        "Alina Oprea",
        "Matthew Jagielski",
        "Florian Tram\u00e8r",
        "Jonathan Ullman"
      ],
      "url": "https://arxiv.org/abs/2208.12348",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774768",
      "fetched_at": "2026-01-09T12:14:35.774769",
      "classified_at": "2026-01-09T13:27:28.576781",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2307.02106": {
      "paper_id": "2307.02106",
      "title": "SoK: Privacy-Preserving Data Synthesis",
      "abstract": "As the prevalence of data analysis grows, safeguarding data privacy has become a paramount concern. Consequently, there has been an upsurge in the development of mechanisms aimed at privacy-preserving data analyses. However, these approaches are task-specific; designing algorithms for new tasks is a cumbersome process. As an alternative, one can create synthetic data that is (ideally) devoid of private information. This paper focuses on privacy-preserving data synthesis (PPDS) by providing a comprehensive overview, analysis, and discussion of the field. Specifically, we put forth a master recipe that unifies two prominent strands of research in PPDS: statistical methods and deep learning (DL)-based methods. Under the master recipe, we further dissect the statistical methods into choices of modeling and representation, and investigate the DL-based methods by different generative modeling principles. To consolidate our findings, we provide comprehensive reference tables, distill key takeaways, and identify open problems in the existing literature. In doing so, we aim to answer the following questions: What are the design principles behind different PPDS methods? How can we categorize these methods, and what are the advantages and disadvantages associated with each category? Can we provide guidelines for method selection in different real-world scenarios? We proceed to benchmark several prominent DL-based methods on the task of private image synthesis and conclude that DP-MERF is an all-purpose approach. Finally, upon systematizing the work over the past decade, we identify future directions and call for actions from researchers.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Yuzheng Hu",
        "Fan Wu",
        "Qinbin Li",
        "Yunhui Long",
        "Gonzalo Munilla Garrido",
        "Chang Ge",
        "Bolin Ding",
        "David Forsyth",
        "Bo Li",
        "Dawn Song"
      ],
      "url": "https://arxiv.org/abs/2307.02106",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774772",
      "fetched_at": "2026-01-09T12:14:35.774773",
      "classified_at": "2026-01-09T13:27:29.182547",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2309.03081": {
      "paper_id": "2309.03081",
      "title": "ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning",
      "abstract": "Data is a critical asset in AI, as high-quality datasets can significantly improve the performance of machine learning models. In safety-critical domains such as autonomous vehicles, offline deep reinforcement learning (offline DRL) is frequently used to train models on pre-collected datasets, as opposed to training these models by interacting with the real-world environment as the online DRL. To support the development of these models, many institutions make datasets publicly available with opensource licenses, but these datasets are at risk of potential misuse or infringement. Injecting watermarks to the dataset may protect the intellectual property of the data, but it cannot handle datasets that have already been published and is infeasible to be altered afterward. Other existing solutions, such as dataset inference and membership inference, do not work well in the offline DRL scenario due to the diverse model behavior characteristics and offline setting constraints. In this paper, we advocate a new paradigm by leveraging the fact that cumulative rewards can act as a unique identifier that distinguishes DRL models trained on a specific dataset. To this end, we propose ORL-AUDITOR, which is the first trajectory-level dataset auditing mechanism for offline RL scenarios. Our experiments on multiple offline DRL models and tasks reveal the efficacy of ORL-AUDITOR, with auditing accuracy over 95% and false positive rates less than 2.88%. We also provide valuable insights into the practical implementation of ORL-AUDITOR by studying various parameter settings. Furthermore, we demonstrate the auditing capability of ORL-AUDITOR on open-source datasets from Google and DeepMind, highlighting its effectiveness in auditing published datasets. ORL-AUDITOR is open-sourced at https://github.com/link-zju/ORL-Auditor.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Linkang Du",
        "Min Chen",
        "Mingyang Sun",
        "Shouling Ji",
        "Peng Cheng",
        "Jiming Chen",
        "Zhikun Zhang"
      ],
      "url": "https://arxiv.org/abs/2309.03081",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML05",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774776",
      "fetched_at": "2026-01-09T12:14:35.774777",
      "classified_at": "2026-01-09T13:27:29.793135",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2410.16618": {
      "paper_id": "2410.16618",
      "title": "SoK: Dataset Copyright Auditing in Machine Learning Systems",
      "abstract": "As the implementation of machine learning (ML) systems becomes more widespread, especially with the introduction of larger ML models, we perceive a spring demand for massive data. However, it inevitably causes infringement and misuse problems with the data, such as using unauthorized online artworks or face images to train ML models. To address this problem, many efforts have been made to audit the copyright of the model training dataset. However, existing solutions vary in auditing assumptions and capabilities, making it difficult to compare their strengths and weaknesses. In addition, robustness evaluations usually consider only part of the ML pipeline and hardly reflect the performance of algorithms in real-world ML applications. Thus, it is essential to take a practical deployment perspective on the current dataset copyright auditing tools, examining their effectiveness and limitations. Concretely, we categorize dataset copyright auditing research into two prominent strands: intrusive methods and non-intrusive methods, depending on whether they require modifications to the original dataset. Then, we break down the intrusive methods into different watermark injection options and examine the non-intrusive methods using various fingerprints. To summarize our results, we offer detailed reference tables, highlight key points, and pinpoint unresolved issues in the current literature. By combining the pipeline in ML systems and analyzing previous studies, we highlight several future directions to make auditing tools more suitable for real-world copyright protection requirements.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Linkang Du",
        "Xuanru Zhou",
        "Min Chen",
        "Chusong Zhang",
        "Zhou Su",
        "Peng Cheng",
        "Jiming Chen",
        "Zhikun Zhang"
      ],
      "url": "https://arxiv.org/abs/2410.16618",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML05",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774781",
      "fetched_at": "2026-01-09T12:14:35.774782",
      "classified_at": "2026-01-09T13:27:30.397100",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_17fc8f35": {
      "paper_id": "seed_17fc8f35",
      "title": "Exploring Connections Between Active Learning and Model Extraction",
      "abstract": "Machine learning is being increasingly used by individuals, research institutions, and corporations. This has resulted in the surge of Machine Learning-as-a-Service (MLaaS) - cloud services that provide (a) tools and resources to learn the model, and (b) a user-friendly query interface to access the model. However, such MLaaS systems raise privacy concerns such as model extraction. In model extraction attacks, adversaries maliciously exploit the query interface to steal the model. More precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (i.e. learned) by a dishonest user who interacts with the server only via the query interface. This attack was introduced by Tramer et al. at the 2016 USENIX Security Symposium, where practical attacks for various models were shown. We believe that better understanding the efficacy of model extraction attacks is paramount to designing secure MLaaS systems. To that end, we take the first step by (a) formalizing model extraction and discussing possible defense strategies, and (b) drawing parallels between model extraction and established area of active learning. In particular, we show that recent advancements in the active learning domain can be used to implement powerful model extraction attacks, and investigate possible defense strategies.",
      "year": 2018,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Varun Chandrasekaran",
        "Kamalika Chaudhuri",
        "Irene Giacomelli",
        "Somesh Jha",
        "Songbai Yan"
      ],
      "url": "https://openalex.org/W2918814112",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774785",
      "fetched_at": "2026-01-09T13:54:24.317709",
      "classified_at": "2026-01-09T14:01:13.745743",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2918814112",
      "doi": "https://doi.org/10.48550/arxiv.1811.02054"
    },
    "1909.01838": {
      "paper_id": "1909.01838",
      "title": "High Accuracy and High Fidelity Extraction of Neural Networks",
      "abstract": "In a model extraction attack, an adversary steals a copy of a remotely deployed machine learning model, given oracle prediction access. We taxonomize model extraction attacks around two objectives: *accuracy*, i.e., performing well on the underlying learning task, and *fidelity*, i.e., matching the predictions of the remote victim classifier on any input.   To extract a high-accuracy model, we develop a learning-based attack exploiting the victim to supervise the training of an extracted model. Through analytical and empirical arguments, we then explain the inherent limitations that prevent any learning-based strategy from extracting a truly high-fidelity model---i.e., extracting a functionally-equivalent model whose predictions are identical to those of the victim model on all possible inputs. Addressing these limitations, we expand on prior work to develop the first practical functionally-equivalent extraction attack for direct extraction (i.e., without training) of a model's weights.   We perform experiments both on academic datasets and a state-of-the-art image classifier trained with 1 billion proprietary images. In addition to broadening the scope of model extraction research, our work demonstrates the practicality of model extraction attacks against production-grade systems.",
      "year": 2020,
      "venue": "USENIX Security",
      "authors": [
        "Matthew Jagielski",
        "Nicholas Carlini",
        "David Berthelot",
        "Alex Kurakin",
        "Nicolas Papernot"
      ],
      "url": "https://arxiv.org/abs/1909.01838",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774787",
      "fetched_at": "2026-01-09T12:14:35.774788",
      "classified_at": "2026-01-09T13:27:31.107106",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_e7048869": {
      "paper_id": "seed_e7048869",
      "title": "DRMI: A Dataset Reduction Technology based on Mutual Information for Black-box Attacks",
      "abstract": null,
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec21-he-yingzhe.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774791",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_f1310987": {
      "paper_id": "seed_f1310987",
      "title": "Entangled Watermarks as a Defense against Model Extraction",
      "abstract": "Machine learning involves expensive data collection and training procedures. Model owners may be concerned that valuable intellectual property can be leaked if adversaries mount model extraction attacks. As it is difficult to defend against model extraction without sacrificing significant prediction accuracy, watermarking instead leverages unused model capacity to have the model overfit to outlier input-output pairs. Such pairs are watermarks, which are not sampled from the task distribution and are only known to the defender. The defender then demonstrates knowledge of the input-output pairs to claim ownership of the model at inference. The effectiveness of watermarks remains limited because they are distinct from the task distribution and can thus be easily removed through compression or other forms of knowledge transfer. \r\nWe introduce Entangled Watermarking Embeddings (EWE). Our approach encourages the model to learn features for classifying data that is sampled from the task distribution and data that encodes watermarks. An adversary attempting to remove watermarks that are entangled with legitimate data is also forced to sacrifice performance on legitimate data. Experiments on MNIST, Fashion-MNIST, CIFAR-10, and Speech Commands validate that the defender can claim model ownership with 95\\% confidence with less than 100 queries to the stolen copy, at a modest cost below 0.81 percentage points on average in the defended model's performance.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Hengrui Jia",
        "Christopher A. Choquette-Choo",
        "Varun Chandrasekaran",
        "Nicolas Papernot"
      ],
      "url": "https://openalex.org/W3159603021",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774794",
      "fetched_at": "2026-01-09T13:54:25.848086",
      "classified_at": "2026-01-09T14:01:16.065714",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3159603021"
    },
    "seed_9103c9cb": {
      "paper_id": "seed_9103c9cb",
      "title": "CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples",
      "abstract": "Cloud-based Machine Learning as a Service (MLaaS) is gradually gaining acceptance as a reliable solution to various real-life scenarios.These services typically utilize Deep Neural Networks (DNNs) to perform classification and detection tasks and are accessed through Application Programming Interfaces (APIs).Unfortunately, it is possible for an adversary to steal models from cloud-based platforms, even with black-box constraints, by repeatedly querying the public prediction API with malicious inputs.In this paper, we introduce an effective and efficient black-box attack methodology that extracts largescale DNN models from cloud-based platforms with near-perfect performance.In comparison to existing attack methods, we significantly reduce the number of queries required to steal the target model by incorporating several novel algorithms, including active learning, transfer learning, and adversarial attacks.During our experimental evaluations, we validate our proposed model for conducting theft attacks on various commercialized MLaaS platforms hosted by Microsoft, Face++, IBM, Google and Clarifai.Our results demonstrate that the proposed method can easily reveal/steal large-scale DNN models from these cloud platforms.The proposed attack method can also be used to accurately evaluates the robustness of DNN based MLaaS classifiers against theft attacks.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Honggang Yu",
        "Kaichen Yang",
        "Teng Zhang",
        "Yun-Yun Tsai",
        "Tsung-Yi Ho",
        "Yier Jin"
      ],
      "url": "https://openalex.org/W3007318395",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774797",
      "fetched_at": "2026-01-09T13:54:26.608193",
      "classified_at": "2026-01-09T14:01:18.298754",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3007318395",
      "doi": "https://doi.org/10.14722/ndss.2020.24178"
    },
    "seed_e954d79d": {
      "paper_id": "seed_e954d79d",
      "title": "Teacher Model Fingerprinting Attacks Against Transfer Learning",
      "abstract": "Transfer learning has become a common solution to address training data scarcity in practice. It trains a specified student model by reusing or fine-tuning early layers of a well-trained teacher model that is usually publicly available. However, besides utility improvement, the transferred public knowledge also brings potential threats to model confidentiality, and even further raises other security and privacy issues. In this paper, we present the first comprehensive investigation of the teacher model exposure threat in the transfer learning context, aiming to gain a deeper insight into the tension between public knowledge and model confidentiality. To this end, we propose a teacher model fingerprinting attack to infer the origin of a student model, i.e., the teacher model it transfers from. Specifically, we propose a novel optimization-based method to carefully generate queries to probe the student model to realize our attack. Unlike existing model reverse engineering approaches, our proposed fingerprinting method neither relies on fine-grained model outputs, e.g., posteriors, nor auxiliary information of the model architecture or training dataset. We systematically evaluate the effectiveness of our proposed attack. The empirical results demonstrate that our attack can accurately identify the model origin with few probing queries. Moreover, we show that the proposed attack can serve as a stepping stone to facilitating other attacks against machine learning models, such as model stealing.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yufei Chen",
        "Chao Shen",
        "Cong Wang",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W3176305602",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774799",
      "fetched_at": "2026-01-09T13:54:27.368021",
      "classified_at": "2026-01-09T14:01:23.205688",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3176305602",
      "doi": "https://doi.org/10.48550/arxiv.2106.12478"
    },
    "2201.05889": {
      "paper_id": "2201.05889",
      "title": "StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning",
      "abstract": "Pre-trained encoders are general-purpose feature extractors that can be used for many downstream tasks. Recent progress in self-supervised learning can pre-train highly effective encoders using a large volume of unlabeled data, leading to the emerging encoder as a service (EaaS). A pre-trained encoder may be deemed confidential because its training requires lots of data and computation resources as well as its public release may facilitate misuse of AI, e.g., for deepfakes generation. In this paper, we propose the first attack called StolenEncoder to steal pre-trained image encoders. We evaluate StolenEncoder on multiple target encoders pre-trained by ourselves and three real-world target encoders including the ImageNet encoder pre-trained by Google, CLIP encoder pre-trained by OpenAI, and Clarifai's General Embedding encoder deployed as a paid EaaS. Our results show that our stolen encoders have similar functionality with the target encoders. In particular, the downstream classifiers built upon a target encoder and a stolen one have similar accuracy. Moreover, stealing a target encoder using StolenEncoder requires much less data and computation resources than pre-training it from scratch. We also explore three defenses that perturb feature vectors produced by a target encoder. Our results show these defenses are not enough to mitigate StolenEncoder.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Yupei Liu",
        "Jinyuan Jia",
        "Hongbin Liu",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://arxiv.org/abs/2201.05889",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774802",
      "fetched_at": "2026-01-09T12:14:35.774803",
      "classified_at": "2026-01-09T13:27:31.717263",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_af4a9a1b": {
      "paper_id": "seed_af4a9a1b",
      "title": "D-DAE: Defense-Penetrating Model Extraction Attacks",
      "abstract": "Recent studies show that machine learning models are vulnerable to model extraction attacks, where the adversary builds a substitute model that achieves almost the same performance of a black-box victim model simply via querying the victim model. To defend against such attacks, a series of methods have been proposed to disrupt the query results before returning them to potential attackers, greatly degrading the performance of existing model extraction attacks.In this paper, we make the first attempt to develop a defense-penetrating model extraction attack framework, named D-DAE, which aims to break disruption-based defenses. The linchpins of D-DAE are the design of two modules, i.e., disruption detection and disruption recovery, which can be integrated with generic model extraction attacks. More specifically, after obtaining query results from the victim model, the disruption detection module infers the defense mechanism adopted by the defender. We design a meta-learning-based disruption detection algorithm for learning the fundamental differences between the distributions of disrupted and undisrupted query results. The algorithm features a good generalization property even if we have no access to the original training dataset of the victim model. Given the detected defense mechanism, the disruption recovery module tries to restore a clean query result from the disrupted query result with well-designed generative models. Our extensive evaluations on MNIST, FashionMNIST, CIFAR-10, GTSRB, and ImageNette datasets demonstrate that D-DAE can enhance the substitute model accuracy of the existing model extraction attacks by as much as 82.24% in the face of 4 state-of-the-art defenses and combinations of multiple defenses. We also verify the effectiveness of D-DAE in penetrating unknown defenses in real-world APIs hosted by Microsoft Azure and Face++.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Yanjiao Chen",
        "Rui Guan",
        "Xueluan Gong",
        "Jianshuo Dong",
        "Meng Xue"
      ],
      "url": "https://openalex.org/W4385080307",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774806",
      "fetched_at": "2026-01-09T13:54:27.863108",
      "classified_at": "2026-01-09T14:01:25.157580",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385080307",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179406"
    },
    "seed_9af43847": {
      "paper_id": "seed_9af43847",
      "title": "SoK: Neural Network Extraction Through Physical Side Channels",
      "abstract": null,
      "year": 2024,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity24-horvath.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774808",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_bcafbaa6": {
      "paper_id": "seed_bcafbaa6",
      "title": "SoK: All You Need to Know About On-Device ML Model Extraction - The Gap Between Research and Practice",
      "abstract": "Although the fifth generation (5G) wireless networks are yet to be fully investigated, the visionaries of the 6th generation (6G) echo systems have already come into the discussion. Therefore, in order to consolidate and solidify the security and privacy in 6G networks, we survey how security may impact the envisioned 6G wireless systems, possible challenges with different 6G technologies, and the potential solutions. We provide our vision on 6G security and security key performance indicators (KPIs) with the tentative threat landscape based on the foreseen 6G network architecture. Moreover, we discuss the security and privacy challenges that may encounter with the available 6G requirements and potential 6G applications. We also give the reader some insights into the standardization efforts and research-level projects relevant to 6G security. In particular, we discuss the security considerations with 6G enabling technologies such as distributed ledger technology (DLT), physical layer security, distributed AI/ML, visible light communication (VLC), THz, and quantum computing. All in all, this work intends to provide enlightening guidance for the subsequent research of 6G security and privacy at this initial phase of vision towards reality.",
      "year": 2021,
      "venue": "IEEE Open Journal of the Communications Society",
      "authors": [
        "Pawani Porambage",
        "G\u00fcrkan G\u00fcr",
        "Diana Pamela Moya Osorio",
        "Madhusanka Liyanage",
        "Andrei Gurtov",
        "Mika Ylianttila"
      ],
      "url": "https://openalex.org/W3161594686",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774812",
      "fetched_at": "2026-01-09T13:54:29.421359",
      "classified_at": "2026-01-09T14:01:26.984672",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3161594686",
      "doi": "https://doi.org/10.1109/ojcoms.2021.3078081"
    },
    "2009.03015": {
      "paper_id": "2009.03015",
      "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding",
      "abstract": "Recent advances in natural language generation have introduced powerful language models with high-quality output text. However, this raises concerns about the potential misuse of such models for malicious purposes. In this paper, we study natural language watermarking as a defense to help better mark and trace the provenance of text. We introduce the Adversarial Watermarking Transformer (AWT) with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message. We further study different training and inference strategies to achieve minimal changes to the semantics and correctness of the input text.   AWT is the first end-to-end model to hide data in text by automatically learning -- without ground truth -- word substitutions along with their locations in order to encode the message. We empirically show that our model is effective in largely preserving text utility and decoding the watermark while hiding its presence against adversaries. Additionally, we demonstrate that our method is robust against a range of attacks.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Sahar Abdelnabi",
        "Mario Fritz"
      ],
      "url": "https://arxiv.org/abs/2009.03015",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774815",
      "fetched_at": "2026-01-09T12:14:35.774816",
      "classified_at": "2026-01-09T13:27:32.458511",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_295a2528": {
      "paper_id": "seed_295a2528",
      "title": "Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation",
      "abstract": "Copyright protection for deep neural networks (DNNs) is an urgent need for AI corporations. To trace illegally distributed model copies, DNN watermarking is an emerging technique for embedding and verifying secret identity messages in the prediction behaviors or the model internals. Sacrificing less functionality and involving more knowledge about the target DNN, the latter branch called \\textit{white-box DNN watermarking} is believed to be accurate, credible and secure against most known watermark removal attacks, with emerging research efforts in both the academy and the industry. In this paper, we present the first systematic study on how the mainstream white-box DNN watermarks are commonly vulnerable to neural structural obfuscation with \\textit{dummy neurons}, a group of neurons which can be added to a target model but leave the model behavior invariant. Devising a comprehensive framework to automatically generate and inject dummy neurons with high stealthiness, our novel attack intensively modifies the architecture of the target model to inhibit the success of watermark verification. With extensive evaluation, our work for the first time shows that nine published watermarking schemes require amendments to their verification procedures.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yifan Yan",
        "Xudong Pan",
        "Mi Zhang",
        "Min Yang"
      ],
      "url": "https://openalex.org/W4353012851",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774818",
      "fetched_at": "2026-01-09T13:54:30.893131",
      "classified_at": "2026-01-09T14:01:28.784820",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4353012851",
      "doi": "https://doi.org/10.48550/arxiv.2303.09732"
    },
    "2401.15239": {
      "paper_id": "2401.15239",
      "title": "MEA-Defender: A Robust Watermark against Model Extraction Attack",
      "abstract": "Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been trained using deep learning algorithms. To protect the Intellectual Property (IP) of the original owners over such DNN models, backdoor-based watermarks have been extensively studied. However, most of such watermarks fail upon model extraction attack, which utilizes input samples to query the target model and obtains the corresponding outputs, thus training a substitute model using such input-output pairs. In this paper, we propose a novel watermark to protect IP of DNN models against model extraction, named MEA-Defender. In particular, we obtain the watermark by combining two samples from two source classes in the input domain and design a watermark loss function that makes the output domain of the watermark within that of the main task samples. Since both the input domain and the output domain of our watermark are indispensable parts of those of the main task samples, the watermark will be extracted into the stolen model along with the main task during model extraction. We conduct extensive experiments on four model extraction attacks, using five datasets and six models trained based on supervised learning and self-supervised learning algorithms. The experimental results demonstrate that MEA-Defender is highly robust against different model extraction attacks, and various watermark removal/detection approaches.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Peizhuo Lv",
        "Hualong Ma",
        "Kai Chen",
        "Jiachen Zhou",
        "Shengzhi Zhang",
        "Ruigang Liang",
        "Shenchen Zhu",
        "Pan Li",
        "Yingjun Zhang"
      ],
      "url": "https://arxiv.org/abs/2401.15239",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774821",
      "fetched_at": "2026-01-09T12:14:35.774822",
      "classified_at": "2026-01-09T13:27:33.148286",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2209.03563": {
      "paper_id": "2209.03563",
      "title": "SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-Supervised Learning",
      "abstract": "Recent years have witnessed tremendous success in Self-Supervised Learning (SSL), which has been widely utilized to facilitate various downstream tasks in Computer Vision (CV) and Natural Language Processing (NLP) domains. However, attackers may steal such SSL models and commercialize them for profit, making it crucial to verify the ownership of the SSL models. Most existing ownership protection solutions (e.g., backdoor-based watermarks) are designed for supervised learning models and cannot be used directly since they require that the models' downstream tasks and target labels be known and available during watermark embedding, which is not always possible in the domain of SSL. To address such a problem, especially when downstream tasks are diverse and unknown during watermark embedding, we propose a novel black-box watermarking solution, named SSL-WM, for verifying the ownership of SSL models. SSL-WM maps watermarked inputs of the protected encoders into an invariant representation space, which causes any downstream classifier to produce expected behavior, thus allowing the detection of embedded watermarks. We evaluate SSL-WM on numerous tasks, such as CV and NLP, using different SSL models both contrastive-based and generative-based. Experimental results demonstrate that SSL-WM can effectively verify the ownership of stolen SSL models in various downstream tasks. Furthermore, SSL-WM is robust against model fine-tuning, pruning, and input preprocessing attacks. Lastly, SSL-WM can also evade detection from evaluated watermark detection approaches, demonstrating its promising application in protecting the ownership of SSL models.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Peizhuo Lv",
        "Pan Li",
        "Shenchen Zhu",
        "Shengzhi Zhang",
        "Kai Chen",
        "Ruigang Liang",
        "Chang Yue",
        "Fan Xiang",
        "Yuling Cai",
        "Hualong Ma",
        "Yingjun Zhang",
        "Guozhu Meng"
      ],
      "url": "https://arxiv.org/abs/2209.03563",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774825",
      "fetched_at": "2026-01-09T12:14:35.774826",
      "classified_at": "2026-01-09T13:27:33.811111",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_0b98e84b": {
      "paper_id": "seed_0b98e84b",
      "title": "Watermarking Language Models for Many Adaptive Users",
      "abstract": "Recent years have witnessed tremendous success in Self-Supervised Learning (SSL), which has been widely utilized to facilitate various downstream tasks in Computer Vision (CV) and Natural Language Processing (NLP) domains. However, attackers may steal such SSL models and commercialize them for profit, making it crucial to verify the ownership of the SSL models. Most existing ownership protection solutions (e.g., backdoor-based watermarks) are designed for supervised learning models and cannot be used directly since they require that the models' downstream tasks and target labels be known and available during watermark embedding, which is not always possible in the domain of SSL. To address such a problem, especially when downstream tasks are diverse and unknown during watermark embedding, we propose a novel black-box watermarking solution, named SSL-WM, for verifying the ownership of SSL models. SSL-WM maps watermarked inputs of the protected encoders into an invariant representation space, which causes any downstream classifier to produce expected behavior, thus allowing the detection of embedded watermarks. We evaluate SSL-WM on numerous tasks, such as CV and NLP, using different SSL models both contrastive-based and generative-based. Experimental results demonstrate that SSL-WM can effectively verify the ownership of stolen SSL models in various downstream tasks. Furthermore, SSL-WM is robust against model fine-tuning, pruning, and input preprocessing attacks. Lastly, SSL-WM can also evade detection from evaluated watermark detection approaches, demonstrating its promising application in protecting the ownership of SSL models.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Peizhuo Lv",
        "Pan Li",
        "Shenchen Zhu",
        "Shengzhi Zhang",
        "Kai Chen",
        "Ruigang Liang",
        "Chang Yue",
        "Fan Xiang",
        "Yuling Cai",
        "Hualong Ma",
        "Yingjun Zhang",
        "Guozhu Meng"
      ],
      "url": "https://arxiv.org/abs/2209.03563",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774829",
      "fetched_at": "2026-01-09T13:54:31.690119",
      "classified_at": "2026-01-09T14:01:31.851266",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "2209.03563"
    },
    "2411.18479": {
      "paper_id": "2411.18479",
      "title": "SoK: Watermarking for AI-Generated Content",
      "abstract": "As the outputs of generative AI (GenAI) techniques improve in quality, it becomes increasingly challenging to distinguish them from human-created content. Watermarking schemes are a promising approach to address the problem of distinguishing between AI and human-generated content. These schemes embed hidden signals within AI-generated content to enable reliable detection. While watermarking is not a silver bullet for addressing all risks associated with GenAI, it can play a crucial role in enhancing AI safety and trustworthiness by combating misinformation and deception. This paper presents a comprehensive overview of watermarking techniques for GenAI, beginning with the need for watermarking from historical and regulatory perspectives. We formalize the definitions and desired properties of watermarking schemes and examine the key objectives and threat models for existing approaches. Practical evaluation strategies are also explored, providing insights into the development of robust watermarking techniques capable of resisting various attacks. Additionally, we review recent representative works, highlight open challenges, and discuss potential directions for this emerging field. By offering a thorough understanding of watermarking in GenAI, this work aims to guide researchers in advancing watermarking methods and applications, and support policymakers in addressing the broader implications of GenAI.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Xuandong Zhao",
        "Sam Gunn",
        "Miranda Christ",
        "Jaiden Fairoze",
        "Andres Fabrega",
        "Nicholas Carlini",
        "Sanjam Garg",
        "Sanghyun Hong",
        "Milad Nasr",
        "Florian Tramer",
        "Somesh Jha",
        "Lei Li",
        "Yu-Xiang Wang",
        "Dawn Song"
      ],
      "url": "https://arxiv.org/abs/2411.18479",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774831",
      "fetched_at": "2026-01-09T12:14:35.774832",
      "classified_at": "2026-01-09T13:27:34.496382",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_0c0aadbb": {
      "paper_id": "seed_0c0aadbb",
      "title": "Provably Robust Multi-bit Watermarking for AI-generated Text",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities of generating texts resembling human language. However, they can be misused by criminals to create deceptive content, such as fake news and phishing emails, which raises ethical concerns. Watermarking is a key technique to address these concerns, which embeds a message (e.g., a bit string) into a text generated by an LLM. By embedding the user ID (represented as a bit string) into generated texts, we can trace generated texts to the user, known as content source tracing. The major limitation of existing watermarking techniques is that they achieve sub-optimal performance for content source tracing in real-world scenarios. The reason is that they cannot accurately or efficiently extract a long message from a generated text. We aim to address the limitations. In this work, we introduce a new watermarking method for LLM-generated text grounded in pseudo-random segment assignment. We also propose multiple techniques to further enhance the robustness of our watermarking algorithm. We conduct extensive experiments to evaluate our method. Our experimental results show that our method substantially outperforms existing baselines in both accuracy and robustness on benchmark datasets. For instance, when embedding a message of length 20 into a 200-token generated text, our method achieves a match rate of $97.6\\%$, while the state-of-the-art work Yoo et al. only achieves $49.2\\%$. Additionally, we prove that our watermark can tolerate edits within an edit distance of 17 on average for each paragraph under the same setting.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Wenjie Qu",
        "Dong Yin",
        "Zixin He",
        "Wei Zou",
        "Tianyang Tao",
        "Jinyuan Jia",
        "Jiaheng Zhang"
      ],
      "url": "https://openalex.org/W4391421113",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774835",
      "fetched_at": "2026-01-09T13:54:32.450675",
      "classified_at": "2026-01-09T14:01:35.387010",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391421113",
      "doi": "https://doi.org/10.48550/arxiv.2401.16820"
    },
    "seed_36ed25cf": {
      "paper_id": "seed_36ed25cf",
      "title": "AUDIO WATERMARK: Dynamic and Harmless Watermark for Black-box Voice Dataset Copyright Protection",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-guo-hanqing.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774838",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_dd96f799": {
      "paper_id": "seed_dd96f799",
      "title": "AudioMarkNet: Audio Watermarking for Deepfake Speech Detection",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-zong.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774840",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_37f18cb0": {
      "paper_id": "seed_37f18cb0",
      "title": "Towards Understanding and Enhancing Security of Proof-of-Training for DNN Model Ownership Verification",
      "abstract": "The great economic values of deep neural networks (DNNs) urge AI enterprises to protect their intellectual property (IP) for these models. Recently, proof-of-training (PoT) has been proposed as a promising solution to DNN IP protection, through which AI enterprises can utilize the record of DNN training process as their ownership proof. To prevent attackers from forging ownership proof, a secure PoT scheme should be able to distinguish honest training records from those forged by attackers. Although existing PoT schemes provide various distinction criteria, these criteria are based on intuitions or observations. The effectiveness of these criteria lacks clear and comprehensive analysis, resulting in existing schemes initially deemed secure being swiftly compromised by simple ideas. In this paper, we make the first move to identify distinction criteria in the style of formal methods, so that their effectiveness can be explicitly demonstrated. Specifically, we conduct systematic modeling to cover a wide range of attacks and then theoretically analyze the distinctions between honest and forged training records. The analysis results not only induce a universal distinction criterion, but also provide detailed reasoning to demonstrate its effectiveness in defending against attacks covered by our model. Guided by the criterion, we propose a generic PoT construction that can be instantiated into concrete schemes. This construction sheds light on the realization that trajectory matching algorithms, previously employed in data distillation, possess significant advantages in PoT construction. Experimental results demonstrate that our scheme can resist attacks that have compromised existing PoT schemes, which corroborates its superiority in security.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yijia Chang",
        "Hongchao Jiang",
        "Chao Lin",
        "Xinyi Huang",
        "Jianping Weng"
      ],
      "url": "https://openalex.org/W4403963863",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774845",
      "fetched_at": "2026-01-09T13:54:34.291735",
      "classified_at": "2026-01-09T14:01:38.498846",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4403963863",
      "doi": "https://doi.org/10.48550/arxiv.2410.04397"
    },
    "seed_ea16c8e7": {
      "paper_id": "seed_ea16c8e7",
      "title": "LightShed: Defeating Perturbation-based Image Copyright Protections",
      "abstract": null,
      "year": 2025,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/usenixsecurity25-foerster.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774847",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_4e0e8713": {
      "paper_id": "seed_4e0e8713",
      "title": "A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks",
      "abstract": "We present a novel attack specifically designed against Tree-Ring, a watermarking technique for diffusion models known for its high imperceptibility and robustness against removal attacks. Unlike previous removal attacks, which rely on strong assumptions about attacker capabilities, our attack only requires access to the variational autoencoder that was used to train the target diffusion model, a component that is often publicly available. By leveraging this variational autoencoder, the attacker can approximate the model's intermediate latent space, enabling more effective surrogate-based attacks. Our evaluation shows that this approach leads to a dramatic reduction in the AUC of Tree-Ring detector's ROC and PR curves, decreasing from 0.993 to 0.153 and from 0.994 to 0.385, respectively, while maintaining high image quality. Notably, our attacks outperform existing methods that assume full access to the diffusion model. These findings highlight the risk of reusing public autoencoders to train diffusion models -- a threat not considered by current industry practices. Furthermore, the results suggest that the Tree-Ring detector's precision, a metric that has been overlooked by previous evaluations, falls short of the requirements for real-world deployment.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Lin, Junhua",
        "Juarez, Marc"
      ],
      "url": "https://openalex.org/W4417351576",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML10",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774850",
      "fetched_at": "2026-01-09T13:54:36.047398",
      "classified_at": "2026-01-09T14:01:40.852971",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4417351576",
      "doi": "https://doi.org/10.48550/arxiv.2506.10502"
    },
    "2103.05633": {
      "paper_id": "2103.05633",
      "title": "Proof-of-Learning: Definitions and Practice",
      "abstract": "Training machine learning (ML) models typically involves expensive iterative optimization. Once the model's final parameters are released, there is currently no mechanism for the entity which trained the model to prove that these parameters were indeed the result of this optimization procedure. Such a mechanism would support security of ML applications in several ways. For instance, it would simplify ownership resolution when multiple parties contest ownership of a specific model. It would also facilitate the distributed training across untrusted workers where Byzantine workers might otherwise mount a denial-of-service by returning incorrect model updates.   In this paper, we remediate this problem by introducing the concept of proof-of-learning in ML. Inspired by research on both proof-of-work and verified computations, we observe how a seminal training algorithm, stochastic gradient descent, accumulates secret information due to its stochasticity. This produces a natural construction for a proof-of-learning which demonstrates that a party has expended the compute require to obtain a set of model parameters correctly. In particular, our analyses and experiments show that an adversary seeking to illegitimately manufacture a proof-of-learning needs to perform *at least* as much work than is needed for gradient descent itself.   We also instantiate a concrete proof-of-learning mechanism in both of the scenarios described above. In model ownership resolution, it protects the intellectual property of models released publicly. In distributed training, it preserves availability of the training procedure. Our empirical evaluation validates that our proof-of-learning mechanism is robust to variance induced by the hardware (ML accelerators) and software stacks.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Hengrui Jia",
        "Mohammad Yaghini",
        "Christopher A. Choquette-Choo",
        "Natalie Dullerud",
        "Anvith Thudi",
        "Varun Chandrasekaran",
        "Nicolas Papernot"
      ],
      "url": "https://arxiv.org/abs/2103.05633",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774853",
      "fetched_at": "2026-01-09T12:14:35.774854",
      "classified_at": "2026-01-09T13:27:35.271151",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_a81c50bf": {
      "paper_id": "seed_a81c50bf",
      "title": "SoK: How Robust is Image Classification Deep Neural Network Watermarking?",
      "abstract": "Deep Neural Network (DNN) watermarking is a method for provenance verification of DNN models. Watermarking should be robust against watermark removal attacks that derive a surrogate model that evades provenance verification. Many watermarking schemes that claim robustness have been proposed, but their robustness is only validated in isolation against a relatively small set of attacks. There is no systematic, empirical evaluation of these claims against a common, comprehensive set of removal attacks. This uncertainty about a watermarking scheme's robustness causes difficulty to trust their deployment in practice. In this paper, we evaluate whether recently proposed watermarking schemes that claim robustness are robust against a large set of removal attacks. We survey methods from the literature that (i) are known removal attacks, (ii) derive surrogate models but have not been evaluated as removal attacks, and (iii) novel removal attacks. Weight shifting and smooth retraining are novel removal attacks adapted to the DNN watermarking schemes surveyed in this paper. We propose taxonomies for watermarking schemes and removal attacks. Our empirical evaluation includes an ablation study over sets of parameters for each attack and watermarking scheme on the CIFAR-10 and ImageNet datasets. Surprisingly, none of the surveyed watermarking schemes is robust in practice. We find that schemes fail to withstand adaptive attacks and known methods for deriving surrogate models that have not been evaluated as removal attacks. This points to intrinsic flaws in how robustness is currently evaluated. We show that watermarking schemes need to be evaluated against a more extensive set of removal attacks with a more realistic adversary model. Our source code and a complete dataset of evaluation results are publicly available, which allows to independently verify our conclusions.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Nils Lukas",
        "Edward Jiang",
        "Xinda Li",
        "Florian Kerschbaum"
      ],
      "url": "https://arxiv.org/abs/2108.04974",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774857",
      "fetched_at": "2026-01-09T13:54:36.738901",
      "classified_at": "2026-01-09T14:01:42.896148",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "2108.04974"
    },
    "seed_6cc17cca": {
      "paper_id": "seed_6cc17cca",
      "title": "Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models",
      "abstract": "Deep learning models, especially those large-scale and high-performance ones, can be very costly to train, demanding a considerable amount of data and computational resources. As a result, deep learning models have become one of the most valuable assets in modern artificial intelligence. Unauthorized duplication or reproduction of deep learning models can lead to copyright infringement and cause huge economic losses to model owners, calling for effective copyright protection techniques. Existing protection techniques are mostly based on watermarking, which embeds an owner-specified watermark into the model. While being able to provide exact ownership verification, these techniques are 1) invasive, i.e., they need to tamper with the training process, which may affect the model utility or introduce new security risks into the model; 2) prone to adaptive attacks that attempt to remove/replace the watermark or adversarially block the retrieval of the watermark; and 3) not robust to the emerging model extraction attacks. Latest fingerprinting work on deep learning models, though being non-invasive, also falls short when facing the diverse and ever-growing attack scenarios.In this paper, we propose a novel testing framework for deep learning copyright protection: DEEPJUDGE. DEEPJUDGE quantitatively tests the similarities between two deep learning models: a victim model and a suspect model. It leverages a diverse set of testing metrics and efficient test case generation algorithms to produce a chain of supporting evidence to help determine whether a suspect model is a copy of the victim model. Advantages of DEEPJUDGE include: 1) non-invasive, as it works directly on the model and does not tamper with the training process; 2) efficient, as it only needs a small set of seed test cases and a quick scan of the two models; 3) flexible, i.e., it can easily incorporate new testing metrics or test case generation methods to obtain more confident and robust judgement; and 4) fairly robust to model extraction attacks and adaptive attacks. We verify the effectiveness of DEEPJUDGE under three typical copyright infringement scenarios, including model finetuning, pruning and extraction, via extensive experiments on both image classification and speech recognition datasets with a variety of model architectures.",
      "year": 2022,
      "venue": "2022 IEEE Symposium on Security and Privacy (SP)",
      "authors": [
        "Jialuo Chen",
        "Jingyi Wang",
        "Tinglan Peng",
        "Youcheng Sun",
        "Peng Cheng",
        "Shouling Ji",
        "Xingjun Ma",
        "Bo Li",
        "Dawn Song"
      ],
      "url": "https://openalex.org/W4200633448",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774860",
      "fetched_at": "2026-01-09T13:54:37.537111",
      "classified_at": "2026-01-09T14:01:44.888459",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4200633448",
      "doi": "https://doi.org/10.1109/sp46214.2022.9833747"
    },
    "2201.11692": {
      "paper_id": "2201.11692",
      "title": "SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders",
      "abstract": "Self-supervised learning is an emerging machine learning paradigm. Compared to supervised learning which leverages high-quality labeled datasets, self-supervised learning relies on unlabeled datasets to pre-train powerful encoders which can then be treated as feature extractors for various downstream tasks. The huge amount of data and computational resources consumption makes the encoders themselves become the valuable intellectual property of the model owner. Recent research has shown that the machine learning model's copyright is threatened by model stealing attacks, which aim to train a surrogate model to mimic the behavior of a given model. We empirically show that pre-trained encoders are highly vulnerable to model stealing attacks. However, most of the current efforts of copyright protection algorithms such as watermarking concentrate on classifiers. Meanwhile, the intrinsic challenges of pre-trained encoder's copyright protection remain largely unstudied. We fill the gap by proposing SSLGuard, the first watermarking scheme for pre-trained encoders. Given a clean pre-trained encoder, SSLGuard injects a watermark into it and outputs a watermarked version. The shadow training technique is also applied to preserve the watermark under potential model stealing attacks. Our extensive evaluation shows that SSLGuard is effective in watermark injection and verification, and it is robust against model stealing and other watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Tianshuo Cong",
        "Xinlei He",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2201.11692",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774863",
      "fetched_at": "2026-01-09T12:14:35.774864",
      "classified_at": "2026-01-09T13:27:35.866767",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_5b1c50c5": {
      "paper_id": "seed_5b1c50c5",
      "title": "RAI2: Responsible Identity Audit Governing the Artificial Intelligence",
      "abstract": "Self-supervised learning is an emerging machine learning paradigm. Compared to supervised learning which leverages high-quality labeled datasets, self-supervised learning relies on unlabeled datasets to pre-train powerful encoders which can then be treated as feature extractors for various downstream tasks. The huge amount of data and computational resources consumption makes the encoders themselves become the valuable intellectual property of the model owner. Recent research has shown that the machine learning model's copyright is threatened by model stealing attacks, which aim to train a surrogate model to mimic the behavior of a given model. We empirically show that pre-trained encoders are highly vulnerable to model stealing attacks. However, most of the current efforts of copyright protection algorithms such as watermarking concentrate on classifiers. Meanwhile, the intrinsic challenges of pre-trained encoder's copyright protection remain largely unstudied. We fill the gap by proposing SSLGuard, the first watermarking scheme for pre-trained encoders. Given a clean pre-trained encoder, SSLGuard injects a watermark into it and outputs a watermarked version. The shadow training technique is also applied to preserve the watermark under potential model stealing attacks. Our extensive evaluation shows that SSLGuard is effective in watermark injection and verification, and it is robust against model stealing and other watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Tianshuo Cong",
        "Xinlei He",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2201.11692",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774866",
      "fetched_at": "2026-01-09T13:54:38.084085",
      "classified_at": "2026-01-09T14:01:47.831537",
      "expanded_at": null,
      "citations_checked_at": null,
      "arxiv_id": "2201.11692"
    },
    "seed_0b75ddff": {
      "paper_id": "seed_0b75ddff",
      "title": "ActiveDaemon: Unconscious DNN Dormancy and Waking Up via User-specific Invisible Token",
      "abstract": "Well-trained deep neural network (DNN) models can be treated as commodities for commercial transactions and generate significant revenues, raising the urgent need for intellectual property (IP) protection against illegitimate reproducing.Emerging studies on IP protection often aim at inserting watermarks into DNNs, allowing owners to passively verify the ownership of target models after counterfeit models appear and commercial benefits are infringed, while active authentication against unauthorized queries of DNN-based applications is still neglected.In this paper, we propose a novel approach to protect model intellectual property, called ActiveDaemon, which incorporates a built-in access control function in DNNs to safeguard against commercial piracy.Specifically, our approach enables DNNs to predict correct outputs only for authorized users with user-specific tokens while producing poor accuracy for unauthorized users.In ActiveDaemon, the user-specific tokens are generated by a specially designed U-Net style encoder-decoder network, which can map strings and input images into numerous noise images to address identity management with large-scale user capacity.Compared to existing studies, these user-specific tokens are invisible, dynamic and more perceptually concealed, enhancing the stealthiness and reliability of model IP protection.To automatically wake up the model accuracy, we utilize the data poisoning-based training technique to unconsciously embed the ActiveDaemon into the neuron's function.We conduct experiments to compare the protection performance of ActiveDaemon with four state-of-the-art approaches over four datasets.The experimental results show that ActiveDaemon can reduce the accuracy of unauthorized queries by as much as 81% with less than a 1.4% decrease in that of authorized queries.Meanwhile, our approach can also reduce the LPIPS scores of the authorized tokens to 0.0027 on CIFAR10 and 0.0368 on ImageNet 1 .",
      "year": 2024,
      "venue": null,
      "authors": [
        "Ge Ren",
        "Gaolei Li",
        "Shenghong Li",
        "Libo Chen",
        "Kui Ren"
      ],
      "url": "https://openalex.org/W4391724751",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774869",
      "fetched_at": "2026-01-09T13:54:38.839499",
      "classified_at": "2026-01-09T14:01:49.650248",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391724751",
      "doi": "https://doi.org/10.14722/ndss.2024.24588"
    },
    "seed_55d356f0": {
      "paper_id": "seed_55d356f0",
      "title": "THEMIS: Towards Practical Intellectual Property Protection for Post-Deployment On-Device Deep Learning Models",
      "abstract": "On-device deep learning (DL) has rapidly gained adoption in mobile apps, offering the benefits of offline model inference and user privacy preservation over cloud-based approaches. However, it inevitably stores models on user devices, introducing new vulnerabilities, particularly model-stealing attacks and intellectual property infringement. While system-level protections like Trusted Execution Environments (TEEs) provide a robust solution, practical challenges remain in achieving scalable on-device DL model protection, including complexities in supporting third-party models and limited adoption in current mobile solutions. Advancements in TEE-enabled hardware, such as NVIDIA's GPU-based TEEs, may address these obstacles in the future. Currently, watermarking serves as a common defense against model theft but also faces challenges here as many mobile app developers lack corresponding machine learning expertise and the inherent read-only and inference-only nature of on-device DL models prevents third parties like app stores from implementing existing watermarking techniques in post-deployment models. To protect the intellectual property of on-device DL models, in this paper, we propose THEMIS, an automatic tool that lifts the read-only restriction of on-device DL models by reconstructing their writable counterparts and leverages the untrainable nature of on-device DL models to solve watermark parameters and protect the model owner's intellectual property. Extensive experimental results across various datasets and model structures show the superiority of THEMIS in terms of different metrics. Further, an empirical investigation of 403 real-world DL mobile apps from Google Play is performed with a success rate of 81.14%, showing the practicality of THEMIS.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Zhi Zhang",
        "Chunyang Chen"
      ],
      "url": "https://openalex.org/W4416413358",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774872",
      "fetched_at": "2026-01-09T13:54:39.399915",
      "classified_at": "2026-01-09T14:01:51.524530",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4416413358",
      "doi": "https://doi.org/10.48550/arxiv.2503.23748"
    },
    "2203.10902": {
      "paper_id": "2203.10902",
      "title": "PublicCheck: Public Integrity Verification for Services of Run-time Deep Models",
      "abstract": "Existing integrity verification approaches for deep models are designed for private verification (i.e., assuming the service provider is honest, with white-box access to model parameters). However, private verification approaches do not allow model users to verify the model at run-time. Instead, they must trust the service provider, who may tamper with the verification results. In contrast, a public verification approach that considers the possibility of dishonest service providers can benefit a wider range of users. In this paper, we propose PublicCheck, a practical public integrity verification solution for services of run-time deep models. PublicCheck considers dishonest service providers, and overcomes public verification challenges of being lightweight, providing anti-counterfeiting protection, and having fingerprinting samples that appear smooth. To capture and fingerprint the inherent prediction behaviors of a run-time model, PublicCheck generates smoothly transformed and augmented encysted samples that are enclosed around the model's decision boundary while ensuring that the verification queries are indistinguishable from normal queries. PublicCheck is also applicable when knowledge of the target model is limited (e.g., with no knowledge of gradients or model parameters). A thorough evaluation of PublicCheck demonstrates the strong capability for model integrity breach detection (100% detection accuracy with less than 10 black-box API queries) against various model integrity attacks and model compression attacks. PublicCheck also demonstrates the smooth appearance, feasibility, and efficiency of generating a plethora of encysted samples for fingerprinting.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Shuo Wang",
        "Sharif Abuadbba",
        "Sidharth Agarwal",
        "Kristen Moore",
        "Ruoxi Sun",
        "Minhui Xue",
        "Surya Nepal",
        "Seyit Camtepe",
        "Salil Kanhere"
      ],
      "url": "https://arxiv.org/abs/2203.10902",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML08",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774875",
      "fetched_at": "2026-01-09T12:14:35.774876",
      "classified_at": "2026-01-09T13:27:36.574781",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2503.09022": {
      "paper_id": "2503.09022",
      "title": "Prompt Inversion Attack against Collaborative Inference of Large Language Models",
      "abstract": "Large language models (LLMs) have been widely applied for their remarkable capability of content generation. However, the practical use of open-source LLMs is hindered by high resource requirements, making deployment expensive and limiting widespread development. The collaborative inference is a promising solution for this problem, in which users collaborate by each hosting a subset of layers and transmitting intermediate activation. Many companies are building collaborative inference platforms to reduce LLM serving costs, leveraging users' underutilized GPUs. Despite widespread interest in collaborative inference within academia and industry, the privacy risks associated with LLM collaborative inference have not been well studied. This is largely because of the challenge posed by inverting LLM activation due to its strong non-linearity.   In this paper, to validate the severity of privacy threats in LLM collaborative inference, we introduce the concept of prompt inversion attack (PIA), where a malicious participant intends to recover the input prompt through the activation transmitted by its previous participant. Extensive experiments show that our PIA method substantially outperforms existing baselines. For example, our method achieves an 88.4\\% token accuracy on the Skytrax dataset with the Llama-65B model when inverting the maximum number of transformer layers, while the best baseline method only achieves 22.8\\% accuracy. The results verify the effectiveness of our PIA attack and highlights its practical threat to LLM collaborative inference systems.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Wenjie Qu",
        "Yuguang Zhou",
        "Yongji Wu",
        "Tingsong Xiao",
        "Binhang Yuan",
        "Yiming Li",
        "Jiaheng Zhang"
      ],
      "url": "https://arxiv.org/abs/2503.09022",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774879",
      "fetched_at": "2026-01-09T12:14:35.774880",
      "classified_at": "2026-01-09T13:27:37.218697",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_9dd9001d": {
      "paper_id": "seed_9dd9001d",
      "title": "On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts",
      "abstract": null,
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2025/223600a392/26hiTFMb8eQ",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774883",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_565523cd": {
      "paper_id": "seed_565523cd",
      "title": "PRSA: Prompt Stealing Attacks against Real-World Prompt Services",
      "abstract": "Recently, large language models (LLMs) have garnered widespread attention for their exceptional capabilities. Prompts are central to the functionality and performance of LLMs, making them highly valuable assets. The increasing reliance on high-quality prompts has driven significant growth in prompt services. However, this growth also expands the potential for prompt leakage, increasing the risk that attackers could replicate original functionalities, create competing products, and severely infringe on developers' intellectual property. Despite these risks, prompt leakage in real-world prompt services remains underexplored. In this paper, we present PRSA, a practical attack framework designed for prompt stealing. PRSA infers the detailed intent of prompts through very limited input-output analysis and can successfully generate stolen prompts that replicate the original functionality. Extensive evaluations demonstrate PRSA's effectiveness across two main types of real-world prompt services. Specifically, compared to previous works, it improves the attack success rate from 17.8% to 46.1% in prompt marketplaces and from 39% to 52% in LLM application stores, respectively. Notably, in the attack on \"Math\", one of the most popular educational applications in OpenAI's GPT Store with over 1 million conversations, PRSA uncovered a hidden Easter egg that had not been revealed previously. Besides, our analysis reveals that higher mutual information between a prompt and its output correlates with an increased risk of leakage. This insight guides the design and evaluation of two potential defenses against the security threats posed by PRSA. We have reported these findings to the prompt service vendors, including PromptBase and OpenAI, and actively collaborate with them to implement defensive measures.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yong Yang",
        "Changjiang Li",
        "Yi Jiang",
        "Xi Chen",
        "Haoyu Wang",
        "Xuhong Zhang",
        "Zonghui Wang",
        "Shouling Ji"
      ],
      "url": "https://openalex.org/W4399554484",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774886",
      "fetched_at": "2026-01-09T13:54:40.686539",
      "classified_at": "2026-01-09T14:01:53.434469",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4399554484",
      "doi": "https://doi.org/10.48550/arxiv.2402.19200"
    },
    "seed_9b4e3778": {
      "paper_id": "seed_9b4e3778",
      "title": "Cross-Modal Prompt Inversion: Unifying Threats to Text and Image Generative AI Models",
      "abstract": "Abstract Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this \u201chierarchical prediction machine\u201d approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.",
      "year": 2013,
      "venue": "Behavioral and Brain Sciences",
      "authors": [
        "Andy Clark"
      ],
      "url": "https://openalex.org/W2153791616",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774889",
      "fetched_at": "2026-01-09T13:54:41.490995",
      "classified_at": "2026-01-09T14:01:55.329801",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W2153791616",
      "doi": "https://doi.org/10.1017/s0140525x12000477"
    },
    "seed_8a4be885": {
      "paper_id": "seed_8a4be885",
      "title": "Prompt Obfuscation for Large Language Models",
      "abstract": "System prompts that include detailed instructions to describe the task performed by the underlying LLM can easily transform foundation models into tools and services with minimal overhead. They are often considered intellectual property, similar to the code of a software product, because of their crucial impact on the utility. However, extracting system prompts is easily possible. As of today, there is no effective countermeasure to prevent the stealing of system prompts, and all safeguarding efforts could be evaded. In this work, we propose an alternative to conventional system prompts. We introduce prompt obfuscation to prevent the extraction of the system prompt with little overhead. The core idea is to find a representation of the original system prompt that leads to the same functionality, while the obfuscated system prompt does not contain any information that allows conclusions to be drawn about the original system prompt. We evaluate our approach by comparing our obfuscated prompt output with the output of the original prompt, using eight distinct metrics to measure the lexical, character-level, and semantic similarity. We show that the obfuscated version is constantly on par with the original one. We further perform three different deobfuscation attacks with varying attacker knowledge--covering both black-box and white-box conditions--and show that in realistic attack scenarios an attacker is unable to extract meaningful information. Overall, we demonstrate that prompt obfuscation is an effective mechanism to safeguard the intellectual property of a system prompt while maintaining the same utility as the original prompt.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "David Pape",
        "Thorsten Eisenhofer",
        "Lea Sch\u00f6nherr"
      ],
      "url": "https://openalex.org/W4403713467",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774892",
      "fetched_at": "2026-01-09T13:54:42.018758",
      "classified_at": "2026-01-09T14:01:57.271240",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4403713467",
      "doi": "https://doi.org/10.48550/arxiv.2409.11026"
    },
    "2503.09291": {
      "paper_id": "2503.09291",
      "title": "Prompt Inference Attack on Distributed Large Language Model Inference Frameworks",
      "abstract": "The inference process of modern large language models (LLMs) demands prohibitive computational resources, rendering them infeasible for deployment on consumer-grade devices. To address this limitation, recent studies propose distributed LLM inference frameworks, which employ split learning principles to enable collaborative LLM inference on resource-constrained hardware. However, distributing LLM layers across participants requires the transmission of intermediate outputs, which may introduce privacy risks to the original input prompts - a critical issue that has yet to be thoroughly explored in the literature.   In this paper, we rigorously examine the privacy vulnerabilities of distributed LLM inference frameworks by designing and evaluating three prompt inference attacks aimed at reconstructing input prompts from intermediate LLM outputs. These attacks are developed under various query and data constraints to reflect diverse real-world LLM service scenarios. Specifically, the first attack assumes an unlimited query budget and access to an auxiliary dataset sharing the same distribution as the target prompts. The second attack also leverages unlimited queries but uses an auxiliary dataset with a distribution differing from the target prompts. The third attack operates under the most restrictive scenario, with limited query budgets and no auxiliary dataset available. We evaluate these attacks on a range of LLMs, including state-of-the-art models such as Llama-3.2 and Phi-3.5, as well as widely-used models like GPT-2 and BERT for comparative analysis. Our experiments show that the first two attacks achieve reconstruction accuracies exceeding 90%, while the third achieves accuracies typically above 50%, even under stringent constraints. These findings highlight privacy risks in distributed LLM inference frameworks, issuing a strong alert on their deployment in real-world applications.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Xinjian Luo",
        "Ting Yu",
        "Xiaokui Xiao"
      ],
      "url": "https://arxiv.org/abs/2503.09291",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774894",
      "fetched_at": "2026-01-09T12:14:35.774895",
      "classified_at": "2026-01-09T13:27:37.979561",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_5ab2a990": {
      "paper_id": "seed_5ab2a990",
      "title": "Codebreaker: Dynamic Extraction Attacks on Code Language Models",
      "abstract": null,
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://ieeexplore.ieee.org/document/11023359",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774898",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_0472ddb1": {
      "paper_id": "seed_0472ddb1",
      "title": "LLMmap: Fingerprinting for Large Language Models",
      "abstract": "We introduce LLMmap, a first-generation fingerprinting technique targeted at LLM-integrated applications. LLMmap employs an active fingerprinting approach, sending carefully crafted queries to the application and analyzing the responses to identify the specific LLM version in use. Our query selection is informed by domain expertise on how LLMs generate uniquely identifiable responses to thematically varied prompts. With as few as 8 interactions, LLMmap can accurately identify 42 different LLM versions with over 95% accuracy. More importantly, LLMmap is designed to be robust across different application layers, allowing it to identify LLM versions--whether open-source or proprietary--from various vendors, operating under various unknown system prompts, stochastic sampling hyperparameters, and even complex generation frameworks such as RAG or Chain-of-Thought. We discuss potential mitigations and demonstrate that, against resourceful adversaries, effective countermeasures may be challenging or even unrealizable.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Dario Pasquini",
        "Evgenios M. Kornaropoulos",
        "Giuseppe Ateniese"
      ],
      "url": "https://openalex.org/W4402856935",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774901",
      "fetched_at": "2026-01-09T13:54:43.283560",
      "classified_at": "2026-01-09T14:01:59.100257",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4402856935",
      "doi": "https://doi.org/10.48550/arxiv.2407.15847"
    },
    "seed_cb5663e4": {
      "paper_id": "seed_cb5663e4",
      "title": "Unlocking the Power of Differentially Private Zeroth-order Optimization for Fine-tuning LLMs",
      "abstract": "Vertical Federated Learning (VFL) is a privacy-preserving distributed learning paradigm where different parties collaboratively learn models using partitioned features of shared samples, without leaking private data. Recent research has shown promising results addressing various challenges in VFL, highlighting its potential for practical applications in cross-domain collaboration. However, the corresponding research is scattered and lacks organization. To advance VFL research, this survey offers a systematic overview of recent developments. First, we provide a history and background introduction, along with a summary of the general training protocol of VFL. We then revisit the taxonomy in recent reviews and analyze limitations in-depth. For a comprehensive and structured discussion, we synthesize recent research from three fundamental perspectives: effectiveness, security, and applicability. Finally, we discuss several critical future research directions in VFL, which will facilitate the developments in this field. We provide a collection of research lists and periodically update them at https://github.com/shentt67/VFL_Survey.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Mang Ye",
        "Wei Shen",
        "Eduard Snezhko",
        "Vassili Kovalev",
        "Pong C. Yuen",
        "Bo Du"
      ],
      "url": "https://openalex.org/W4399151237",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774903",
      "fetched_at": "2026-01-09T13:54:43.806957",
      "classified_at": "2026-01-09T14:02:00.906457",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4399151237",
      "doi": "https://doi.org/10.48550/arxiv.2405.17495"
    },
    "seed_9139fe98": {
      "paper_id": "seed_9139fe98",
      "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion",
      "abstract": "Large Language Models (LLMs) are increasingly integrated into daily routines, yet they raise significant privacy and safety concerns. Recent research proposes collaborative inference, which outsources the early-layer inference to ensure data locality, and introduces model safety auditing based on inner neuron patterns. Both techniques expose the LLM's Internal States (ISs), which are traditionally considered irreversible to inputs due to optimization challenges and the highly abstract representations in deep layers. In this work, we challenge this assumption by proposing four inversion attacks that significantly improve the semantic similarity and token matching rate of inverted inputs. Specifically, we first develop two white-box optimization-based attacks tailored for low-depth and high-depth ISs. These attacks avoid local minima convergence, a limitation observed in prior work, through a two-phase inversion process. Then, we extend our optimization attack under more practical black-box weight access by leveraging the transferability between the source and the derived LLMs. Additionally, we introduce a generation-based attack that treats inversion as a translation task, employing an inversion model to reconstruct inputs. Extensive evaluation of short and long prompts from medical consulting and coding assistance datasets and 6 LLMs validates the effectiveness of our inversion attacks. Notably, a 4,112-token long medical consulting prompt can be nearly perfectly inverted with 86.88 F1 token matching from the middle layer of Llama-3 model. Finally, we evaluate four practical defenses that we found cannot perfectly prevent ISs inversion and draw conclusions for future mitigation design.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Tian Dong",
        "Yan Meng",
        "Shaofeng Li",
        "Guoxing Chen",
        "Zhen Liu",
        "Haojin Zhu"
      ],
      "url": "https://openalex.org/W4414421755",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774907",
      "fetched_at": "2026-01-09T13:54:44.330247",
      "classified_at": "2026-01-09T14:02:02.734670",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4414421755",
      "doi": "https://doi.org/10.48550/arxiv.2507.16372"
    },
    "seed_2c81920f": {
      "paper_id": "seed_2c81920f",
      "title": "Evaluating LLM-based Personal Information Extraction and Countermeasures",
      "abstract": "Automatically extracting personal information -- such as name, phone number, and email address -- from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing. Traditional methods -- such as regular expression, keyword search, and entity detection -- achieve limited success at such personal information extraction. In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures. Towards this goal, we present a framework for LLM-based extraction attacks; collect four datasets including a synthetic dataset generated by GPT-4 and three real-world datasets with manually labeled eight categories of personal information; introduce a novel mitigation strategy based on prompt injection; and systematically benchmark LLM-based attacks and countermeasures using ten LLMs and five datasets. Our key findings include: LLM can be misused by attackers to accurately extract various personal information from personal profiles; LLM outperforms traditional methods; and prompt injection can defend against strong LLM-based attacks, reducing the attack to less effective traditional ones.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yingzhen Liu",
        "Y. Jia",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://openalex.org/W4402427674",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML05",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774909",
      "fetched_at": "2026-01-09T13:54:44.836045",
      "classified_at": "2026-01-09T14:02:04.662280",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4402427674",
      "doi": "https://doi.org/10.48550/arxiv.2408.07291"
    },
    "seed_5d56148b": {
      "paper_id": "seed_5d56148b",
      "title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty",
      "abstract": "Large Language Models (LLMs) are widely used in sensitive domains, including healthcare, finance, and legal services, raising concerns about potential private information leaks during inference. Privacy extraction attacks, such as jailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the models to output sensitive information. However, these attacks cannot verify whether the extracted private information is accurate, as no public datasets exist for cross-validation, leaving a critical gap in private information detection during inference. To address this, we propose PrivacyXray, a novel framework detecting privacy breaches by analyzing LLM inner states. Our analysis reveals that LLMs exhibit higher semantic coherence and probabilistic certainty when generating correct private outputs. Based on this, PrivacyXray detects privacy breaches using four metrics: intra-layer and inter-layer semantic similarity, token-level and sentence-level probability distributions. PrivacyXray addresses critical challenges in private information detection by overcoming the lack of open-source private datasets and eliminating reliance on external data for validation. It achieves this through the synthesis of realistic private data and a detection mechanism based on the inner states of LLMs. Experiments show that PrivacyXray achieves consistent performance, with an average accuracy of 92.69% across five LLMs. Compared to state-of-the-art methods, PrivacyXray achieves significant improvements, with an average accuracy increase of 20.06%, highlighting its stability and practical utility in real-world applications.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jinwen He",
        "Yi Lu",
        "Zijin Lin",
        "Kai Chen",
        "Yue Zhao"
      ],
      "url": "https://openalex.org/W4414684146",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774912",
      "fetched_at": "2026-01-09T13:54:45.370682",
      "classified_at": "2026-01-09T14:02:07.523713",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4414684146",
      "doi": "https://doi.org/10.48550/arxiv.2506.19563"
    },
    "seed_55d8825e": {
      "paper_id": "seed_55d8825e",
      "title": "Synthetic Artifact Auditing: Tracing LLM-Generated Synthetic Data Usage in Downstream Applications",
      "abstract": "Large language models (LLMs) have facilitated the generation of high-quality, cost-effective synthetic data for developing downstream models and conducting statistical analyses in various domains. However, the increased reliance on synthetic data may pose potential negative impacts. Numerous studies have demonstrated that LLM-generated synthetic data can perpetuate and even amplify societal biases and stereotypes, and produce erroneous outputs known as ``hallucinations'' that deviate from factual knowledge. In this paper, we aim to audit artifacts, such as classifiers, generators, or statistical plots, to identify those trained on or derived from synthetic data and raise user awareness, thereby reducing unexpected consequences and risks in downstream applications. To this end, we take the first step to introduce synthetic artifact auditing to assess whether a given artifact is derived from LLM-generated synthetic data. We then propose an auditing framework with three methods including metric-based auditing, tuning-based auditing, and classification-based auditing. These methods operate without requiring the artifact owner to disclose proprietary training details. We evaluate our auditing framework on three text classification tasks, two text summarization tasks, and two data visualization tasks across three training scenarios. Our evaluation demonstrates the effectiveness of all proposed auditing methods across all these tasks. For instance, black-box metric-based auditing can achieve an average accuracy of $0.868 \\pm 0.071$ for auditing classifiers and $0.880 \\pm 0.052$ for auditing generators using only 200 random queries across three scenarios. We hope our research will enhance model transparency and regulatory compliance, ensuring the ethical and responsible use of synthetic data.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yixin Wu",
        "Ziqing Yang",
        "Yun Shen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W4407124110",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML09",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774914",
      "fetched_at": "2026-01-09T13:54:46.190348",
      "classified_at": "2026-01-09T14:02:09.475255",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4407124110",
      "doi": "https://doi.org/10.48550/arxiv.2502.00808"
    },
    "seed_c267b2cc": {
      "paper_id": "seed_c267b2cc",
      "title": "Whispering Under the Eaves: Protecting User Privacy Against Commercial and LLM-powered Automatic Speech Recognition Systems",
      "abstract": "The widespread application of automatic speech recognition (ASR) supports large-scale voice surveillance, raising concerns about privacy among users. In this paper, we concentrate on using adversarial examples to mitigate unauthorized disclosure of speech privacy thwarted by potential eavesdroppers in speech communications. While audio adversarial examples have demonstrated the capability to mislead ASR models or evade ASR surveillance, they are typically constructed through time-intensive offline optimization, restricting their practicality in real-time voice communication. Recent work overcame this limitation by generating universal adversarial perturbations (UAPs) and enhancing their transferability for black-box scenarios. However, they introduced excessive noise that significantly degrades audio quality and affects human perception, thereby limiting their effectiveness in practical scenarios. To address this limitation and protect live users' speech against ASR systems, we propose a novel framework, AudioShield. Central to this framework is the concept of Transferable Universal Adversarial Perturbations in the Latent Space (LS-TUAP). By transferring the perturbations to the latent space, the audio quality is preserved to a large extent. Additionally, we propose target feature adaptation to enhance the transferability of UAPs by embedding target text features into the perturbations. Comprehensive evaluation on four commercial ASR APIs (Google, Amazon, iFlytek, and Alibaba), three voice assistants, two LLM-powered ASR and one NN-based ASR demonstrates the protection superiority of AudioShield over existing competitors, and both objective and subjective evaluations indicate that AudioShield significantly improves the audio quality. Moreover, AudioShield also shows high effectiveness in real-time end-to-end scenarios, and demonstrates strong resilience against adaptive countermeasures.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Weifei Jin",
        "Yuxin Cao",
        "J.-C. Su",
        "Derui Wang",
        "Yedi Zhang",
        "Minhui Xue",
        "Jie Hao",
        "Jin Song Dong",
        "Yixian Yang"
      ],
      "url": "https://openalex.org/W4417229822",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774917",
      "fetched_at": "2026-01-09T13:54:46.719939",
      "classified_at": "2026-01-09T14:02:12.455376",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4417229822",
      "doi": "https://doi.org/10.48550/arxiv.2504.00858"
    },
    "seed_0c25d4b0": {
      "paper_id": "seed_0c25d4b0",
      "title": "Effective PII Extraction from LLMs through Augmented Few-Shot Learning",
      "abstract": "Abstract Educational technology innovations leveraging large language models (LLMs) have shown the potential to automate the laborious process of generating and analysing textual content. While various innovations have been developed to automate a range of educational tasks (eg, question generation, feedback provision, and essay grading), there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs\u2010based innovations in authentic educational contexts. To address this, we conducted a systematic scoping review of 118 peer\u2010reviewed papers published since 2017 to pinpoint the current state of research on using LLMs to automate and support educational tasks. The findings revealed 53 use cases for LLMs in automating education tasks, categorised into nine main categories: profiling/labelling, detection, grading, teaching support, prediction, knowledge representation, feedback, content generation, and recommendation. Additionally, we also identified several practical and ethical challenges, including low technological readiness, lack of replicability and transparency and insufficient privacy and beneficence considerations. The findings were summarised into three recommendations for future studies, including updating existing innovations with state\u2010of\u2010the\u2010art models (eg, GPT\u20103/4), embracing the initiative of open\u2010sourcing models/systems, and adopting a human\u2010centred approach throughout the developmental process. As the intersection of AI and education is continuously evolving, the findings of this study can serve as an essential reference point for researchers, allowing them to leverage the strengths, learn from the limitations, and uncover potential research opportunities enabled by ChatGPT and other generative AI models. Practitioner notes What is currently known about this topic Generating and analysing text\u2010based content are time\u2010consuming and laborious tasks. Large language models are capable of efficiently analysing an unprecedented amount of textual content and completing complex natural language processing and generation tasks. Large language models have been increasingly used to develop educational technologies that aim to automate the generation and analysis of textual content, such as automated question generation and essay scoring. What this paper adds A comprehensive list of different educational tasks that could potentially benefit from LLMs\u2010based innovations through automation. A structured assessment of the practicality and ethicality of existing LLMs\u2010based innovations from seven important aspects using established frameworks. Three recommendations that could potentially support future studies to develop LLMs\u2010based innovations that are practical and ethical to implement in authentic educational contexts. Implications for practice and/or policy Updating existing innovations with state\u2010of\u2010the\u2010art models may further reduce the amount of manual effort required for adapting existing models to different educational tasks. The reporting standards of empirical research that aims to develop educational technologies using large language models need to be improved. Adopting a human\u2010centred approach throughout the developmental process could contribute to resolving the practical and ethical challenges of large language models in education.",
      "year": 2023,
      "venue": "British Journal of Educational Technology",
      "authors": [
        "Lixiang Yan",
        "Lele Sha",
        "Linxuan Zhao",
        "Yuheng Li",
        "Roberto Mart\u00ednez\u2010Maldonado",
        "Guanliang Chen",
        "Xinyu Li",
        "Yueqiao Jin",
        "Dragan Ga\u0161evi\u0107"
      ],
      "url": "https://openalex.org/W4385632485",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774920",
      "fetched_at": "2026-01-09T13:54:47.525199",
      "classified_at": "2026-01-09T14:02:14.274918",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385632485",
      "doi": "https://doi.org/10.1111/bjet.13370"
    },
    "seed_9cbf640c": {
      "paper_id": "seed_9cbf640c",
      "title": "Private Investigator: Extracting Personally Identifiable Information from Large Language Models Using Optimized Prompts",
      "abstract": "Integrating Artificial Intelligence (AI) in healthcare represents a transformative shift with substantial potential for enhancing patient care. This paper critically examines this integration, confronting significant ethical, legal, and technological challenges, particularly in patient privacy, decision-making autonomy, and data integrity. A structured exploration of these issues focuses on Differential Privacy as a critical method for preserving patient confidentiality in AI-driven healthcare systems. We analyze the balance between privacy preservation and the practical utility of healthcare data, emphasizing the effectiveness of encryption, Differential Privacy, and mixed-model approaches. The paper navigates the complex ethical and legal frameworks essential for AI integration in healthcare. We comprehensively examine patient rights and the nuances of informed consent, along with the challenges of harmonizing advanced technologies like blockchain with the General Data Protection Regulation (GDPR). The issue of algorithmic bias in healthcare is also explored, underscoring the urgent need for effective bias detection and mitigation strategies to build patient trust. The evolving roles of decentralized data sharing, regulatory frameworks, and patient agency are discussed in depth. Advocating for an interdisciplinary, multi-stakeholder approach and responsive governance, the paper aims to align healthcare AI with ethical principles, prioritize patient-centered outcomes, and steer AI towards responsible and equitable enhancements in patient care.",
      "year": 2024,
      "venue": "Applied Sciences",
      "authors": [
        "S. Williamson",
        "Victor R. Prybutok"
      ],
      "url": "https://openalex.org/W4390829176",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774922",
      "fetched_at": "2026-01-09T13:54:48.286514",
      "classified_at": "2026-01-09T14:02:16.255423",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4390829176",
      "doi": "https://doi.org/10.3390/app14020675"
    },
    "seed_7b93c37c": {
      "paper_id": "seed_7b93c37c",
      "title": "Fawkes: Protecting Privacy against Unauthorized Deep Learning Models",
      "abstract": "Today's proliferation of powerful facial recognition systems poses a real threat to personal privacy. As Clearview.ai demonstrated, anyone can canvas the Internet for data and train highly accurate facial recognition models of individuals without their knowledge. We need tools to protect ourselves from potential misuses of unauthorized facial recognition systems. Unfortunately, no practical or effective solutions exist. In this paper, we propose Fawkes, a system that helps individuals inoculate their images against unauthorized facial recognition models. Fawkes achieves this by helping users add imperceptible pixel-level changes (we call them \"cloaks\") to their own photos before releasing them. When used to train facial recognition models, these \"cloaked\" images produce functional models that consistently cause normal images of the user to be misidentified. We experimentally demonstrate that Fawkes provides 95+% protection against user recognition regardless of how trackers train their models. Even when clean, uncloaked images are \"leaked\" to the tracker and used for training, Fawkes can still maintain an 80+% protection success rate. We achieve 100% success in experiments against today's state-of-the-art facial recognition services. Finally, we show that Fawkes is robust against a variety of countermeasures that try to detect or disrupt image cloaks.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Shawn Shan",
        "Emily Wenger",
        "Jiayun Zhang",
        "Huiying Li",
        "Hai-Tao Zheng",
        "Ben Y. Zhao"
      ],
      "url": "https://openalex.org/W3036952806",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774925",
      "fetched_at": "2026-01-09T13:54:49.100091",
      "classified_at": "2026-01-09T14:02:18.053754",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3036952806",
      "doi": "https://doi.org/10.48550/arxiv.2002.08327"
    },
    "seed_0da85706": {
      "paper_id": "seed_0da85706",
      "title": "Automatically Detecting Bystanders in Photos to Reduce Privacy Risks",
      "abstract": "Photographs taken in public places often contain bystanders~-- people who are not the main subject of a photo. These photos, when shared online, can reach a large number of viewers and potentially undermine the bystanders' privacy. Furthermore, recent developments in computer vision and machine learning can be used by online platforms to identify and track individuals. To combat this problem, researchers have proposed technical solutions that require bystanders to be proactive and use specific devices and/or applications to broadcast their privacy policy and identifying information while being located in an image. We explore the prospect of a different approach~-- identifying bystanders solely based on the visual information present in an image. Through an online user study, we catalog the rationale humans use to classify subjects and bystanders in an image, and systematically validate a set of intuitive concepts (such as intentionally posing for a photo) that can be used to automatically identify bystanders. Using image data, we infer those concepts and then use them to train several classifier models. We extensively evaluate the models and compare them with human raters. On our training data set, which features a 10-fold cross validation, our best model achieves a mean detection accuracy of 93% for images when human raters have 100% agreement on the class label and 80% when the agreement is only 67%. We validate this model on a completely different test data set and achieve similar results, demonstrating that our model generalizes well.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Rakibul Hasan",
        "David Crandall",
        "Mario Fritz",
        "Apu Kapadia"
      ],
      "url": "https://openalex.org/W3014111715",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774929",
      "fetched_at": "2026-01-09T13:54:49.925569",
      "classified_at": "2026-01-09T14:02:19.933650",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3014111715",
      "doi": "https://doi.org/10.1109/sp40000.2020.00097"
    },
    "seed_19c97cc2": {
      "paper_id": "seed_19c97cc2",
      "title": "Characterizing and Detecting Non-Consensual Photo Sharing on Social Networks",
      "abstract": "Photo capturing and sharing have become routine daily activities for social platform users. Alongside the entertainment of social interaction, we are experiencing tremendous visual violation and photo abusing. Especially, users may be unconsciously filmed and exposed online, which is termed as the non-consensual sharing issue. Unfortunately, this problem cannot be well handled with proactive access control or dedicated bystander detection, as users are unaware of their situations and may be filmed stealthily. We propose Videre on behalf of the privacy of the unaware parties in a way that they would be automatically identified and warned before such photos go public. For this, we first elaborate on the predominant features encountered in non-consensual captured photos via a thorough user study. Then we establish a dataset for this context and build a classifier as a proactive detector based on multi-deep-feature fusion. To relieve the burden of person-wise unawareness detection, we further design a signature-based filter for local pre-authorization, which can also implicitly avoid classification errors. We implement and test Videre in various field settings to demonstrate its effectiveness and performance.",
      "year": 2022,
      "venue": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
      "authors": [
        "Tengfei Zheng",
        "Tongqing Zhou",
        "Qiang Liu",
        "Kui Wu",
        "Zhiping Cai"
      ],
      "url": "https://openalex.org/W4308381123",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774931",
      "fetched_at": "2026-01-09T13:54:50.706517",
      "classified_at": "2026-01-09T14:02:22.785495",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4308381123",
      "doi": "https://doi.org/10.1145/3548606.3560571"
    },
    "seed_63696f9d": {
      "paper_id": "seed_63696f9d",
      "title": "Fairness Properties of Face Recognition and Obfuscation Systems",
      "abstract": "The proliferation of automated face recognition in the commercial and government sectors has caused significant privacy concerns for individuals. One approach to address these privacy concerns is to employ evasion attacks against the metric embedding networks powering face recognition systems: Face obfuscation systems generate imperceptibly perturbed images that cause face recognition systems to misidentify the user. Perturbed faces are generated on metric embedding networks, which are known to be unfair in the context of face recognition. A question of demographic fairness naturally follows: are there demographic disparities in face obfuscation system performance? We answer this question with an analytical and empirical exploration of recent face obfuscation systems. Metric embedding networks are found to be demographically aware: face embeddings are clustered by demographic. We show how this clustering behavior leads to reduced face obfuscation utility for faces in minority groups. An intuitive analytical model yields insight into these phenomena.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Harrison Rosenberg",
        "Brian Tang",
        "Kassem Fawaz",
        "Somesh Jha"
      ],
      "url": "https://openalex.org/W3188750497",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML01",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774935",
      "fetched_at": "2026-01-09T13:54:51.485345",
      "classified_at": "2026-01-09T14:02:24.856067",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3188750497",
      "doi": "https://doi.org/10.48550/arxiv.2108.02707"
    },
    "2005.10296": {
      "paper_id": "2005.10296",
      "title": "SWIFT: Super-fast and Robust Privacy-Preserving Machine Learning",
      "abstract": "Performing machine learning (ML) computation on private data while maintaining data privacy, aka Privacy-preserving Machine Learning~(PPML), is an emergent field of research. Recently, PPML has seen a visible shift towards the adoption of the Secure Outsourced Computation~(SOC) paradigm due to the heavy computation that it entails. In the SOC paradigm, computation is outsourced to a set of powerful and specially equipped servers that provide service on a pay-per-use basis. In this work, we propose SWIFT, a robust PPML framework for a range of ML algorithms in SOC setting, that guarantees output delivery to the users irrespective of any adversarial behaviour. Robustness, a highly desirable feature, evokes user participation without the fear of denial of service.   At the heart of our framework lies a highly-efficient, maliciously-secure, three-party computation (3PC) over rings that provides guaranteed output delivery (GOD) in the honest-majority setting. To the best of our knowledge, SWIFT is the first robust and efficient PPML framework in the 3PC setting. SWIFT is as fast as (and is strictly better in some cases than) the best-known 3PC framework BLAZE (Patra et al. NDSS'20), which only achieves fairness. We extend our 3PC framework for four parties (4PC). In this regime, SWIFT is as fast as the best known fair 4PC framework Trident (Chaudhari et al. NDSS'20) and twice faster than the best-known robust 4PC framework FLASH (Byali et al. PETS'20).   We demonstrate our framework's practical relevance by benchmarking popular ML algorithms such as Logistic Regression and deep Neural Networks such as VGG16 and LeNet, both over a 64-bit ring in a WAN setting. For deep NN, our results testify to our claims that we provide improved security guarantee while incurring no additional overhead for 3PC and obtaining 2x improvement for 4PC.",
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [
        "Nishat Koti",
        "Mahak Pancholi",
        "Arpita Patra",
        "Ajith Suresh"
      ],
      "url": "https://arxiv.org/abs/2005.10296",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774938",
      "fetched_at": "2026-01-09T12:14:35.774939",
      "classified_at": "2026-01-09T13:27:38.684937",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_9ab7596f": {
      "paper_id": "seed_9ab7596f",
      "title": "BLAZE: Blazing Fast Privacy-Preserving Machine Learning",
      "abstract": "Machine learning tools have illustrated their potential in many significant sectors such as healthcare and finance, to aide in deriving useful inferences. The sensitive and confidential nature of the data, in such sectors, raise natural concerns for the privacy of data. This motivated the area of Privacy-preserving Machine Learning (PPML) where privacy of the data is guaranteed. Typically, ML techniques require large computing power, which leads clients with limited infrastructure to rely on the method of Secure Outsourced Computation (SOC). In SOC setting, the computation is outsourced to a set of specialized and powerful cloud servers and the service is availed on a pay-per-use basis. In this work, we explore PPML techniques in the SOC setting for widely used ML algorithms-- Linear Regression, Logistic Regression, and Neural Networks. We propose BLAZE, a blazing fast PPML framework in the three server setting tolerating one malicious corruption over a ring (\\Z{\\ell}). BLAZE achieves the stronger security guarantee of fairness (all honest servers get the output whenever the corrupt server obtains the same). Leveraging an input-independent preprocessing phase, BLAZE has a fast input-dependent online phase relying on efficient PPML primitives such as: (i) A dot product protocol for which the communication in the online phase is independent of the vector size, the first of its kind in the three server setting; (ii) A method for truncation that shuns evaluating expensive circuit for Ripple Carry Adders (RCA) and achieves a constant round complexity. This improves over the truncation method of ABY3 (Mohassel et al., CCS 2018) that uses RCA and consumes a round complexity that is of the order of the depth of RCA. An extensive benchmarking of BLAZE for the aforementioned ML algorithms over a 64-bit ring in both WAN and LAN settings shows massive improvements over ABY3.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Arpita Patra",
        "Ajith Suresh"
      ],
      "url": "https://openalex.org/W3003823166",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774941",
      "fetched_at": "2026-01-09T13:54:52.268604",
      "classified_at": "2026-01-09T14:02:26.786804",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3003823166",
      "doi": "https://doi.org/10.14722/ndss.2020.24202"
    },
    "2210.01988": {
      "paper_id": "2210.01988",
      "title": "Bicoptor: Two-round Secure Three-party Non-linear Computation without Preprocessing for Privacy-preserving Machine Learning",
      "abstract": "The overhead of non-linear functions dominates the performance of the secure multiparty computation (MPC) based privacy-preserving machine learning (PPML). This work introduces a family of novel secure three-party computation (3PC) protocols, Bicoptor, which improve the efficiency of evaluating non-linear functions. The basis of Bicoptor is a new sign determination protocol, which relies on a clever use of the truncation protocol proposed in SecureML (S\\&P 2017). Our 3PC sign determination protocol only requires two communication rounds, and does not involve any preprocessing. Such sign determination protocol is well-suited for computing non-linear functions in PPML, e.g. the activation function ReLU, Maxpool, and their variants. We develop suitable protocols for these non-linear functions, which form a family of GPU-friendly protocols, Bicoptor. All Bicoptor protocols only require two communication rounds without preprocessing. We evaluate Bicoptor under a 3-party LAN network over a public cloud, and achieve more than 370,000 DReLU/ReLU or 41,000 Maxpool (find the maximum value of nine inputs) operations per second. Under the same settings and environment, our ReLU protocol has a one or even two orders of magnitude improvement to the state-of-the-art works, Falcon (PETS 2021) or Edabits (CRYPTO 2020), respectively without batch processing.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Lijing Zhou",
        "Ziyu Wang",
        "Hongrui Cui",
        "Qingrui Song",
        "Yu Yu"
      ],
      "url": "https://arxiv.org/abs/2210.01988",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774944",
      "fetched_at": "2026-01-09T12:14:35.774945",
      "classified_at": "2026-01-09T13:27:39.338794",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2406.07948": {
      "paper_id": "2406.07948",
      "title": "Ents: An Efficient Three-party Training Framework for Decision Trees by Communication Optimization",
      "abstract": "Multi-party training frameworks for decision trees based on secure multi-party computation enable multiple parties to train high-performance models on distributed private data with privacy preservation. The training process essentially involves frequent dataset splitting according to the splitting criterion (e.g. Gini impurity). However, existing multi-party training frameworks for decision trees demonstrate communication inefficiency due to the following issues: (1) They suffer from huge communication overhead in securely splitting a dataset with continuous attributes. (2) They suffer from huge communication overhead due to performing almost all the computations on a large ring to accommodate the secure computations for the splitting criterion.   In this paper, we are motivated to present an efficient three-party training framework, namely Ents, for decision trees by communication optimization. For the first issue, we present a series of training protocols based on the secure radix sort protocols to efficiently and securely split a dataset with continuous attributes. For the second issue, we propose an efficient share conversion protocol to convert shares between a small ring and a large ring to reduce the communication overhead incurred by performing almost all the computations on a large ring. Experimental results from eight widely used datasets show that Ents outperforms state-of-the-art frameworks by $5.5\\times \\sim 9.3\\times$ in communication sizes and $3.9\\times \\sim 5.3\\times$ in communication rounds. In terms of training time, Ents yields an improvement of $3.5\\times \\sim 6.7\\times$. To demonstrate its practicality, Ents requires less than three hours to securely train a decision tree on a widely used real-world dataset (Skin Segmentation) with more than 245,000 samples in the WAN setting.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Guopeng Lin",
        "Weili Han",
        "Wenqiang Ruan",
        "Ruisheng Zhou",
        "Lushan Song",
        "Bingshuai Li",
        "Yunfeng Shao"
      ],
      "url": "https://arxiv.org/abs/2406.07948",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774948",
      "fetched_at": "2026-01-09T12:14:35.774949",
      "classified_at": "2026-01-09T13:27:39.987001",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "1912.02631": {
      "paper_id": "1912.02631",
      "title": "Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning",
      "abstract": "Machine learning has started to be deployed in fields such as healthcare and finance, which propelled the need for and growth of privacy-preserving machine learning (PPML). We propose an actively secure four-party protocol (4PC), and a framework for PPML, showcasing its applications on four of the most widely-known machine learning algorithms -- Linear Regression, Logistic Regression, Neural Networks, and Convolutional Neural Networks. Our 4PC protocol tolerating at most one malicious corruption is practically efficient as compared to the existing works. We use the protocol to build an efficient mixed-world framework (Trident) to switch between the Arithmetic, Boolean, and Garbled worlds. Our framework operates in the offline-online paradigm over rings and is instantiated in an outsourced setting for machine learning. Also, we propose conversions especially relevant to privacy-preserving machine learning. The highlights of our framework include using a minimal number of expensive circuits overall as compared to ABY3. This can be seen in our technique for truncation, which does not affect the online cost of multiplication and removes the need for any circuits in the offline phase. Our B2A conversion has an improvement of $\\mathbf{7} \\times$ in rounds and $\\mathbf{18} \\times$ in the communication complexity. The practicality of our framework is argued through improvements in the benchmarking of the aforementioned algorithms when compared with ABY3. All the protocols are implemented over a 64-bit ring in both LAN and WAN settings. Our improvements go up to $\\mathbf{187} \\times$ for the training phase and $\\mathbf{158} \\times$ for the prediction phase when observed over LAN and WAN.",
      "year": 2020,
      "venue": "NDSS",
      "authors": [
        "Harsh Chaudhari",
        "Rahul Rachuri",
        "Ajith Suresh"
      ],
      "url": "https://arxiv.org/abs/1912.02631",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774952",
      "fetched_at": "2026-01-09T12:14:35.774953",
      "classified_at": "2026-01-09T13:27:40.688721",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_1575c562": {
      "paper_id": "seed_1575c562",
      "title": "Cerebro: A Platform for Multi-Party Cryptographic Collaborative Learning",
      "abstract": null,
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [],
      "url": "https://www.usenix.org/system/files/sec21-zheng.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774956",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2208.08662": {
      "paper_id": "2208.08662",
      "title": "Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy",
      "abstract": "Secure multi-party computation-based machine learning, referred to as MPL, has become an important technology to utilize data from multiple parties with privacy preservation. While MPL provides rigorous security guarantees for the computation process, the models trained by MPL are still vulnerable to attacks that solely depend on access to the models. Differential privacy could help to defend against such attacks. However, the accuracy loss brought by differential privacy and the huge communication overhead of secure multi-party computation protocols make it highly challenging to balance the 3-way trade-off between privacy, efficiency, and accuracy.   In this paper, we are motivated to resolve the above issue by proposing a solution, referred to as PEA (Private, Efficient, Accurate), which consists of a secure DPSGD protocol and two optimization methods. First, we propose a secure DPSGD protocol to enforce DPSGD in secret sharing-based MPL frameworks. Second, to reduce the accuracy loss led by differential privacy noise and the huge communication overhead of MPL, we propose two optimization methods for the training process of MPL: (1) the data-independent feature extraction method, which aims to simplify the trained model structure; (2) the local data-based global model initialization method, which aims to speed up the convergence of the model training. We implement PEA in two open-source MPL frameworks: TF-Encrypted and Queqiao. The experimental results on various datasets demonstrate the efficiency and effectiveness of PEA. E.g. when $\u03b5$ = 2, we can train a differentially private classification model with an accuracy of 88% for CIFAR-10 within 7 minutes under the LAN setting. This result significantly outperforms the one from CryptGPU, one SOTA MPL framework: it costs more than 16 hours to train a non-private deep neural network model on CIFAR-10 with the same accuracy.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Wenqiang Ruan",
        "Mingxin Xu",
        "Wenjing Fang",
        "Li Wang",
        "Lei Wang",
        "Weili Han"
      ],
      "url": "https://arxiv.org/abs/2208.08662",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774958",
      "fetched_at": "2026-01-09T12:14:35.774959",
      "classified_at": "2026-01-09T13:27:41.562809",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_8c64353b": {
      "paper_id": "seed_8c64353b",
      "title": "MPCDiff: Testing and Repairing MPC-Hardened Deep Learning Models",
      "abstract": "Secure multi-party computation (MPC) has recently become prominent as a concept to enable multiple parties to perform privacy-preserving machine learning without leaking sensitive data or details of pre-trained models to the other parties. Industry and the community have been actively developing and promoting high-quality MPC frameworks (e.g., based on TensorFlow and PyTorch) to enable the usage of MPC-hardened models, greatly easing the development cycle of integrating deep learning models with MPC primitives. Despite the prosperous development and adoption of MPC frameworks, a principled and systematic understanding toward the correctness of those MPC frameworks does not yet exist. To fill this critical gap, this paper introduces MPCDiff, a differential testing framework to effectively uncover inputs that cause deviant outputs of MPC-hardened models and their plaintext versions. We further develop techniques to localize error-causing computation units in MPC-hardened models and automatically repair those defects. We evaluate MPCDiff using real-world popular MPC frameworks for deep learning developed by Meta (Facebook), Alibaba Group, Cape Privacy, and OpenMined. MPCDiff successfully detected over one thousand inputs that result in largely deviant outputs. These deviation-triggering inputs are (visually) meaningful in comparison to regular inputs, indicating that our findings may cause great confusion in the daily usage of MPC frameworks. After localizing and repairing error-causing computation units, the robustness of MPC-hardened models can be notably enhanced without sacrificing accuracy and with negligible overhead.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Qi Pang",
        "Yuanyuan Yuan",
        "Shuai Wang"
      ],
      "url": "https://openalex.org/W4391725262",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774962",
      "fetched_at": "2026-01-09T13:54:53.566764",
      "classified_at": "2026-01-09T14:02:28.727561",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391725262",
      "doi": "https://doi.org/10.14722/ndss.2024.23380"
    },
    "seed_5403e716": {
      "paper_id": "seed_5403e716",
      "title": "Pencil: Private and Extensible Collaborative Learning without the Non-Colluding Assumption",
      "abstract": "The escalating focus on data privacy poses significant challenges for collaborative neural network training, where data ownership and model training/deployment responsibilities reside with distinct entities.Our community has made substantial contributions to addressing this challenge, proposing various approaches such as federated learning (FL) and privacy-preserving machine learning based on cryptographic constructs like homomorphic encryption (HE) and secure multiparty computation (MPC).However, FL completely overlooks model privacy, and HE has limited extensibility (confined to only one data provider).While the state-of-the-art MPC frameworks provide reasonable throughput and simultaneously ensure model/data privacy, they rely on a critical non-colluding assumption on the computing servers, and relaxing this assumption is still an open problem.In this paper, we present Pencil, the first private training framework for collaborative learning that simultaneously offers data privacy, model privacy, and extensibility to multiple data providers, without relying on the non-colluding assumption.Our fundamental design principle is to construct the n-party collaborative training protocol based on an efficient two-party protocol, and meanwhile ensuring that switching to different data providers during model training introduces no extra cost.We introduce several novel cryptographic protocols to realize this design principle and conduct a rigorous security and privacy analysis.Our comprehensive evaluations of Pencil demonstrate that (i) models trained in plaintext and models trained privately using Pencil exhibit nearly identical test accuracies; (ii) The training overhead of Pencil is greatly reduced: Pencil achieves 10 \u223c 260\u00d7 higher throughput and 2 orders of magnitude less communication than prior art; (iii) Pencil is resilient against both existing and adaptive (white-box) attacks.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Xuanqi Liu",
        "Zhuotao Liu",
        "Qi Li",
        "Ke Xu",
        "Mingwei Xu"
      ],
      "url": "https://openalex.org/W4391724821",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774965",
      "fetched_at": "2026-01-09T13:54:54.360701",
      "classified_at": "2026-01-09T14:02:30.558238",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391724821",
      "doi": "https://doi.org/10.14722/ndss.2024.24512"
    },
    "seed_b9309184": {
      "paper_id": "seed_b9309184",
      "title": "Securely Training Decision Trees Efficiently",
      "abstract": null,
      "year": 2024,
      "venue": "CCS",
      "authors": [],
      "url": "https://eprint.iacr.org/2024/1077.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774968",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_a5384e06": {
      "paper_id": "seed_a5384e06",
      "title": "CoGNN: Towards Secure and Efficient Collaborative Graph Learning",
      "abstract": null,
      "year": 2024,
      "venue": "CCS",
      "authors": [],
      "url": "https://eprint.iacr.org/2024/987.pdf",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774971",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_e0f521da": {
      "paper_id": "seed_e0f521da",
      "title": "SoK: Cryptographic Neural-Network Computation",
      "abstract": "We studied 53 privacy-preserving neural-network papers in 2016-2022 based on cryptography (without trusted processors or differential privacy), 16 of which only use homomorphic encryption, 19 use secure computation for inference, and 18 use non-colluding servers (among which 12 support training), solving a wide variety of research problems. We dissect their cryptographic techniques and \"love-hate relationships\" with machine learning alongside a genealogy highlighting noteworthy developments. We also re-evaluate the state of the art under WAN. We hope this can serve as a go-to guide connecting different experts in related fields.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Lucien K. L. Ng",
        "Sherman S. M. Chow"
      ],
      "url": "https://openalex.org/W4385679725",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "discarded",
      "classification": "NONE",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774973",
      "fetched_at": "2026-01-09T13:54:56.688729",
      "classified_at": "2026-01-09T14:02:32.487496",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385679725",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179483"
    },
    "seed_ebc97993": {
      "paper_id": "seed_ebc97993",
      "title": "From Individual Computation to Allied Optimization: Remodeling Privacy-Preserving Neural Inference with Function Input Tuning",
      "abstract": null,
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [],
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a101/1Ub238IknIs",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "pending",
      "classification": null,
      "classification_confidence": null,
      "added_at": "2026-01-09T12:14:35.774976",
      "fetched_at": null,
      "classified_at": null,
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_a6fc93cc": {
      "paper_id": "seed_a6fc93cc",
      "title": "BOLT: Privacy-Preserving, Accurate and Efficient Inference for Transformers",
      "abstract": "The advent of transformers has brought about significant advancements in traditional machine learning tasks. However, their pervasive deployment has raised concerns about the potential leakage of sensitive information during inference. Existing approaches using secure multiparty computation (MPC) face limitations when applied to transformers due to the extensive model size and resource-intensive matrix-matrix multiplications. In this paper, we present BOLT, a privacy-preserving inference framework for transformer models that supports efficient matrix multiplications and nonlinear computations. Combined with our novel machine learning optimizations, BOLT reduces the communication cost by 10.91\u00d7. Our evaluation on diverse datasets demonstrates that BOLT maintains comparable accuracy to floating-point models and achieves 4.8-9.5\u00d7 faster inference across various network settings compared to the state-of-the-art system.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Qi Pang",
        "Jinhao Zhu",
        "Helen M\u00f6llering",
        "Wenting Zheng",
        "Thomas Schneider"
      ],
      "url": "https://openalex.org/W4402263660",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774979",
      "fetched_at": "2026-01-09T13:54:58.259175",
      "classified_at": "2026-01-09T14:02:34.431469",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4402263660",
      "doi": "https://doi.org/10.1109/sp54263.2024.00130"
    },
    "seed_02eead2f": {
      "paper_id": "seed_02eead2f",
      "title": "Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning",
      "abstract": "This paper introduces Flamingo, a system for secure aggregation of data across a large set of clients. In secure aggregation, a server sums up the private inputs of clients and obtains the result without learning anything about the individual inputs beyond what is implied by the final sum. Flamingo focuses on the multi-round setting found in federated learning in which many consecutive summations (averages) of model weights are performed to derive a good model. Previous protocols, such as Bell et al. (CCS '20), have been designed for a single round and are adapted to the federated learning setting by repeating the protocol multiple times. Flamingo eliminates the need for the per-round setup of previous protocols, and has a new lightweight dropout resilience protocol to ensure that if clients leave in the middle of a sum the server can still obtain a meaningful result. Furthermore, Flamingo introduces a new way to locally choose the so-called client neighborhood introduced by Bell et al. These techniques help Flamingo reduce the number of interactions between clients and the server, resulting in a significant reduction in the end-to-end runtime for a full training session over prior work. We implement and evaluate Flamingo and show that it can securely train a neural network on the (Extended) MNIST and CIFAR-100 datasets, and the model converges without a loss in accuracy, compared to a non-private federated learning system.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Yiping Ma",
        "Jess Woods",
        "Sebastian Angel",
        "Antigoni Polychroniadou",
        "Tal Rabin"
      ],
      "url": "https://openalex.org/W4385679694",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774981",
      "fetched_at": "2026-01-09T13:54:59.082506",
      "classified_at": "2026-01-09T14:02:36.329855",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385679694",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179434"
    },
    "seed_80aedcd7": {
      "paper_id": "seed_80aedcd7",
      "title": "ELSA: Secure Aggregation for Federated Learning with Malicious Actors",
      "abstract": "Federated learning (FL) is an increasingly popular approach for machine learning (ML) in cases where the training dataset is highly distributed. Clients perform local training on their datasets and the updates are then aggregated into the global model. Existing protocols for aggregation are either inefficient, or don't consider the case of malicious actors in the system. This is a major barrier in making FL an ideal solution for privacy-sensitive ML applications. We present Elsa, a secure aggregation protocol for FL, which breaks this barrier - it is efficient and addresses the existence of malicious actors at the core of its design. Similar to prior work on Prio and Prio+, Elsa provides a novel secure aggregation protocol built out of distributed trust across two servers that keeps individual client updates private as long as one server is honest, defends against malicious clients, and is efficient end-to-end. Compared to prior works, the distinguishing theme in Elsa is that instead of the servers generating cryptographic correlations interactively, the clients act as untrusted dealers of these correlations without compromising the protocol's security. This leads to a much faster protocol while also achieving stronger security at that efficiency compared to prior work. We introduce new techniques that retain privacy even when a server is malicious at a small added cost of 7-25% in runtime with negligible increase in communication over the case of semi-honest server. Our work improves end-to-end runtime over prior work with similar security guarantees by big margins - single-aggregator RoFL by up to 305x (for the models we consider), and distributed trust Prio by up to 8x.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Mayank Rathee",
        "Conghao Shen",
        "Sameer Wagh",
        "Raluca Ada Popa"
      ],
      "url": "https://openalex.org/W4385299238",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.774984",
      "fetched_at": "2026-01-09T13:54:59.854917",
      "classified_at": "2026-01-09T14:02:38.133768",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385299238",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179468"
    },
    "seed_c76b43b1": {
      "paper_id": "seed_c76b43b1",
      "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
      "abstract": "Inference attacks against Machine Learning (ML) models allow adversaries to learn sensitive information about training data, model parameters, etc. While researchers have studied, in depth, several kinds of attacks, they have done so in isolation. As a result, we lack a comprehensive picture of the risks caused by the attacks, e.g., the different scenarios they can be applied to, the common factors that influence their performance, the relationship among them, or the effectiveness of possible defenses. In this paper, we fill this gap by presenting a first-of-its-kind holistic risk assessment of different inference attacks against machine learning models. We concentrate on four attacks -- namely, membership inference, model inversion, attribute inference, and model stealing -- and establish a threat model taxonomy. Our extensive experimental evaluation, run on five model architectures and four image datasets, shows that the complexity of the training dataset plays an important role with respect to the attack's performance, while the effectiveness of model stealing and membership inference attacks are negatively correlated. We also show that defenses like DP-SGD and Knowledge Distillation can only mitigate some of the inference attacks. Our analysis relies on a modular re-usable software, ML-Doctor, which enables ML model owners to assess the risks of deploying their models, and equally serves as a benchmark tool for researchers and practitioners.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yugeng Liu",
        "Rui Wen",
        "Xinlei He",
        "Ahmed Salem",
        "Zhikun Zhang",
        "Michael Backes",
        "Emiliano De Cristofaro",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W3126152116",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.775039",
      "fetched_at": "2026-01-09T13:55:00.661446",
      "classified_at": "2026-01-09T14:02:40.060251",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W3126152116",
      "doi": "https://doi.org/10.48550/arxiv.2102.02551"
    },
    "2212.10986": {
      "paper_id": "2212.10986",
      "title": "SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning",
      "abstract": "Deploying machine learning models in production may allow adversaries to infer sensitive information about training data. There is a vast literature analyzing different types of inference risks, ranging from membership inference to reconstruction attacks. Inspired by the success of games (i.e., probabilistic experiments) to study security properties in cryptography, some authors describe privacy inference risks in machine learning using a similar game-based style. However, adversary capabilities and goals are often stated in subtly different ways from one presentation to the other, which makes it hard to relate and compose results. In this paper, we present a game-based framework to systematize the body of knowledge on privacy inference risks in machine learning. We use this framework to (1) provide a unifying structure for definitions of inference risks, (2) formally establish known relations among definitions, and (3) to uncover hitherto unknown relations that would have been difficult to spot otherwise.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Ahmed Salem",
        "Giovanni Cherubin",
        "David Evans",
        "Boris K\u00f6pf",
        "Andrew Paverd",
        "Anshuman Suri",
        "Shruti Tople",
        "Santiago Zanella-B\u00e9guelin"
      ],
      "url": "https://arxiv.org/abs/2212.10986",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.775042",
      "fetched_at": "2026-01-09T12:14:35.775043",
      "classified_at": "2026-01-09T13:27:42.203380",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_794e5960": {
      "paper_id": "seed_794e5960",
      "title": "Federated Boosted Decision Trees with Differential Privacy",
      "abstract": "There is great demand for scalable, secure, and effcient privacypreserving machine learning models that can be trained over distributed data. While deep learning models typically achieve the best results in a centralized non-secure setting, different models can excel when privacy and communication constraints are imposed. Instead, tree-based approaches such as XGBoost have attracted much attention for their high performance and ease of use; in particular, they often achieve state-of-the-art results on tabular data. Consequently, several recent works have focused on translating Gradient Boosted Decision Tree (GBDT) models like XGBoost into federated settings, via cryptographic mechanisms such as Homomorphic Encryption (HE) and Secure Multi-Party Computation (MPC). However, these do not always provide formal privacy guarantees, or consider the full range of hyperparameters and implementation settings. In this work, we implement the GBDT model under Differential Privacy (DP). We propose a general framework that captures and extends existing approaches for differentially private decision trees. Our framework of methods is tailored to the federated setting, and we show that with a careful choice of techniques it is possible to achieve very high utility while maintaining strong levels of privacy.&#13;\\n&#13;\\n",
      "year": 2022,
      "venue": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
      "authors": [
        "Samuel Maddock",
        "Graham Cormode",
        "Tianhao Wang",
        "Carsten Maple",
        "Somesh Jha"
      ],
      "url": "https://openalex.org/W4308643126",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML06",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.775045",
      "fetched_at": "2026-01-09T13:55:01.427044",
      "classified_at": "2026-01-09T14:02:41.997723",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4308643126",
      "doi": "https://doi.org/10.1145/3548606.3560687"
    },
    "seed_5abd658b": {
      "paper_id": "seed_5abd658b",
      "title": "Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering",
      "abstract": "Differential privacy is a widely accepted measure of privacy in the context of deep learning algorithms, and achieving it relies on a noisy training approach known as differentially private stochastic gradient descent (DP-SGD). DP-SGD requires direct noise addition to every gradient in a dense neural network, the privacy is achieved at a significant utility cost. In this work, we present Spectral-DP, a new differentially private learning approach which combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility. We develop differentially private deep learning methods based on Spectral-DP for architectures that contain both convolution and fully connected layers. In particular, for fully connected layers, we combine a block-circulant based spatial restructuring with Spectral-DP to achieve better utility. Through comprehensive experiments, we study and provide guidelines to implement Spectral-DP deep learning on benchmark datasets. In comparison with state-of-the-art DP-SGD based approaches, Spectral-DP is shown to have uniformly better utility performance in both training from scratch and transfer learning settings.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Ce Feng",
        "Nuo Xu",
        "Wujie Wen",
        "Parv Venkitasubramaniam",
        "Caiwen Ding"
      ],
      "url": "https://openalex.org/W4385679682",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.775049",
      "fetched_at": "2026-01-09T13:55:02.230665",
      "classified_at": "2026-01-09T14:02:43.921690",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385679682",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179457"
    },
    "2311.02324": {
      "paper_id": "2311.02324",
      "title": "Bounded and Unbiased Composite Differential Privacy",
      "abstract": "The objective of differential privacy (DP) is to protect privacy by producing an output distribution that is indistinguishable between any two neighboring databases. However, traditional differentially private mechanisms tend to produce unbounded outputs in order to achieve maximum disturbance range, which is not always in line with real-world applications. Existing solutions attempt to address this issue by employing post-processing or truncation techniques to restrict the output results, but at the cost of introducing bias issues. In this paper, we propose a novel differentially private mechanism which uses a composite probability density function to generate bounded and unbiased outputs for any numerical input data. The composition consists of an activation function and a base function, providing users with the flexibility to define the functions according to the DP constraints. We also develop an optimization algorithm that enables the iterative search for the optimal hyper-parameter setting without the need for repeated experiments, which prevents additional privacy overhead. Furthermore, we evaluate the utility of the proposed mechanism by assessing the variance of the composite probability density function and introducing two alternative metrics that are simpler to compute than variance estimation. Our extensive evaluation on three benchmark datasets demonstrates consistent and significant improvement over the traditional Laplace and Gaussian mechanisms. The proposed bounded and unbiased composite differentially private mechanism will underpin the broader DP arsenal and foster future privacy-preserving studies.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Kai Zhang",
        "Yanjun Zhang",
        "Ruoxi Sun",
        "Pei-Wei Tsai",
        "Muneeb Ul Hassan",
        "Xin Yuan",
        "Minhui Xue",
        "Jinjun Chen"
      ],
      "url": "https://arxiv.org/abs/2311.02324",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.775053",
      "fetched_at": "2026-01-09T12:14:35.775054",
      "classified_at": "2026-01-09T13:27:42.860443",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2301.08517": {
      "paper_id": "2301.08517",
      "title": "Cohere: Managing Differential Privacy in Large Scale Systems",
      "abstract": "The need for a privacy management layer in today's systems started to manifest with the emergence of new systems for privacy-preserving analytics and privacy compliance. As a result, many independent efforts have emerged that try to provide system support for privacy. Recently, the scope of privacy solutions used in systems has expanded to encompass more complex techniques such as Differential Privacy (DP). The use of these solutions in large-scale systems imposes new challenges and requirements. Careful planning and coordination are necessary to ensure that privacy guarantees are maintained across a wide range of heterogeneous applications and data systems. This requires new solutions for managing and allocating scarce and non-replenishable privacy resources. In this paper, we introduce Cohere, a new system that simplifies the use of DP in large-scale systems. Cohere implements a unified interface that allows heterogeneous applications to operate on a unified view of users' data. In this work, we further address two pressing system challenges that arise in the context of real-world deployments: ensuring the continuity of privacy-based applications (i.e., preventing privacy budget depletion) and effectively allocating scarce shared privacy resources (i.e., budget) under complex preferences. Our experiments show that Cohere achieves a 6.4--28x improvement in utility compared to the state-of-the-art across a range of complex workloads.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Nicolas K\u00fcchler",
        "Emanuel Opel",
        "Hidde Lycklama",
        "Alexander Viand",
        "Anwar Hithnawi"
      ],
      "url": "https://arxiv.org/abs/2301.08517",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.775057",
      "fetched_at": "2026-01-09T12:14:35.775058",
      "classified_at": "2026-01-09T13:27:43.507368",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "seed_546bce6d": {
      "paper_id": "seed_546bce6d",
      "title": "You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks",
      "abstract": "Image data have been extensively used in Deep Neural Network (DNN) tasks in various scenarios, e.g., autonomous driving and medical image analysis, which incurs significant privacy concerns.Existing privacy protection techniques are unable to efficiently protect such data.For example, Differential Privacy (DP) that is an emerging technique protects data with strong privacy guarantee cannot effectively protect visual features of exposed image dataset.In this paper, we propose a novel privacy-preserving framework VisualMixer that protects the training data of visual DNN tasks by pixel shuffling, while not injecting any noises.VisualMixer utilizes a new privacy metric called Visual Feature Entropy (VFE) to effectively quantify the visual features of an image from both biological and machine vision aspects.In VisualMixer, we devise a task-agnostic image obfuscation method to protect the visual privacy of data for DNN training and inference.For each image, it determines regions for pixel shuffling in the image and the sizes of these regions according to the desired VFE.It shuffles pixels both in the spatial domain and in the chromatic channel space in the regions without injecting noises so that it can prevent visual features from being discerned and recognized, while incurring negligible accuracy loss.Extensive experiments on real-world datasets demonstrate that VisualMixer can effectively preserve the visual privacy with negligible accuracy loss, i.e., at average 2.35 percentage points of model accuracy loss, and almost no performance degradation on model training.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Qiushi Li",
        "Yan Zhang",
        "Ju Ren",
        "Qi Li",
        "Yaoxue Zhang"
      ],
      "url": "https://openalex.org/W4391724770",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.775061",
      "fetched_at": "2026-01-09T13:55:03.038408",
      "classified_at": "2026-01-09T14:02:45.730023",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4391724770",
      "doi": "https://doi.org/10.14722/ndss.2024.241361"
    },
    "seed_0c4e3f3c": {
      "paper_id": "seed_0c4e3f3c",
      "title": "Locally Differentially Private Frequency Estimation Based on Convolution Framework",
      "abstract": "Local differential privacy (LDP) collects user data while protecting user privacy and eliminating the need for a trusted data collector. Several LDP protocols have been proposed and deployed in real-world applications. Frequency estimation is a fundamental task in the LDP protocols, which enables more advanced tasks in data analytics. However, the existing LDP protocols amplify the added noise in estimating the frequencies and therefore do not achieve optimal performance in accuracy. This paper introduces a convolution framework to analyze and optimize the estimated frequencies of LDP protocols. The convolution framework can equivalently transform the original frequency estimation problem into a deconvolution problem with noise. We thus add the Wiener filter-based deconvolution algorithms to LDP protocols to estimate the frequency while suppressing the added noise. Experimental results on different real-world datasets demonstrate that our proposed algorithms can lead to significantly better accuracy for state-of-the-art LDP protocols by orders of magnitude for the smooth dataset. And these algorithms also work on non-smooth datasets, but only to a limited extent. Our code is available at https://github.com/SEUNICK/LDP.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Huiyu Fang",
        "Liquan Chen",
        "Yali Liu",
        "Yuan Gao"
      ],
      "url": "https://openalex.org/W4385679702",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML03",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.775064",
      "fetched_at": "2026-01-09T13:55:03.810177",
      "classified_at": "2026-01-09T14:02:47.524844",
      "expanded_at": null,
      "citations_checked_at": null,
      "openalex_id": "https://openalex.org/W4385679702",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179389"
    },
    "2406.19466": {
      "paper_id": "2406.19466",
      "title": "Data Poisoning Attacks to Locally Differentially Private Frequent Itemset Mining Protocols",
      "abstract": "Local differential privacy (LDP) provides a way for an untrusted data collector to aggregate users' data without violating their privacy. Various privacy-preserving data analysis tasks have been studied under the protection of LDP, such as frequency estimation, frequent itemset mining, and machine learning. Despite its privacy-preserving properties, recent research has demonstrated the vulnerability of certain LDP protocols to data poisoning attacks. However, existing data poisoning attacks are focused on basic statistics under LDP, such as frequency estimation and mean/variance estimation. As an important data analysis task, the security of LDP frequent itemset mining has yet to be thoroughly examined. In this paper, we aim to address this issue by presenting novel and practical data poisoning attacks against LDP frequent itemset mining protocols. By introducing a unified attack framework with composable attack operations, our data poisoning attack can successfully manipulate the state-of-the-art LDP frequent itemset mining protocols and has the potential to be adapted to other protocols with similar structures. We conduct extensive experiments on three datasets to compare the proposed attack with four baseline attacks. The results demonstrate the severity of the threat and the effectiveness of the proposed attack.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Wei Tong",
        "Haoyu Chen",
        "Jiacheng Niu",
        "Sheng Zhong"
      ],
      "url": "https://arxiv.org/abs/2406.19466",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML02",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.775068",
      "fetched_at": "2026-01-09T12:14:35.775069",
      "classified_at": "2026-01-09T13:27:44.330955",
      "expanded_at": null,
      "citations_checked_at": null
    },
    "2405.06823": {
      "paper_id": "2405.06823",
      "title": "PLeak: Prompt Leaking Attacks against Large Language Model Applications",
      "abstract": "Large Language Models (LLMs) enable a new ecosystem with many downstream applications, called LLM applications, with different natural language processing tasks. The functionality and performance of an LLM application highly depend on its system prompt, which instructs the backend LLM on what task to perform. Therefore, an LLM application developer often keeps a system prompt confidential to protect its intellectual property. As a result, a natural attack, called prompt leaking, is to steal the system prompt from an LLM application, which compromises the developer's intellectual property. Existing prompt leaking attacks primarily rely on manually crafted queries, and thus achieve limited effectiveness.   In this paper, we design a novel, closed-box prompt leaking attack framework, called PLeak, to optimize an adversarial query such that when the attacker sends it to a target LLM application, its response reveals its own system prompt. We formulate finding such an adversarial query as an optimization problem and solve it with a gradient-based method approximately. Our key idea is to break down the optimization goal by optimizing adversary queries for system prompts incrementally, i.e., starting from the first few tokens of each system prompt step by step until the entire length of the system prompt.   We evaluate PLeak in both offline settings and for real-world LLM applications, e.g., those on Poe, a popular platform hosting such applications. Our results show that PLeak can effectively leak system prompts and significantly outperforms not only baselines that manually curate queries but also baselines with optimized queries that are modified and adapted from existing jailbreaking attacks. We responsibly reported the issues to Poe and are still waiting for their response. Our implementation is available at this repository: https://github.com/BHui97/PLeak.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Bo Hui",
        "Haolin Yuan",
        "Neil Gong",
        "Philippe Burlina",
        "Yinzhi Cao"
      ],
      "url": "https://arxiv.org/abs/2405.06823",
      "source": "seed",
      "source_paper_id": null,
      "depth": 0,
      "status": "classified",
      "classification": "ML04",
      "classification_confidence": "HIGH",
      "added_at": "2026-01-09T12:14:35.775072",
      "fetched_at": "2026-01-09T12:14:35.775073",
      "classified_at": "2026-01-09T13:27:45.001045",
      "expanded_at": null,
      "citations_checked_at": null
    }
  },
  "metadata": {
    "total_papers": 457,
    "by_status": {
      "classified": 347,
      "pending": 59,
      "discarded": 51
    },
    "by_category": {
      "ML01": 105,
      "ML07": 2,
      "NONE": 51,
      "ML02": 74,
      "ML09": 21,
      "ML06": 17,
      "ML10": 4,
      "ML08": 4,
      "ML04": 24,
      "ML03": 85,
      "ML05": 11
    },
    "last_updated": "2026-01-09T14:02:49.025051"
  }
}