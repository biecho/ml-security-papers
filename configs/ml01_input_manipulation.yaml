# Configuration for paper filtering - ML01: Input Manipulation Attack
# OWASP Machine Learning Security Top 10

domain:
  name: "input_manipulation"
  owasp_id: "ML01"
  owasp_name: "Input Manipulation Attack"
  description: "Attacks that trick ML models into misclassifying inputs through malicious tampering"
  short_description: "Adversarial examples and evasion attacks on ML models"

# High-quality keywords that strongly indicate relevant papers
high_quality_keywords:
  - "adversarial example"
  - "adversarial attack"
  - "evasion attack"
  - "adversarial perturbation"
  - "adversarial patch"
  - "adversarial robustness"
  - "fooling deep neural network"

# Core keywords that may indicate relevant papers
core_keywords:
  - "adversarial input"
  - "input perturbation"
  - "misclassification attack"
  - "evasion"
  - "adversarial sample"
  - "robust classifier"
  - "adversarial training"
  - "perturbation attack"
  - "image perturbation"
  - "fooling neural network"
  - "adversarial noise"

# Defense-related keywords (still relevant to the domain)
defense_keywords:
  - "adversarial defense"
  - "adversarial robustness"
  - "certified defense"
  - "robust neural network"
  - "adversarial detection"

# Keywords known to cause false positives - should be avoided
problematic_keywords:
  - "adversarial network"
  - "generative adversarial"
  - "GAN"
  - "adversarial learning"
  - "domain adversarial"

# Terms that must appear in abstract for a paper to be considered relevant
required_abstract_terms:
  - "adversarial"
  - "evasion"
  - "perturbation"
  - "robust"
  - "attack"

# Signals that indicate the paper is about something else
exclusion_signals:
  generative_models:
    - "generative adversarial network"
    - "GAN training"
    - "image generation"
  domain_adaptation:
    - "domain adversarial"
    - "domain adaptation"
  game_theory:
    - "adversarial game"
    - "multi-agent"

# Topics that may mention the domain but are primarily about something else
other_topics:
  model_stealing:
    - "model extraction"
    - "model stealing"
  data_poisoning:
    - "poisoning attack"
    - "backdoor attack"
  privacy:
    - "membership inference"
    - "model inversion"

# Filtering thresholds and rules
filtering_rules:
  min_term_mentions: 1
  watermark_dominance_threshold: 3
  topic_dominance_ratio: 2.0
  context_window: 50
  first_paragraph_length: 300
